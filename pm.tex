%!TEX encoding = UTF-8 Unicode
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[hmargin=1in,vmargin=1in]{geometry}

\usepackage[dvipsnames,usenames]{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=Blue, citecolor=Green, linkcolor=BrickRed, breaklinks, unicode}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{euscript}
\usepackage{mathpazo}
\usepackage[scaled=.90]{berasans,beramono}

%\usepackage{epsfig}
%\usepackage{floatflt}
\usepackage{graphicx}

\usepackage{microtype}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage[shortlabels, inline]{enumitem}

\usepackage{latexsym,amsmath,amssymb,amsthm}
\usepackage{algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools} % for \coloneqq

\usepackage[title]{appendix}

\usepackage{ifpdf}

%\graphicspath{{fig/}}

%\DeclareGraphicsExtensions{.pdf}

%\ifpdf
%\else
%   \DeclareGraphicsExtensions{.eps}
%\fi

\def\etal{\textsl{et~al.}}
\def\polylog{\mathop{\mathrm{polylog}}}
\def\eps{\varepsilon}
\def\softO{\widetilde{O}}
\def\bd{{\partial}}
\def\reals{\mathbb{R}}
\def\ints{\mathbb{N}}
\def\flr#1{{\lfloor #1\rfloor}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\tsupply{\lambda}
\def\fsupply{\phi}

\def\Refine{\textsc{Refine}}
\def\Update{\textsc{Update}}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{definition}[lemma]{Definition}
\numberwithin{figure}{section}

\newcommand{\lemlab}[1]{\label{lem:#1}}
\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\theolab}[1]{\label{theo:#1}}
\newcommand{\corlab}[1]{\label{cor:#1}}
\def\lemref#1{Lemma~\ref{lemma:#1}}
\def\theoref#1{Theorem~\ref{theo:#1}}
\def\corref#1{Corollary~\ref{cor:#1}}
\def\figref#1{Figure~\ref{fig:#1}} \def\secref#1{Section~\ref{sec:#1}}

\def\cost{\operatorname{cost}}
\def\parent{\operatorname{par}}
\def\short{\operatorname{short}}


\makeatletter
\long\def\@makecaption#1#2{
    \vskip 10pt
    \setbox\@tempboxa\hbox{{\footnotesize {\bf #1.} #2}}
    \ifdim \wd\@tempboxa >\hsize         % IF longer than one line:
        {\footnotesize {\bf #1.} #2\par}% THEN set as ordinary paragraph.
      \else                              %   ELSE  center.
        \hbox to\hsize{\hfil\box\@tempboxa\hfil}
    \fi}
%\dbltextfloatsep 18pt plus 2pt minus 4pt   % was 20pt plus 2pt minus 4pt
%\textfloatsep 18pt plus 2pt minus 4pt      % was 20pt plus 2pt minus 4pt
\makeatother

% ----------------------------------------------------------------------
%  Notes to myself.  The margin flags are broken, thanks to an
%  incompatibility with the geometry package.
% ----------------------------------------------------------------------
\def\n@te#1{\textsf{\boldmath \textbf{$\langle\!\langle$#1$\rangle\!\rangle$}}\leavevmode}
\def\note#1{\textcolor{red}{\n@te{#1}}}


%----------------------------------------------------------------------
% 'cramped' list style, stolen from Jeff Vitter.  Doesn't always work.
%----------------------------------------------------------------------
\def\cramped
  {\parskip\@outerparskip\@topsep\parskip\@topsepadd2pt\itemsep0pt
}


%% METAFILE
\title{ Geometric Partial Matching and Unbalanced Transportation %
\date{\today} % replace with date?
\author{
Pankaj K.\ Agarwal
\and
Allen Xiao
}
}


\begin{document}

\maketitle

\section{Introduction}

Consider the problem of finding a minimum-cost bichromatic matching between
a set of red points $A$ and a set of blue points $B$ lying in the plane,
where the cost of a matching edge $(a, b)$ is the Euclidean distance
$\|a - b\|$;
in other words, the minimum-cost bipartite matching problem on the Euclidean
complete graph $G = (A \cup B, A \times B)$.
Let $r$ be the number of vertices in $A$, and $n$ be the number of vertices in $B$.
Without loss of generality assume that $r \leq n$.
We consider the problem of \emph{partial matching}, where the task is to
find the minimum-cost matching of size $k$ (which by definition is at most $r$).
When $k = r = n$, we say the matching instance is \emph{balanced}
and call the problem \emph{perfect matching} or the \emph{assignment problem}.
When $k = r < n$ ($A$ and $B$ have different sizes, but the matching is
maximal), we say the matching instance is \emph{unblanced}.
\note{Don't mix up the definition of \emph{balanced} and \emph{perfect}; the former is between $r$ and $n$, where the latter is between $k$ and $r$.}
Partial matching generalizes both perfect matching and unbalanced matching.
We will refer to the geometric problem as \emph{geometric partial matching}.
\text{Maybe bad nameing; there is nothing geometric about this name.}

% \begin{TODO}
% Previous work
% \begin{itemize}\itemsep=0pt
% 	\item on matching
% 	\item on geometric matching
% 	\item on unbalanced/partial problems (GHKT, the push-relabel one, the tech report)
% \end{itemize}
% \noindent\textcolor{blue}{[Hsien: State your TODO list explicitly in the pdf file so that it's easier to read.  Make everything that you are planning to do, and put priorities on them.]}
% \end{TODO}


\subsection{Contributions}

In this paper, we present two algorithms for geometric partial matching
that are based on fitting nearest-neighbor (NN) and geometric closest pair
(BCP) oracles into primal-dual algorithms for non-geometric bipartite matching
and minimum-cost flow.
This pattern is not new, see for example \note{TODO}.
Unlike these previous works, we focus on obtaining running time dependencies on
$k$ or $r$ instead of $n$, that is, faster for inputs with small $r$ or $k$.
We begin in Section~\ref{section:prelim} by introducing notation for matching
and minimum-cost flow.

% O((n + k^2)\polylog n)

First in Section~\ref{section:hung}, we show that the Hungarian algorithm~\cite{Kuhn55}
combined with a BCP oracle solves geometric partial matching exactly in time
$O((n + k^2)\polylog n)$.
Mainly, we show that we can separate the $O(n\polylog n)$ preprocessing time
for building the BCP data structure from the augmenting paths' search time,
and update duals in a lazy fashion such that the number of dual updates per
augmenting path is $O(k)$.

\begin{theorem}
\label{theorem:hung}
Let $A$ and $B$ be two point sets in the plane with $|A| = r$ and $|B| = n$ satisfying $r \le n$, and let $k$ be a
parameter.  A minimum-cost geometric partial matching of size $k$
can be computed between $A$ and $B$ in $O((n + k^2)\polylog n)$ time.
\end{theorem}

% O((n + k\sqrt{k})\polylog n\log(n/\eps))

Next in Section~\ref{section:goldberg}, we apply a similar technique to the unit-capacity min-cost circulation
algorithm of Goldberg, Hed, Kaplan, and Tarjan~\cite{GHKT17}.
The resulting algorithm finds a $(1 + \eps)$-approximation to the optimal
geometric partial matching in $O((n + k\sqrt{k})\polylog n \log(n/\eps))$
time.

\begin{theorem}
\label{theorem:gmcm}
Let $A$ and $B$ be two point sets in the plane with $|A| = r$ and $|B| = n$ satisfying $r \le n$, and let $k$ be a
parameter.
A $(1+\eps)$ geometric partial matching of size $k$
can be computed between $A$ and $B$ in
$O((n + k\sqrt{k})\polylog n \log(n/\eps))$ time.
\end{theorem}

% add the ~O(rn^{3/2}) version

Our third algorithm solves the transportation problem in the unbalanced
setting.
%with $|A| = r$ and $|B| = n$.
This time, we use the strongly polynomial uncapacitated min-cost flow algorithm
by Orlin~\cite{xxx} \note{Cite}.
The result is an $O(n^{3/2} r \polylog n)$ time algorithm for unbalanced
transportation.
This improves over the $O(n^2 \polylog n)$ time algorithm of %TODO cite
when $r = o(\sqrt{n})$.

\begin{theorem}
\label{theorem:orlin}
Let $A$ and $B$ be two point sets in the plane with $|A| = r$ and $|B| = n$ satisfying $r \le n$, with
supplies and demands given by the function $\tsupply(\cdot)$ \note{from $A\cup B$ to $\mathbb{Z}$?} such that
$\sum_{a \in A} \tsupply(a) = \sum_{b \in B} \tsupply(b)$.
An optimal transportation map can be computed in $O(n^{3/2}r\polylog n)$ time.
\end{theorem}

%TODO STILL IN PROGRESS, WE DON'T HAVE A SOLUTION YET. REMOVE IF FAILS.

%Finally, we show that an algorithm by Orlin~\cite{} for uncapacitated min-cost
%flow can be modified to give an $O(nr\polylog n)$ time algorithm for the
%unbalanced transportation problem.
%Unlike matching, vertices may be ``matched'' (transporting) to more than one
%other vertex.
%As a result, it seems possible that the search procedure reaches vertices which
%do not contribute to a new augmenting path or towards finding unreached $B$
%vertices, forcing the search to backtrack.
%Avoiding the backtracking is the main challenge for this algorithm.
%% how?

By nature of the BCP/NN oracles we use, our results generalize to any $L_p$ distances. \note{Mention Euclidean distance earlier.}


\section{Preliminaries}
\label{section:prelim}

\subsection{Matching}

Let $G$ be a bipartite graph between vertex sets $A$ and $B$ and edge set $E$,
with costs $c(v, w)$ for each edge $e$ in $E$.
We use $C \coloneqq \max_{e \in E} c(e)$, and assume that the problem is scaled such
that $\min_{e \in E} c(e) = 1$.
A \emph{matching} $M \subseteq E$ is a set of edges where no two edges share an
endpoint.
We use $V(M)$ to denote the vertices matched by $M$.
The \emph{size} of a matching is the number of edges in the set, and the
\emph{cost} of a matching is the sum of costs of its edges.
The \emph{minimum-cost partial matching problem (MPM)} asks to find a size-$k$
matching $M^*$ of minimum cost.

\subsection{Minimum-cost flow}

\note{Collect definitions into paragraphs.}

\paragraph{Network.}
For minimum-cost flow, let $G_0 = (V, E_0)$ be a directed graph with
nonnegative arc capacities $u(v, w)$ and costs $c(v, w)$ for each arc
$(v, w) \in E_0$.
We say $G_0$ is \emph{unit-capacity} if $u(v, w) = 1$ holds for all arc $(v, w)$.
Let $\fsupply: V \to \ints_{\geq 0}$ be a supply-demand function on $V$, satisfying $\sum_{v \in V} \fsupply(v) = 0$.
The positive values of $\fsupply(v)$ are referred to as \emph{supply}, and the negative values of $\fsupply(v)$ as \emph{demand}.

We augment $G_0$ to make it \emph{symmetric}
%for every arc $(v, w) \in E_0$ its reverse $(w, v)$ is also an arc;
and the costs
\emph{antisymmetric}
%$c(v, w) = -c(w, v)$).
by creating an arc $(w, v)$ for each $(v, w) \in E_0$ and define $u(w, v) = 0$ and $c(w, v) = -c(v, w)$.
Denote this set of new \emph{reverse arcs} by $E^R$.
\note{How do you feel about using darts and arcs to describe the distinction?}
From here forward, we work with the symmetric multigraph
$G = (V, E = E_0 \cup E^R)$. \note{Oh man.  This is a mouthful, use English.}
A \emph{network} $(G, c, u, \fsupply)$ is a graph $G$ augmented with arc costs, capacities, and a supply-demand function on vertices of $G$.

\paragraph{Pseudoflows.}
A \emph{pseudoflow} $f$ is an antisymmetric function on arcs \note{define codomain; integers?}
satisfying $f(v, w) \leq u(v, w)$ for all arcs $(v, w)$.
We say that $f$ \emph{saturates} an arc $e$ if $f(v, w) = u(v, w)$.
The \emph{support} of $f$ is $E_{>0}(f) \coloneqq \{(v, w) \mid f(v, w) > 0\}$.
All our algorithms will handle integer-valued pseudoflows, so in the
unit-capacity setting an arc is either saturated or has zero flow.
Given a pseudoflow $f$, we define the \emph{imbalance} of a vertex to be
\[
e_f(v) \coloneqq \fsupply(v) + \sum_{(w, v) \in E}{f(w, v)} - \sum_{(v, w) \in E}{f(v, w)}.
\]
We call positive imbalance \emph{excess} and negative imbalance \emph{deficit}; and
vertices with positive and negative imbalance \emph{excess vertices} and \emph{deficit vertices}, respectively.
A vertex is \emph{balanced} if it has zero imbalance.
If all vertices are balanced, the pseudoflow is a \emph{circulation}.
The cost of a pseudoflow is
\[
\cost(f) \coloneqq \sum_{(v, w) \in E} c(v, w) \cdot f(v, w).
\]
The \emph{minimum-cost flow problem (MCF)} asks to find the circulation $f^*$ of
minimum cost.

\paragraph{Residual network.}
For each arc $(v, w)$, the \emph{residual capacity} with respect to
pseudoflow $f$ is defined to be $u_f(v, w) \coloneqq u(v, w) - f(v, w)$.
The set of \emph{residual edges} \note{arcs?} is defined as
\[
E_f \coloneqq \{(v, w) \in E \mid u_f(v, w) > 0\}.
\]
We call $G_f = (V, E_f)$ the \emph{residual graph} with respect to pseudoflow $f$.
A pseudoflow $f'$ in $G_f$ can be ``added'' or ``augmented'' to $f$ to produce
a new pseudoflow (that is, the arc-wise addition $f + f'$ is a valid pseudoflow
in $G$).
A pseudoflow $f'$ in $G_f$ is an \emph{improving flow} if
\begin{enumerate}[(1)]\itemsep=0pt
\item
$0 \leq e_{f'}(v) \leq e_f(v)$ for all excess vertices $v$,
\item
$0 \leq -e_{f'}(v) \leq -e_f(v)$ for all deficit vertices $v$, and
%\item
% if $e_f(v) = 0$ then $e_{f'}(v) = 0$, \note{this follows from (1) or (2)} and
\item $\sum_{v \in V} |e_{f'}(v)| < \sum_{v \in V} |e_f(v)|$ holds.
\end{enumerate}
If improving flow $f'$ is on a simple path (from an excess vertex to a deficit
vertex), we call it an \emph{augmenting-path flow} \note{path flow for short? only used twice throughout the paper} and its underlying support path \note{support undefined} an
\emph{augmenting path}.
If $f'$ saturates at least one residual arc in every augmenting path in $G_f$,
we call $f'$ a \emph{blocking flow}.
In other words, for blocking flow $f'$, there is no augmenting-path flow
$f''$ in $G_f$ for which $f + (f' + f'')$ is a feasible pseudoflow in $G$.
\note{Move to Section 4.4 where blocking flow is first used.}

\subsection{Primal-dual augmentation algorithms}

The Hungarian algorithm begins with an empty matching and gradually increases its size to $k$ using \emph{alternating augmenting paths}.
Given a non-maximal matching $M$, an alternating augmenting path $P$ is a path
between an unmatched vertex $a \in A$ and unmatched $b \in B$.
\note{What is $A$ and $B$? Remind the readers about the bipartite graph again.}
Then, $M' = M \oplus P$ is a matching of size 1 greater.
\note{$\oplus$ undefined.  Personally I believe it's easier to define in words; using notation is fine too.}
By restricting alternating augmenting paths to edges \note{when do you use edges and when do you use arcs?}
which satisfy a certain
cost condition (admissibility, defined momentarily) \note{define it or don't say it}, one can prove that each
intermediate matching of size $j \leq k$ is of minimum-cost among matchings of size $j$.

There is a similar augmentation procedure for flows, which sends improving
flows (e.g. augmenting-path flows) \note{no reason to give out details that does not help with sketch of ideas} to gradually reduce the imbalance in a
pseudoflow to 0, making it a circulation. \note{imbalance of a pseudoflow is undefined; you only define it on the vertices.}
By restricting augmentations to residual arcs satisfying a certain cost
condition (admissibility), one can prove that the resulting circulation is
minimum cost.

\note{The above paragraph might be clearer if you put it after the definition of admissibility; because you can actually provide a formal proof.  Sketch of ideas are not that useful because for experts they don't need to read it, for beginners they won't understand without former definitions.}

\paragraph{LP-duality and admissability.}
Formally, the
\emph{potentials} $\pi(v)$ are the variables of the linear program dual to \note{which primal problem? State the correspodning linear problems explicitly}.
The \emph{reduced cost} of an arc $(v, w)$ in $E_f$ with respect to $\pi$ is
\[
c_\pi(v, w) \coloneqq c(v, w) - \pi(v) + \pi(w).
\]
\note{Mention that reduced costs are still antisymmetric.}
The \emph{dual feasibility constraint} is that $c_\pi(v, w) \geq 0$ holds for all
residual arcs \note{naturally, not the reverse arcs; thus the distinction between arcs and darts}; potentials which satisfy this constraint are said to be \emph{feasible}.
The linear programming \emph{optimality conditions} state that, for an optimal
circulation $f^*$, there are feasible potentials $\pi^*$ which satisfy
$c_\pi(v, w) = 0$ \note{$\pi^*$?} on all arcs with $f^*(v, w) > 0$.
We can similarly define potentials and reduced costs for matchings, using
$c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b)$ for $(a, b) \in A \times B$. \note{How about the reverse arcs?}

Suppose we relax the dual feasibility constraint to allow for a violation of
$\eps > 0$.
We say that a pseudoflow $f$ is \emph{$\eps$-optimal} \note{with respect to $\pi$} if
$c_\pi(v, w) \geq -\eps$ for all arcs $(v, w)$ in $E_f$ with $u_f(v, w) > 0$.
Note that 0-optimality coincides with the optimality conditions. \note{Careful here.  Technically it's dual feasibility.}
We say that a residual arc $(v ,w)$ satisfying $c_\pi(v, w) \leq 0$ is \emph{admissible}.
We say that an improving flow $f'$ is \emph{admissible} if $f'(v, w) > 0$
only on admissible arcs $(v, w)$.

For matchings, we say that matching $M$ is $\eps$-optimal if $c_\pi(a, b) \leq \eps$ for
$(a, b) \in M$ and $c_\pi(a, b) \geq -\eps$ $(a, b) \in E \setminus M$.
Matching edges (resp.\ nonmatching edges) are \emph{admissible} if $c_\pi(a, b) \geq 0$ (resp.\ $c_\pi(a, b) \leq 0$); and an alternating augmenting path is \emph{admissible}
if all its edges are.
For both matching and flows, 0-optimal $f$ implies the admissibility condition
is with equality, instead ($c_\pi(v, w) = 0$).

\note{I got the sense that it might be helpful to define admissibility at the start of the flow section and the matching section separately.}

%Now, we can concretely state how admissible augmentations lead to a correct algorithm for $\eps > 0$.
\begin{lemma}
	Let $f$ be an $\eps$-optimal pseudoflow in $G$ and let $f'$ be an
	admissible improving flow in $G_f$.
	Then $f + f'$ is also $\eps$-optimal.
\end{lemma}
\begin{proof}
	Augmentation by $f'$ will not change the potentials, but may introduce
	new arcs with $u_{f+f'}(v, w) > 0$.
	We will verify that these arcs satisfy the $\eps$-optimality condition.
	Such an arc $(v, w)$ must have $u(v, w) = f(v, w) > (f+f')(v, w)$,
	implying $f'(w, v) > 0$.
\note{I don't understand.  Never write a sentense that requires multiple steps to decode. I think what you meant is $u_f = 0$ thus $u=f$, and $u_{f+f'} > 0$ thus $u-(f+f') > 0$; finally $f'(v,w) > 0$ thus $f'(w,v) < 0$.}
	By assumption that $f'$ is admissible, $(w, v)$ was an admissible arc, thus
	$c_\pi(w, v) \leq 0$, implying $c_\pi(v, w) \geq 0$.
	Thus, all such arcs have $c_\pi(v, w) \geq 0 \geq -\eps$, and $f + f'$
	is $\eps$-optimal.
\end{proof}

With a similar argument, we could prove the same for matchings. \note{Proof it or cite it, at least in full version.}
Finally, we show that $\eps$-optimality is sufficient to certify that a
circulation is an approximate MCF solution, when the underlying graph is
of unit-capacity.

\begin{lemma}
\label{lemma:mcf_cost}
Let $G$ be a unit-capacity graph with $n$ vertices and $m$ arcs, let $f$ be an
$\eps$-optimal circulation in $G$, and let $f^*$ be an optimal circulation for
$G$.
Then, $\cost(f) \leq \cost(f^*) + m\eps$.
\end{lemma}

\note{AX: this may be the wrong cost analysis for approximation. would like a
stronger statement that addresses 0 cost edges (for instance)}
\begin{proof}
By the flow decomposition theorem \note{cite?}, there is a residual pseudoflow
$f'$ such that $f + f' = f^*$, and $f'$ can be decomposed into a set of unit
flows on edge-disjoint cycles in $G_f$.
The number of edges used by these cycles is at most $m$.
The cost of a residual cycle is equal to its reduced cost, since the potentials
telescope, so the cost of each $f'$ cycle $\Gamma$ is at least $-|\Gamma|\eps$
by $\eps$-optimality of $f$.
Thus, $\cost(f') \geq -m\eps$ , and therefore
$\cost(f) \leq \cost(f^*) + m\eps$.
\end{proof}

This bound can be improved if we have a better upper bound on the number of
edges used in the cycles of $f'$.


\section{Matching with the Hungarian algorithm}
\label{section:hung}

The Hungarian algorithm maintains a 0-optimal (initially empty) matching $M$,
and repeatedly augments by alternating augmenting paths of admissible edges
until $|M| = k$.
To this end, the algorithm maintains a set of feasible potentials $\pi$ and
updates them to find augmenting paths of admissible edges. \note{admissible augmenting path?}
It maintains the invariant that matching edges are admissible.
Since there are $k$ augmentations and each alternating path has length at most
$2k-1$, the total time spent on bookkeeping the matching is $O(k^2)$.
This leaves the analysis of the subroutine that updates the potentials and
finds an admissible augmenting path; we call this subrouotine the \emph{Hungarian search}.

\begin{figure*}
\centering
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Hungarian algorithm}
\begin{algorithmic}[1]
\Function{Match}{$G = (A \cup B, E)$, $k$}
	\State $M \gets \emptyset$
	\State $\pi(v) \gets 0$ for all $v \in A \cup B$
	\While{$|M| < k$}
		\State $\Pi \gets$ \Call{Hungarian-Search}{$G$, $M$, $\pi$}
		\State $M \gets M \oplus \Pi$
	\EndWhile
	\State\Return $M$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}


\begin{theorem}[Time for Hungarian algorithm]
\label{theorem:hung_orig}
Let $G = (A \cup B, A \times B)$ be an instance of geometric partial matching
with $|A| = r \geq |B| = n$ \note{$r \le n$?}, and parameter $k \leq r$.
Suppose the Hungarian search finds each augmenting path in $T(n, k)$ time after
a one-time $P(n, k)$ preprocessing time.
Then, the Hungarian algorithm finds the optimal size $k$ matching in time
$O(P(n, k) + k T(n, k) + k^2)$.
\end{theorem}

\subsection{Hungarian search}

Let $S$ be the set of vertices that can be reached from an unmatched $a \in A$
by admissible residual edges, initially the unmatched vertices of $A$.
The Hungarian search updates potentials in a Dijkstra's algorithm-like manner,
expanding $S$ until it includes an unmatched $b \in B$ (and thus an admissible
alternating augmenting path).
The ``search frontier'' of the Hungarian search is
$(S \cap A) \times (B \setminus S)$.
We \emph{relax} the edge in the frontier with minimum reduced cost, changing
the potentials of vertices $S$ such that the edge becomes admissible, and adding the head of the edge into $S$. \note{Well, either you add both endpoints into $S$, or an admissible augmenting path is found.}

The potential update uniformly decreases the reduced costs of the frontier
edges.
Since $(a', b')$ is the minimum reduced cost frontier edge, the potential
update in line~\ref{line:hs_update} does not make any reduced cost negative,
and thus preserves the dual feasibility constraint for all edges.
The algorithm is shown below as Algorithm~\ref{algorithm:hung_hs}.

\begin{figure*}
\centering
\begin{minipage}{.8\linewidth}
\begin{algorithm}[H]
\caption{Hungarian Search (matching)}
\label{algorithm:hung_hs}
\begin{algorithmic}[1]
\Require{$c_\pi(a, b) = 0$ for all $(a, b) \in M$}
%\Statex %newline
\Function{Hungarian-Search}{$G = (A \cup B, E)$, $M$, $\pi$}
	\State $S \gets a \in (A \setminus V(M))$ \note{arbitrary $a$?}
	\Repeat
		\State $(a', b') \gets \argmin\{c_\pi(a, b) \mid (a, b) \in (S \cap A) \times (B \setminus S)\}$
		\State $\gamma \gets c_\pi(a', b')$
		\State $\pi(v) \gets \pi(v) + \gamma, \forall v \in S$
			\Comment{make $(a', b')$ admissible}
			\label{line:hs_update}
		\State $S \gets S \cup \{b'\}$
		% \Statex %newline
		\If{$b' \not\in V(M)$} \Comment{$b'$ unmatched}
			\State $\Pi \gets$ alternating augmenting path from $S$ to $b'$ \note{not uniquely defined; do you mean from $a$?}
			\State\Return $\Pi$
		\Else \Comment{$b'$ is matched to some $a'' \in A \cap V(M)$}
			\State $S \gets S \cup \{a''\}$
		\EndIf
	\Until{$S = A \cup B$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

By tracking the forest of relaxed edges (e.g. back pointers), it is
straightforward to recover the alternating augmenting path $\Pi$ once we reach
an unmatched $b' \in B$.
We make the following observation about the Hungarian search:

\begin{lemma}
\label{lemma:hungsearch_length}
There are at most $k$ edge relaxations before the Hungarian search finds an
alternating augmenting path.
\end{lemma}

\begin{proof}
Each edge relaxation either leads to a matched vertex in $B$ (there are at most
$k-1$ such vertices), or finds an unmatched vertex and ends the search.
\end{proof}

In general graphs, the minimum edge is typically found by pushing all
encountered $(S \cap A) \times (B \setminus S)$ edges into a priority queue.
However, in the bipartite complete graph, this may take $\Theta(rn\polylog n)$
time for each Hungarian search --- edges are being pushed into the queue even when
they are not relaxed.
We avoid this problem by finding an edge with minimum cost using \emph{bichromatic
closest pair} (BCP) queries on an additively weighted Euclidean distances,
for which there exist fast %(poly-logarithmic query and update)
dynamic data structures.
Given two point sets
$P$ and $Q$ in the plane,
the task for BCP problem is to find two points $p \in P$ and $q \in Q$ minimizing the (adjusted) distance
$\|p - q\| - \omega(p) + \omega(q)$, for some real-valued vertex weights
$\omega(p)$.
In our setting, the vertex weights will be set as the potentials; the corresponding adjusted distance then will be the reduced costs.

\note{Short history on BCP?}
The state of the art dynamic BCP data structure from Kaplan, Mulzer,
Roditty, Seiferth, and Sharir~\cite{KMRSS17} supports point insertions and deletions in
$O(\polylog n)$ time, and answers queries in $O(\log^2 n)$ time.
The following lemma, combined with Theorem~\ref{theorem:hung_orig}, completes
the proof of Theorem~\ref{theorem:hung}.

\begin{lemma}
\label{lemma:hs_time}
Using the dynamic BCP data structure from Kaplan \etal, we can implement
Hungarian search with $T(n, k) = O(k\polylog n)$ and
$P(n, k) = O(n\polylog n)$.
\end{lemma}

\begin{proof}
Recall that we maintain a BCP data structure between $P = (S \cap A)$ and
$Q = (B \setminus S)$.
Changes to the $P$ and $Q$ are entirely driven by changing $S$; that is,
updates to $S$ incur BCP insertions/deletions.
We first analyze the bookkeeping besides the potential updates, and then
show how potential updates can be implemented efficiently.

\begin{enumerate}
\item Let $S^t_0$ \note{Is there a reason why you want the subscript?  Do you ever define $S^t$?} denote the initial set $S$ at the beginning of the
	$t$-th Hungarian search, that is, the set of unmatched points in $A$
	after $t$ augmentations.
	At the very beginning of the Hungarian algorithm, we initialize
	$S^0_0 \gets A$ (meaning that $P = A$ and $Q = B$), which is a
	one-time insertion of $O(n)$ points into BCP, attributed to $P(n, k)$.
	On each successive Hungarian search, $S^t_0$ shrinks as more
	and more points in $A$ are matched.
	Assume for now that, at the beginning of the $(t+1)$-th
	Hungarian search, we are able to construct $S^t_0$ from the
	previous iteration.
	To construct $S^{t+1}_0$, we simply remove the $A$ point \note{point in $A$} that
	was matched by the $t$-th augmenting path.
	Thus, with that assumption, we are able to initialize $S$ using
	one BCP deletion operation per augmentation.

\item During each Hungarian search, points are added to $P$ (that is, some points in $A$ are
	added to $S$) and removed from $Q$ (points in $B$ added to $S$), which will happen at most once per edge relaxation.
	By Lemma~\ref{lemma:hungsearch_length} the number of relaxed
	edges is at most $k$, so the number of such BCP operations is
	also at most $k$.

\item To obtain $S^t_0$, we keep track \note{give a name to such points} of the
	points added since $S^t_0$ in the last Hungarian search
	(i.e.\ those of (2)). \note{Unclear}
	After the augmentation, we use this log \note{use the name} to delete the added
	vertices from $S$ and recover $S^t_0$.
	By the argument in (2) there are $O(k)$ of such points to
	delete, so reconstructing $S^t_0$ takes $O(k)$ BCP operations.
	\note{TODO instead of reversing a log, is persistence an easier solution to this?}
\end{enumerate}

We spend $P(n, k) = O(n \polylog n)$ time to build the initial BCP.
The number of BCP operations associated with each Hungarian search is
$O(k)$, so the time spent on BCP operations in each Hungarian search
is $O(k \polylog n)$.

As for the potential updates, we modify a trick from Vaidya~\cite{Vaidya89} to
batch potential updates.
Potentials have a \emph{stored value}, i.e. the current value of $\pi(v)$,
and a \emph{true value}, which may have changed from $\pi(v)$.
The algorithm uses the true value when dealing with reduced costs and updates
the stored value rarely; we explain the mechanism shortly.

Throughout the course of the algorithm, we maintain a nonnegative value
$\delta$ (initially 0) which aggregates potential changes.
Vertices that are added to $S$ are immediately added to a BCP data structure
with weight $\omega(p) \gets \pi(p) - \delta$, for whatever value $\delta$ is
at the time of insertion.
When the points of $S$ have potentials increased by $\gamma$ in (2), we instead
raise $\delta \gets \delta + \gamma$.
Thus, true value for any potential of a point in $S$ is $\omega(p) + \delta$.
For points of $(A \cup B) \setminus S$, the true potential is equal to the
stored potential.

Since potentials for $S$ points are uniformly offsetted by $\delta$, the
minimum edge returned by the BCP oracle does not change.
Once a point is removed from $S$, we update its stored potential
to be $\pi(p) \gets \omega(p) + \delta$, for the current value of $\delta$.
Importantly, $\delta$ is not reset at the end of a Hungarian search, and
persists throughout the entire algorithm.
This way, the unmatched points in each $S^t_0$ have their true potentials
accurately represented by $\delta$ and $\omega(p)$.

The number of updates to $\delta$ is equal to the number of edge relaxations,
which is $O(k)$ per Hungarian search.
We update stored potentials when removing a point from $S$ (by the rewind
mechanism, or due to an augmentation) which occurs $O(k)$ times per Hungarian
search.
The time spent on potential updates per Hungarian search is therefore $O(k)$.
Overall, the time spent per Hungarian search is $T(n, k) = O(k\polylog n)$.

\note{The proof gets more handwavy as the paragraph progresses.  Consider a revision after this round.}
\end{proof}


\section{Matching with the Goldberg~{\etal} algorithm}
\label{section:goldberg}

\note{Remind the readers what you want to achieve in this section.}

The basis of the algorithm in this section is a \emph{cost-scaling} algorithm \note{try not to repeat words}
for unit-capacity min-cost flow from \cite{GHKT17} \note{don't use citation as a noun}.
Before describing the algorithm, we first give a linear-time reduction from
min-cost partial matching to unit-capacity min-cost flow, which allows us to apply the
Goldberg~{\etal} algorithm \note{never defined} to partial matching.

\subsection{MPM to unit-capacity MCF reduction}
\label{subsection:mcm_mcf_reduction}

For a partial matching problem on a bipartite graph $G = (A \cup B, E_0)$ with parameter $k$, we
direct each bipartite edge in $E_0$ from $A$ to $B$, with cost equal to the
original cost $c(a, b)$ and capacity $1$.
Next, we add a dummy vertex $s$ with arcs $(s, a)$ to every vertex $a$ in $A$,
and a dummy vertex $t$ with arcs $(b, t)$ for every vertex $b$ in $B$,
all with cost $0$ and capacity $1$.
For each of the above arcs $(v, w)$, we also add a reverse arc $(w, v)$ with
cost $c(w, v) = -c(v, w)$ and capacity $0$. \note{is this consistent with the other residual graph description?}
Let the complete set of arcs be $E$, and $V = A \cup B \cup \{s, t\}$.
Set $\fsupply(s) = k$, $\fsupply(t) = -k$, and $\fsupply(v) = 0$ for all other
vertices. \note{What is $\phi$?  Undefined in this section.}
Let the resulting graph be $H = (V, E)$. \note{Do you mean network, as you have defined cost, capacity, and supply-demand function?}

\begin{observation}
\label{observation:dag}
	The arcs of $H$ with positive capacity form a directed acyclic graph.
\end{observation}

In other words, there will be no cycles of positive flow in circulations on
$H$.
With this, we can show that the number of arcs used by any integer pseudoflow
in $H$ is asymptotically bounded by the excess of the pseudoflow.

\begin{lemma}
\label{lemma:reduction_count}
Let $f$ be an integer pseudoflow in $H$ with $O(k)$ excess.
Then, $|E_{>0}(f)| = O(k)$.
\end{lemma}

\begin{proof}
By Observation~\ref{observation:dag}, the positive-flow edges of $f$ do not
contain a cycle.
Thus, $E_{>0}(f)$ can be decomposed into a set of inclusion-maximal paths,
each of which creates a single unit of excess if it does not terminate at $t$.
A path may also create excess at $t$ if there are at least $k$ other paths
terminating at $t$.
By assumption, there are $O(k)$ units of excess to which we can associate
paths, and at most $k$ paths that we cannot associate with a unit of excess.
The maximum length of any path with positive-flow arcs in $H$ is $3$ by
construction.
We conclude that the number of positive flow arcs in $f$ is $O(k)$.
\end{proof}

It is straightforward to show that any integer circulation on $H$ uses exactly
$k$ of the $A$-to-$B$ arcs, which correspond to the edges of a size $k$ \note{size-$k$}
matching.
For a circulation $f$ in $H$, we use $M_f$ to denote the
corresponding matching.
Observe that $\cost(f) = \cost(M_f)$, so an $\alpha$-approximation to the MCF
problem on $H$ is an $\alpha$-approximation to the matching problem on $G$.

We can improve the additive error bound in Lemma~\ref{lemma:mcf_cost} on $H$.
Note that for integer-valued capacities (in particular, unit capacity), there is always an
integer-valued optimal circulation.

\note{Move the definitions of $\eps$-optimality and admissibility for flows here?}

\begin{lemma}
\label{lemma:goldberg_cost_add}
Let $f$ an $\eps$-optimal integer circulation in $H$, and $f^*$ an optimal
integer circulation for $H$.
Then, $\cost(f) \leq \cost(f^*) + 6k\eps$.
\end{lemma}

\begin{proof}
We label the arcs of $H_f$ \note{what is $H_f$? residual network?} as follows:
\begin{itemize}\itemsep=0pt
\item \emph{forward arcs}: arcs directed from
$s \to A$ or $A \to B$ or $B \to t$, and \item \emph{reverse arcs}: arcs point in the opposite
directions.
\end{itemize}
Observe that a residual cycle $\Gamma$ \note{directed, by definition} must have exactly half of its edges being
reverse arcs.
The reverse arcs may either be
\begin{enumerate}[(i)]\itemsep=0pt
\item on one of the $M_f$ edges \note{what does this mean? the underlying edge is in $M_f$?},
\item between $s$ and $A$, or
\item between $B$ and $t$.
\end{enumerate}
If it is of type (ii) or (iii), there is an adjacent type (i) reverse arc.
Thus, we can charge the reverse arcs of $\Gamma$ to $M_f \cap \Gamma$ edges \note{now this is notation abuse; $\Gamma$ cannot both be an edge set and an arc set}
with at most 3 charges per edge in $M_f \cap \Gamma$.
We can then charge all arcs of $f' = (f^* - f) = \sum \Gamma_i$ \note{cannot parse; just arcs in the difference of $f^*$ and $f$?} to $M_f$ with
at most 6 charge per edge in $M_f$.
As $|M_f| = k$, the number of arcs in $f'$ is at most $6k$.
The rest of the argument proceeds as in Lemma~\ref{lemma:mcf_cost}. \note{State at the start of the lemma what property do you need to prove in order to apply the black box.  Even better, rewrite Lemma 2 the take care of both situation assuming some bounds, and derive two corollaries from it.}
\end{proof}

Suppose we scaled arc costs (via uniform scaling of the input points) such that
the minimum cost (closest pair distance) is 1. \note{You already made this assumption in Section~2.1; recall preliminaries.}
Then, $\cost(f^*) \geq k$, and we can turn Lemma~\ref{lemma:goldberg_cost_add}
into a relative approximation.

\note{I am not sure if you want to state the assumption again in the statements of the corollaries, since this assumption is something you can make when constructing $H$.}

\begin{corollary}
\label{corollary:flow_approx}
Let $f$ an $\eps$-optimal integer circulation in $H$, and $f^*$ an optimal
integer circulation for $H$.
Suppose costs are scaled to be at least $1$ on all arcs.
Then, $\cost(f) \leq (1 + 6\eps) \cost(f^*)$.
\end{corollary}

\begin{corollary}
\label{corollary:match_approx}
Let $f$ an $(\eps'/6)$-optimal integer circulation in $H$, and $M^*$ an optimal
size $k$ matching of $G$.
Suppose costs are scaled to be at least $1$ on all arcs.
Then, $\cost(M_f) \leq (1 + \eps') \cost(M^*)$.
\end{corollary}

\note{Where do you use these two corollaries?}

In other words, a $(\eps'/6)$-optimal circulation is sufficient for a
$(1 + \eps')$-approximate matching.

\note{Put the spread reduction here?}

\subsection{Algorithm description}

The Goldberg~{\etal}~\cite{GHKT17} algorithm is based on \emph{cost-scaling} or
\emph{successive approximation}, originally due to Goldberg and
Tarjan~\cite{GT90}.
The algorithm finds $\eps$-optimal circulations for geometrically shrinking
values of $\eps$.
Each period \note{what's a period?} where $\eps$ holds constant is called a \emph{scale}.
Once $\eps$ is sufficiently small, the $\eps$-optimal flow is a suitable
approximation (or even an optimal flow itself when costs are integers)~\cite{GT90,GHKT17}.
We present this algorithm as an approximation is because costs in the geometric
partial matching settings (with respect to Euclidean distances) are generally not integers.

\begin{figure*}[h]
\centering
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Cost-Scaling MCF}
\begin{algorithmic}[1]
\Function{MCF}{$H$, $\eps^*$}
	\State $\eps \gets kC$,
	$f \gets 0$,
	$\pi \gets 0$
	\While{$\eps > \eps^*/6$}
		\State $(f, \pi) \gets$ \Call{Scale-Init}{$H$, $f$, $\pi$}
		\State $(f, \pi) \gets$ \Call{Refine}{$H$, $f$, $\pi$}
		\State $\eps \gets \eps/2$
	\EndWhile
	\State\Return $f$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Note that the zero flow is trivially $kC$-optimal for $H$.
At the beginning of each scale, \note{it feels weird to use "scale" this way; "scale" sounds like the manitute of $\eps$.  How about "iteration"?} \textsc{Scale-Init} takes the previous
circulation (now $2\eps$-optimal) and transforms it into an $\eps$-optimal
pseudoflow with $O(k)$ excess.
For the rest of the scale, in \textsc{Refine} \note{undefined}, reduces the excess in the newly constructed
pseudoflow to zero, making it an $\eps$-optimal circulation.
The bulk of this section will be the descriptions and analysis of \textsc{Scale-Init} and
\textsc{Refine}.
\note{Mention Algorithm 3 somehow, to justify the placement of the pseudocode.}

First we analyze the number of scales (iterations of the outer loop).
Initially $\eps = kC$, and the algorithm stops once $\eps$ is at most $\eps^*/6$.
Thus, the number of scales is $O(\log(kC/\eps^*))$.
For the geometric matching problem, there is a simple way to preprocess the point
set such that $C = O(n^2)$, effectively, by Sharathkumar and Agarwal~\cite{SA12}.
\note{With that assumption that the minimum distance is at least $1$?}
This gives us $O(\log(n/\eps^*))$ scales as a result.
We briefly describe this preprocessing step at the end of this section. \note{where? do it here if needed.}

\begin{lemma}
\label{lemma:goldberg_scales}
The cost-scaling algorithm finds a $(1 + \eps^*)$-approximate matching after
$O(\log(n/\eps^*))$ scales.
\end{lemma}

\subsection{\textsc{Scale-Init}}

The procedure is described in Algorithm~\ref{algorithm:scale_init}.

\begin{figure*}[h]
\centering
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Scale Initialization}
\label{algorithm:scale_init}
\begin{algorithmic}[1]
\Function{Scale-Init}{$H$, $f$, $\pi$}
	\State $\forall a \in A, \pi(a) \gets \pi(a) + \eps$
	\State $\forall b \in B, \pi(b) \gets \pi(b) + 2\eps$
	\State $\pi(t) \gets \pi(t) + 3\eps$
	%\Statex %newline
	\ForAll{$(v, w) \in E_{>0}(f)$}
		\If{$c_\pi(w, v) < -\eps$}
			\State $f(v, w) \gets 0$
		\EndIf
	\EndFor
	\State\Return $(f, \pi)$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Let the $H_f$ arcs directed from $s \to A$ or $A \to B$ or $B \to t$ be
\emph{forward arcs}, and let those in the opposite directions be
\emph{reverse arcs}.
\note{Already did so in Lemma 4.3; might want to move this definition to somewhere before lemma 4.3.}
The first four lines \note{try not use use specific numbers, in case you change the pseudocode later} of \textsc{Scale-Init} raise the reduced cost of each
forward arc by $\eps$, therefore making all forward arcs $\eps$-optimal.
\note{Instead of an example, mention that at the start of the iteration, every forward arc is $2\eps$-optimal.}
% For example, a forward arc of $A \to B$ now has reduced cost
% \begin{equation*}
% 	c(a, b) - (\pi(a) + \eps) + (\pi(b) + 2\eps)
% 	= c_\pi(a, b) + \eps
% 	\geq -2\eps + \eps
% 	= -\eps.
% \end{equation*}
In the lines after \note{the for-loop}, we deal with the reduced cost of reverse arcs by simply
de-saturating them if they violate $\eps$-optimality.
Note that forward arcs will not be de-saturated in this step, since they are
now $\eps$-optimal.

\begin{lemma}
\label{lemma:scale_init}
\textsc{Scale-Init} turns a $2\eps$-optimal circulation into an
$\eps$-optimal pseudoflow with $O(k)$ excess in $O(n)$ time.
\end{lemma}

\begin{proof}
The potential updates affect every vertex except $s$, so this takes $O(n)$
time.
As for the arc de-saturation, every reverse arc is induced by positive flow on
a forward arc, and the number of positive flow edges in $f$ is $O(k)$ by
Lemma~\ref{lemma:reduction_count}.
The total number of edges examined by the loop is $O(k)$.
In total, this takes $O(n)$ time.

%For the amount of excess,
Notice that new excess vertex is only created due to the
de-saturation of reverse arcs.
Because the arcs in the graph has unit capacity, each de-saturation creates one unit of
excess.
By Lemma~\ref{lemma:reduction_count}, there are $|E_{>0}(f)| = O(k)$ reverse
arcs, so the total excess created must be $O(k)$.
\end{proof}


\subsection{\textsc{Refine}}

\textsc{Refine} is implemented using a primal-dual augmentation algorithm,
which sends improving flows on admissible edges like the Hungarian algorithm.
Unlike the Hungarian algorithm, it uses blocking flows instead of augmenting
paths.

\begin{figure*}[ht]
\centering
\begin{minipage}{.8\linewidth}
\begin{algorithm}[H]
\caption{Refinement}
\begin{algorithmic}[1]
\Function{Refine}{$H = (V, E)$, $f$, $\pi$}
	\While{$\sum_{v \in V} |e_f(v)| > 0$}
		\State $\pi \gets$ \Call{Hungarian-Search2}{$H$, $f$, $\pi$}
		\State $f' \gets$ \Call{DFS}{$H$, $f$, $\pi$}
			\Comment{$f'$ is an admissible blocking flow}
		\State $f \gets f + f'$
	\EndWhile
	\State\Return $(f, \pi)$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Using the properties of blocking flows and the unit-capacity input graph,
Goldberg~{\etal} \cite{xxx} \note{cite} prove that there are $O(\sqrt{k})$ blocking flows before
excess becomes 0.
\note{This is not honest, I think. There is a derivation almost identical to theirs, but they are using a slightly different reduction graph.}
\note{HC: I am not familiar enough with their algorithm; do you think it's a straightforward application of the technique, or are there something subtle that requires a complete proof?}

\begin{lemma}[Goldberg~{\etal}~{\cite[Lemma~3.11 and \S{6}]{GHKT17}}]
\label{lemma:goldberg_refine_iterations}
Let $f$ be a pseudoflow in $H$ with $O(k)$ excess.
There are $O(\sqrt{k})$ blocking flows before excess is 0.
\end{lemma}

We can combine this with Lemma~\ref{lemma:reduction_count} to argue that the
amount of time spent updating the flow within \textsc{Refine} is
$O(k\sqrt{k})$.

Each step of \textsc{Refine} finds an admissible blocking flow in two stages.
\begin{enumerate}
\item A \emph{Hungarian search}, which updates duals in a Dijkstra-like
	manner until there is an excess-deficit path of admissible edges.
	There are slight differences from the Hungarian algorithm's ``Hungarian
	search,'' but the final running time is identical. \note{In your paper there are only two.  So just mention that this one is different from the one for matchings.}
	We call the procedure \textsc{Hungarian-Search2} to distinguish.

\item A \emph{depth-first search} (\textsc{DFS}) through the set of admissible
	edges to construct an admissible blocking flow.
	It suffices to repeatedly extract admissible augmenting paths until
	no more admissible excess-deficit paths remain.
	By definition, the union of such paths is a blocking flow.
\end{enumerate}
For \textsc{Hungarian-Search2}, we again use a dynamic BCP data structure to
accelerate the Hungarian search after a once-per-\textsc{Refine} preprocessing.
To perform \textsc{DFS} quickly, we can use a dynamic \emph{nearest-neighbor}
(NN) data structure, to discover admissible edges without handling the set of
admissible edges explicitly.
This is applied in a similar way as the BCP is for Hungarian search.

\begin{lemma}
\label{lemma:goldberg_refine_time}
Suppose \textsc{Hungarian-Search2} can be implemented in $T_1(n, k)$ time after
a once-per-\textsc{Refine} $P_1(n, k)$ time preprocessing, and
\textsc{DFS} can be implemented in $T_2(n, k)$ time after $P_2(n, k)$ preprocessing.
Then, \textsc{Refine} can be implemented in time
\[
O(P_1(n, k) + P_2(n, k) + \sqrt{k}T_1(n, k) + \sqrt{k}T_2(n, k) + k\sqrt{k}).
\]
\end{lemma}

As we will show shortly (Lemmas~\ref{lemma:goldberg_hs_time} and \ref{lemma:goldberg_dfs_time}), the total running time for \textsc{Refine} is
$O((n + k\sqrt{k})\polylog n)$.
Combining with Lemmas~\ref{lemma:goldberg_scales} and \ref{lemma:scale_init}
completes the proof of Theorem~\ref{theorem:gmcm}.
At a high level, our analysis strategy is to charge relaxation events in
the search to the arcs in $E_{>0}(f)$.
We first extend Lemma~\ref{lemma:reduction_count} to bound the size of
$E_{>0}(f)$ throughout \textsc{Refine}, by observing that the amount of excess
decreases in each iteration of \textsc{Refine}.

\begin{corollary}
\label{corollary:support_size_during}
Let $f$ be the pseudoflow before or after any iteration of \textsc{Refine}.
Then, $|E_{>0}(f)| = O(k)$.
\end{corollary}

% \subsubsection{Hungarian search}
%
% \begin{figure*}
% \centering
% \begin{minipage}{.8\linewidth}
% \begin{algorithm}[H]
% \caption{Hungarian Search (cost-scaling)}
% \begin{algorithmic}[1]
% \Function{Hungarian-Search2}{$H = (V, E)$, $f$, $\pi$}
% 	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
% 	\Repeat
% 		\State $\Pi \gets \argmin\{c_\pi(\Pi) \mid \text{$\Pi$ non-empty edge or empty 2-/3-path leaving $S$}\}$
% 			\label{line:hs_relaxation}
% 		\State $\gamma \gets c_\pi(\Pi)$
% 		\If{$\gamma > 0$}
% 			\Comment{make $\Pi$ admissible if it isn't}
% 			\State $\pi(v) \gets \pi(v) + \gamma\lceil\frac{\gamma}{\eps}\rceil, \forall v \in S$
% 		\EndIf
% 		\State $S \gets S \cup \Pi$
% 		\Statex %newline
% 		\State Let $\Pi = v_1, \ldots, v_\ell$
% 		\If{$e_f(v_\ell) < 0$} \Comment{$\Pi$ reached a deficit}
% 			\State\Return $(f, \pi)$
% 		\EndIf
% 	\Until{$S = (A \cup B)$}
% 	\State\Return failure
% \EndFunction
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \end{figure*}

\note{Move to the start of next subsubsection?}
As it turns out, there are some vertices whose relaxation events we cannot
charge to the support size.
However, we can replace $H_f$ with an equivalent graph without them \note{who? the empty vertices of course, but here the readers have yet to see them, so the proof sketch is not super helpful},
and run \textsc{Hungarian-Search2} and \textsc{DFS} on the resulting graph.
We describe these vertices and their properties, before describing
\textsc{Hungarian-Search2} and \textsc{DFS}.

\subsubsection{Empty vertices and the shortcut graph}

We say $v \in A \cup B$ is an \emph{empty vertex} if $e_f(v) = 0$ and no edges
of $E_{>0}(f)$ adjoin $v$. \note{Hmm. How do you feel about calling them "irrelavant vertices" or "null vertices"?}
We are unable to charge relaxation steps involving empty vertices to
$|E_{>0}(f)|$, so the algorithm must deal with them separately.
Namely, there is no edge with $f(e) > 0$ adjacent to an empty vertex,
reaching an empty vertex does not terminate the search, and there may be
$\Omega(n)$ empty vertices at once (consider $H_{f = 0}$ \note{notation overload}, the residual graph
of the empty flow).
We use $A_\emptyset$ and $B_\emptyset$ to denote the empty vertices of $A$ and
$B$ respectively.
Vertices that are not empty are called \emph{non-empty vertices}.

For an empty vertex $v$, either residual in-degree ($v \in A_\emptyset$) or
residual out-degree ($v \in B_\emptyset$) is 1.
Call a length 2 paths through $v$ to/from non-empty vertices an
\emph{empty 2-path}.
For example, if $v \in A_\emptyset$ (resp. $v \in B_\emptyset$), then its empty
2-paths have the form $(s, v, b)$ (resp. $(a, v, t)$) for each
$b \in B \setminus B_\emptyset$ (resp. $a \in A \setminus A_\emptyset$).
We say that $(s, v, b)$ is an empty 2-path \emph{surrounding} empty vertex $v$.
Separately, we define the length 3 $s$-$t$ paths that pass through two empty
vertices to be \emph{empty 3-paths}.
As with 2-paths, we say an empty 3-path $(s, v_1, v_2, t)$ \emph{surrounds}
$v_1 \in A_\emptyset$ and $v_2 \in B_\emptyset$.

As for the costs of empty paths, consider an empty 2-path $(s, v, b)$ that
surrounds $v \in A_\emptyset$.
Because reduced costs telescope for residual paths, the reduced cost of
$(s, v, b)$ does not depend on the potential of $v$.
\begin{equation*}
	c_\pi((s, v, b)) = c_\pi(s, v) + c_\pi(v, b) = c(v, b) - \pi(s) + \pi(b)
\end{equation*}
Something similar holds for empty 2-paths surrounding $B_\emptyset$ vertices,
and empty 3-paths.

We construct the \emph{shortcut graph} $\tilde{H}_f$ from $H_f$ by removing all
empty vertices and their adjacent edges, and then inserting a direct arc
between the end points of each empty path $\Pi$ of equal cost.
We call this direct edge the \emph{shortcut} $\short(\Pi)$ of empty path $\Pi$.
For example, the empty 2-path $(s, v, b)$ for $v \in A_\emptyset$ is replaced
with a shortcut $(s, b)$ of cost $c(\short(s, v, b)) \coloneqq c(v, b)$.
Similarly, the empty 3-path $(s, v_1, v_2, t)$ would be replaced with a
shortcut $(s, t)$ of cost $c(\short((s, v_1, v_2 t))) \coloneqq c(v_1, v_2)$.

The resulting multigraph $\tilde{H}_f$ contains only the non-empty vertices of
$V$, and has the same connectivity between non-empty vertices as $H_f$.
Consider a path $\Pi$ from non-empty $v$ to non-empty $w$ in $H_f$.
Any empty vertex in $\Pi$ is surrounded by an empty 2- or 3-path contained
in $\Pi$, since the only nontrivial residual paths through an empty vertex are
its surrounding empty paths.
Thus, there is a corresponding $v$-to-$w$ path $\tilde{\Pi}$ in $\tilde{H}_f$
by replacing each empty path contained in $\Pi$ with its shortcut.
Furthermore, we have $c(\Pi) = c(\tilde{\Pi})$.
We argue now that $\tilde{H}_f$ is fine as a surrogate for $H_f$, by showing
that we can recover $\eps$-optimal potentials for the non-empty vertices.

\begin{lemma}
\label{lemma:empty_correct}
Let $\tilde{\pi}$ be a $\eps$-optimal set of potentials for non-empty
vertices of $H_f$.
Construct potentials $\pi$, extending $\tilde{\pi}$ to empty vertices, by
setting $\pi(a) \gets \tilde{\pi}(s)$ for $a \in A_\emptyset$ and
$\pi(b) \gets \tilde{\pi}(t)$ for $b \in B_\emptyset$.
Then,
\begin{enumerate}
\item $\pi$ is a set of $\eps$-optimal potentials for $H_f$, and
\item if a shortcut $\short(\Pi)$ is admissible under $\tilde{\pi}$,
	then every arc of $\Pi$ is admissible under $\pi$.
\end{enumerate}
\end{lemma}

\begin{proof}
Reduced costs for non-empty to non-empty arcs are unchanged between
$\tilde{\pi}$ and $\pi$, so $\eps$-optimality are preserved for these.
Recall that an empty path is comprised of one $A$-to-$B$ arc, and 1 or 2
zero-cost arcs (connecting the empty vertex/vertices to $s$ and $t$).
With our choice of empty vertex potentials, we observe that the zero-cost arcs
have reduced cost 0:
for an empty $a \in A_\emptyset$, $c_\pi(s, a) = 0$, for an empty
$b \in B_\emptyset$, $c_\pi(b, t) = 0$.
These arcs are both $\eps$-optimal ($\geq -\eps$) and admissible ($\leq 0$), so
it remains to prove $\eps$-optimality and admissibility for arcs $(a, b)$ where
either $a$ or $b$ is an empty vertex.

Let $(a, b) \in A \times B$ such that at least one of $a$ or $b$ is empty.
There exists an empty path $\Pi$ that contains $(a, b)$.
Observe that $c_\pi(a, b) = c_\pi(\Pi)$,
which we can prove for all varieties of empty paths.
\begin{itemize}
\item If $\Pi = (s, a, b)$ for $a \in A_\emptyset$:
	\begin{equation*}
	c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b) = c(a, b) - \pi(s) + \pi(b) = c_\pi(\Pi)
	\end{equation*}
\item If $\Pi = (a, b, t)$ for $b \in B_\emptyset$:
	\begin{equation*}
	c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b) = c(a, b) - \pi(a) + \pi(t) = c_\pi(\Pi)
	\end{equation*}
\item If $\Pi = (s, a, b, t)$ for $a \in A_\emptyset$ and $b \in B_\emptyset$:
	\begin{equation*}
	c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b) = c(a, b) - \pi(s) + \pi(t) = c_\pi(\Pi)
	\end{equation*}
\end{itemize}
By construction, $c_\pi(\Pi) = c_{\tilde{\pi}}(\short(\Pi))$, so we have
$c_\pi(a, b) = c_{\tilde{\pi}}(\short(\Pi)) \geq -\eps$ and $(a, b)$ is
$\eps$-optimal.
Additionally, if $\short(\Pi)$ is admissible under $\tilde{\pi}$, then so is
$(a, b)$ under $\pi$.
Empty paths cover all arcs adjoining empty vertices, so we have proved both
parts of the lemma for all arcs in $H_f$.
\end{proof}

In \textsc{Refine}, we do not explicitly construct $\tilde{H}_f$ for running
\textsc{Hungarian-Search2} or \textsc{DFS}, but query its edges using BCP/NN
oracles and min/max heaps on elements of $H_f$.
Potentials for empty vertices are only required at the end of \textsc{Refine}
(for the next scale), and right before an augmentation sends flow through an
empty path, making its surrounded vertices non-empty.
During these occasions, we use the procedure in Lemma~\ref{lemma:empty_correct}
to find feasible, $\eps$-optimal potentials for empty vertices which
also preserve the structure of admissibility.

\begin{lemma}
\label{lemma:empty_updates}
The number of end-of-\textsc{Refine} empty vertex potential updates is $O(n)$.
The number of augmentation-induced empty vertex potential updates in each
invocation of \textsc{Refine} is $O(\sum_i N_i)$ where $N_i$ is the number
of positive flow arcs in the $i$-th blocking flow.
\end{lemma}

\begin{proof}
The number of end-of-\textsc{Refine} potential updates is $O(n)$.
Each update due to flow augmentation involves a blocking flow sending positive
flow through an empty path, causing a potential update on the surrounded
empty vertex.
We charge this potential update to the edges of that empty path, which are in
turn arcs with positive flow in the blocking flow.
For each blocking flow, no positive arc is charged more than twice.
It follows that the number of augmentation-induced updates is $O(N_i)$ for the
$i$-th blocking flow, and $O(\sum_i N_i)$ over the course of \textsc{Refine}.
\end{proof}

Ultimately, we prove that $\sum_i N_i = O(k\sqrt{k})$, but this requires that
we explain the process creating each blocking flow.
We revisit this lemma after analyzing \textsc{DFS}.

\subsubsection{Hungarian search}

\begin{figure*}
\centering
\begin{minipage}{.8\linewidth}
\begin{algorithm}[H]
\caption{Hungarian Search (cost-scaling)}
\begin{algorithmic}[1]
\Function{Hungarian-Search2}{$H = (V, E)$, $f$, $\pi$}
	\State $\tilde{H}_f \gets$ the shortcut graph of $H_f$
	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
	\Repeat
		\State $(v', w') \gets \argmin\{c_\pi(v', w') \mid v' \in S, w' \not\in S, (v', w') \in \tilde{H}_f)\}$
			\label{line:hs_relaxation}
		\State $\gamma \gets c_\pi(v', w')$
		\If{$\gamma > 0$}
			\Comment{make $(v', w')$ admissible if it isn't}
			\State $\pi(v) \gets \pi(v) + \gamma\lceil\frac{\gamma}{\eps}\rceil, \forall v \in S$
		\EndIf
		\State $S \gets S \cup \{w'\}$
		\If{$e_f(w') < 0$} \Comment{reached a deficit}
			\State\Return $(f, \pi)$
		\EndIf
	\Until{$S = (A \setminus A_\emptyset) \cup (B \setminus B_\emptyset)$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Logically, we are executing the Hungarian search (``raise prices'') from
\cite[Section 3.2]{GHKT17} on the shortcut graph $\tilde{H}_f$.
We describe how we can query the minimum-reduced cost arc leaving $S$ in
$O(\polylog n)$ time, for the shortcut graph, without constructing
$\tilde{H}_f$ explicitly.
For this purpose, let $S'$ be a set of ``reached'' vertices maintained
alongside $S$, identical except whenever a shortcut is relaxed, we add its
surrounded empty vertices to $S'$ in addition to its (non-empty) endpoints.
Observe that the arcs of $\tilde{H}_f$ leaving $S$ fall into $O(1)$ categories.
\begin{enumerate}
\item Non-shortcut reverse arcs $(v, w)$ with $(w, v) \in E_{>0}(f)$.
	For these, we can maintain a min-heap on $E_{>0}(f)$ arcs as each $v$
	arrives in $S$.
\item Non-shortcut $A$-to-$B$ forward arcs.
	For these, we can use a BCP data structure between
	$(A \setminus A_\emptyset) \cap S$ and
	$(B \setminus B_\emptyset) \setminus S$, weighted by potential.
\item Non-shortcut forward arcs from $s$-to-$A$ and from $B$-to-$t$.
	For $s$, we can maintain a min-heap on the potentials of
	$B \setminus S$, queried while $s \in S$.
	For $t$, we can maintain a max-heap on the potentials of
	$A \cap S$, queried while $t \not\in S$.

\item Shortcut arcs $(s, b)$ corresponding to empty 2-paths from $s$ to
	$b \in (B \setminus B_\emptyset) \setminus S'$.
	For these, we maintain a BCP data structure with $P = A_\emptyset$,
	$Q = (B \setminus B_\emptyset) \setminus S')$ with weights
	$\omega(p) = \pi(s)$ for all $p \in P$, and $\omega(q) = \pi(q)$ for
	all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 2-path $(s, a, b)$.
	This is only queried while $s \in S'$.
\item Shortcut arcs $(a, t)$ corresponding to empty 2-paths from
	$a \in (A \setminus A_\emptyset) \cap S'$ to $t$.
	For these, we maintain a BCP data structure with
	$P = (A \setminus A_\emptyset) \cap S'$,
	$Q = B_\emptyset \setminus S'$ with weights $\omega(p) = \pi(p)$ for
	all $p \in P$, and $\omega(q) = \pi(t)$ for all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 2-path $(a, b, t)$.
	This is only queried while $t \not\in S'$.
\item Shortcut arcs $(s, t)$ corresponding to empty 3-paths.
	For these, we maintain in a BCP data structure with
	$P = A_\emptyset \setminus S'$, $Q = B_\emptyset \setminus S'$ with
	weights $\omega(p) = \pi(s)$ for all
	$p \in P$, and $\omega(q) = \pi(t)$ for all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 3-path $(s, a, b, t)$.
	This is only queried while $s \in S'$ and $t \not\in S'$.
\end{enumerate}

By construction, the BCP distance of each datastructure in (4-6) is equal to
the reduced cost of the shortcut, which is equal to the reduced cost of the
corresponding empty path.
Each of the above data structures requires one query per relaxation, and an
insertion/deletion operation whenever a new vertex moves into $S$.
The data structures above can perform both in $O(\polylog n)$ time each, so the
running time of \textsc{Hungarian-Search2} outside of potential updates can be
bounded in the number of relaxation steps.

\begin{lemma}
\label{lemma:goldberg_hs_length1}
There are $O(k)$ non-shortcut relaxations in \textsc{Hungarian-Search2} before
a deficit vertex is reached.
\end{lemma}

\begin{proof}
Each edge relaxation adds a new vertex to $S$, and non-shortcut relaxations
only add non-empty vertices.
The vertices of $V \setminus S$ fall into several categories:
(i) $s$ or $t$, (ii) $A$ or $B$ vertex with 0 imbalance, and (iii) $A$ or $B$
vertex with deficit ($S$ contains all excess vertices).
The number of vertices in (i) and (iii) is $O(k)$, leaving us to bound the
number of (ii) vertices.

An $A$ or $B$ vertex with 0 imbalance must have an even number of $E_{>0}(f)$
edges.
There is either only one positive-capacity incoming edge (for $A$) or outgoing
edge (for $B$), so this quantity is either 0 or 2.
Since the vertex is non-empty, this must be 2.
We charge 0.5 to each of the two $E_{>0}(f)$ edges; the edges of $E_{>0}(f)$
have no more than 1 charge each.
Thus, the number of (ii) vertex relaxations is $O(|E_{>0}(f)|)$.
By Corollary~\ref{corollary:support_size_during}, $O(|E_{>0}(f)|) = O(k)$.
\end{proof}

\begin{lemma}
\label{lemma:goldberg_hs_length2}
There are $O(k)$ shortcut relaxations in \textsc{Hungarian-Search2} before a
deficit vertex is reached.
\end{lemma}

\begin{proof}
Recall the categories of shortcuts from the list of datastructures above.
We have shortcuts corresponding to (i) empty 2-paths surrounding
$a \in A_\emptyset$, (ii) empty 2-paths surrounding $b \in B_\emptyset$, and
(iii) empty 3-paths, which go from $s$ to $t$.

There is only one relaxation of type (iii), since $t$ can only be added to $S$
once.
The same argument holds for type (ii).

Each type (i) relaxation adds some non-empty $b \in B \setminus B_\emptyset$
into $S$.
Since $b$ is non-empty, it must either have deficit or an adjacent edge of
$E_{>0}(f)$.
We charge this relaxation to $b$ if it is deficit, or the adjacent $E_{>0}(f)$
edge otherwise.
No vertex is charged more than once, and no $E_{>0}(f)$ edge is charged more
than twice, therefore the total number of type (i) relaxations is
$O(|E_{>0}(f)|)$.
By Corollary~\ref{corollary:support_size_during}, $O(|E_{>0}(f)|) = O(k)$.
\end{proof}

\begin{corollary}
\label{corollary:goldberg_hs_length}
There are $O(k)$ relaxations in \textsc{Hungarian-Search2} before a deficit
vertex is reached.
\end{corollary}

In the following lemma, we complete the time analysis of
\textsc{Hungarian-Search2} by proving that potentials can be maintained in
$O(k)$ time over the course of the search.

\begin{lemma}
\label{lemma:goldberg_hs_time}
Using a dynamic BCP, we can implement \textsc{Hungarian-Search2} with
$T_1(n, k) = O(k\polylog n)$ and $P_1(n, k) = O(n\polylog n)$.
\end{lemma}

\begin{proof}
The initial sets for each data structure can be constructed in
$O(n\polylog n)$ time.
For each of the $O(1)$ data structures that are queried during a relaxation,
the new vertex moved into $S$ as a result of the relaxation causes $O(1)$
insertion/deletion operations.
For each of the data structures mentioned above, insertions and deletions
can be performed in $O(\polylog n)$ time.
Using Lemma~\ref{lemma:hs_time} as a basis, we first analyze the number of BCP
operations over the course of \textsc{Hungarian-Search2}.

\begin{enumerate}
\item Let $S^t_0$ denote the initial set $S$ at the beginning of the
	$t$-th Hungarian search, i.e. the set of $v \in V$ with
	$e_f(v) > 0$ after $t$ blocking flows.
	Assume for now that, at the beginning of the $(t+1)$-th
	Hungarian search, we have on hand the $S^t_0$ from the
	previous iteration.
	To construct $S^{t+1}_0$, we remove the vertices that had
	excess decreased to 0 by the $t$-th blocking flow.
	Thus, with that assumption, we are able to initialize $S$ at
	the cost of one BCP deletion per excess vertex, which sums to
	$O(k)$ over the entire course of \textsc{Refine}.
\item During each Hungarian search, a vertex entering $S$ may cause $P$
	or $Q$ to update and incur one BCP insertion/deletion.
	Like before, we can charge these to the number of edge
	relaxations over the course of \textsc{Hungarian-Search2}.
	The number of these is $O(k)$ by
	Corollary~\ref{corollary:goldberg_hs_length}.
\item Like before, we can meet the assumption in (1) by rewinding a log
	of point additions to $S$, and recover $S^t_0$.
\end{enumerate}

For potential updates, we use the same trick as in Lemma~\ref{lemma:hs_time} to
lazily update potentials after vertices leave $S$, but only for non-empty
vertices.
Non-empty vertices are stored in each data structure with weight
$\omega(v) = \pi(v) - \delta$, and $\delta$ is increased in lieu of increasing
the potential of all $S$ vertices.
When vertices leave $S$ (through the rewind mechansim above), we restore
their potentials as $\pi(v) \gets \omega(v) + \delta$.
With lazy updates, the number of potential updates on non-empty vertices is
bounded by the number of relaxations in the Hungarian search, which is $O(k)$
by Corollary~\ref{corollary:goldberg_hs_length}.
Note that empty vertex potentials are not handled in
\textsc{Hungarian-Search2}.
\end{proof}

\subsubsection{Depth-first search}

\begin{algorithm}
\caption{Depth-first search}
\label{algorithm:goldberg_dfs}
\begin{algorithmic}[1]
\Function{DFS}{$H = (V, E)$, $f$, $\pi$}
	\State $\tilde{H}_f \gets$ the shortcut graph of $H_f$
	\State $f' \gets 0$.
	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
	\State $S_0 \gets \{v \in V \mid e_f(v) > 0\}$
		\Comment{stack of excess vertices}
	\State $P \gets$ \Call{Pop}{$S_0$}
		\Comment{current path; stack}
	\Repeat
		\State $v' \gets$ \Call{Peek}{$P$}
		\If{$e_f(v') < 0$}
			\Comment{if we reached a deficit, save the path to $f'$}
			\State add to $f'$ a unit flow on the path $P$
			\State $P \gets$ \Call{Pop}{$S_0$}
		\Else
			\State $w' \gets \argmin\{c_\pi(v', w') \mid w' \not\in S, (v', w') \in \tilde{H}_f\}$
			\State $\gamma \gets c_\pi(v', w')$

			\If{$\gamma \leq 0$}
				\Comment{if $(v', w')$ is admissible, extend the current path}
				\State $S \gets S \cup \{w'\}$
				\State $P \gets$ \Call{Push}{$P$, $w'$}
			\Else
				\Comment{No admissible arcs leaving $v'$, remove from $P$}
				\State \Call{Pop}{$P$}
			\EndIf
		\EndIf
	\Until{$S_0 = \emptyset$}
\EndFunction
\end{algorithmic}
\end{algorithm}

The depth-first search is similar to \textsc{Hungarian-Search2} in that it
uses the relaxation of minimum-reduced cost arcs/empty paths, this time to
identify admissible arcs/empty paths in a depth-first manner.
This requires some adjustments to the data structures for finding the
minimum-reduced cost arc leaving $v' \in S$.
Given $v' \in S$, we would like to query:
\begin{enumerate}
\item Non-shortcut reverse arcs $(v', w')$ with $(w', v') \in E_{>0}(f)$.
	For these, we can maintain a min-heap on $(w', v') \in E_{>0}(f)$ arcs
	for each non-empty $v' \in V$.
\item Non-shortcut $A$-to-$B$ forward arcs.
	For these, we maintain a NN data structure over
	$P = (B \setminus B_\emptyset) \setminus S$, with weights
	$\omega(p) = \pi(p)$ for each $p \in P$.
	We subtract $\pi(v')$ from the NN distance to recover the reduced cost
	of the arc from $v'$.
\item Non-shortcut forward arcs from $s$-to-$A$ and from $B$-to-$t$.
	For $s$, we can maintain a min-heap on the potentials of
	$B \setminus S$, queried only if $v' = s$.
	For $B$-to-$t$ arcs, there is only one arc to check if $v' \in B$,
	which we can examine manually.

\item Shortcut arcs $(s, b)$ corresponding to empty 2-paths from $s$ to
	$b \in (B \setminus B_\emptyset) \setminus S'$.
	For these, we maintain a BCP data structure with $P = A_\emptyset$,
	$Q = (B \setminus B_\emptyset) \setminus S')$ with weights
	$\omega(p) = \pi(s)$ for all $p \in P$, and $\omega(q) = \pi(q)$ for
	all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 2-path $(s, a, b)$.
	This is only queried if $v' = s$.
\item Shortcut arcs $(a, t)$ corresponding to empty 2-paths from
	$a \in (A \setminus A_\emptyset) \cap S'$ to $t$.
	For these, we maintain a NN data structure over
	$P = B_\emptyset \setminus S'$ with weights $\omega(p) = \pi(t)$ for
	each $p \in P$.
	A response $(v', b)$ corresponds to th empty 2-path $(v', b, t)$.
	We subtract $\pi(v')$ from the NN distance to recover the reduced cost
	of the arc from $v'$.
	This is not queried if $t \in S$.
\item Shortcut arcs $(s, t)$ corresponding to empty 3-paths.
	For these, we maintain in a BCP data structure with
	$P = A_\emptyset \setminus S'$, $Q = B_\emptyset \setminus S'$ with
	weights $\omega(p) = \pi(s)$ for all
	$p \in P$, and $\omega(q) = \pi(t)$ for all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 3-path $(s, a, b, t)$.
	This is only queried while $v' = s$ and $t \not\in S'$.
\end{enumerate}

Each data structure above performs $O(\polylog n)$ time worth of query and
insertion/deletion per relaxation, so the running time is again bounded by
$O(\polylog n)$ times the number of relaxations.
Since the pseudoflow is not changed within \textsc{DFS} (since the end of
\textsc{Hungarian-Search2}), the proofs from
Lemmas~\ref{lemma:goldberg_hs_length1} and \ref{lemma:goldberg_hs_length2} can
be recycled for \textsc{DFS}.

\begin{lemma}
\label{lemma:goldberg_dfs_length1}
There are $O(k)$ non-shortcut relaxations in \textsc{DFS} before a deficit
vertex is reached.
\end{lemma}

\begin{lemma}
\label{lemma:goldberg_dfs_length2}
There are $O(k)$ shortcut relaxations in \textsc{DFS} before a deficit vertex
is reached.
\end{lemma}

\begin{corollary}
\label{corollary:goldberg_dfs_length}
There are $O(k)$ relaxations in \textsc{DFS} before a deficit vertex is
reached.
\end{corollary}

There are no potentials to update within \textsc{DFS}, so the running time of
\textsc{DFS} boils down to the time spent to querying and updating the data
structures.

\begin{lemma}
\label{lemma:goldberg_dfs_time}
Using a dynamic NN, we can implement \textsc{DFS} with
$T_2(n, k) = O(k\polylog n)$ and $P_2(n, k) = O(n\polylog n)$.
\end{lemma}

\begin{proof}
At the beginning of \textsc{Refine}, we can initialize the $O(1)$ data
structures used in \textsc{DFS} in $P_2(n, k) = O(n\polylog n)$ time.
We use the same rewinding mechanism as \textsc{Hungarian-Search2}
(Lemma~\ref{lemma:goldberg_hs_time}) to avoid reconstructing the data
structures across iterations of \textsc{Refine}, so the total time spent
is bounded by the $O(\polylog n)$ times the number of relaxations.
By Corollary~\ref{corollary:goldberg_dfs_length}, we obtain
$T_2(n, k) = O(k\polylog n)$.
\end{proof}

\subsubsection{Size of the blocking flow and completing time analysis}

With Lemmas~\ref{lemma:goldberg_hs_time} and \ref{lemma:goldberg_dfs_time},
we can complete the proof of Lemma~\ref{lemma:goldberg_refine_time}
(time per \textsc{Refine}) by bounding the total number of arcs whose flow is
updated by a blocking flow during \textsc{Refine}.
This bounds both the time spent updating the flow value of these arcs, and
also the time spent on empty vertex potential updates
(Lemma~\ref{lemma:empty_updates}).

\begin{lemma}
\label{lemma:goldberg_bf_size}
Let $N_i$ be the number of positive flow arcs in the $i$-ith blocking flow
of \textsc{Refine}.
Then, $\sum_i N_i = O(k\sqrt{k})$.
\end{lemma}

\begin{proof}
Let $i$ be fixed and consider the invocation of \textsc{DFS} which produces the
$i$-th blocking flow $f_i$.
\textsc{DFS} constructs $f_i$ as a sequence of admissible excess-deficit paths,
which appear as path $P$ in Algorithm~\ref{algorithm:goldberg_dfs}.
Every arc in $P$ is an arc relaxed by \textsc{DFS}, so $N_i$ is bounded by the
number of relaxations performed in \textsc{DFS}.
Using Corollary~\ref{corollary:goldberg_dfs_length}, we have $N_i = O(k)$.

By Lemma~\ref{lemma:goldberg_refine_iterations}, there are $O(\sqrt{k})$
iterations of \textsc{Refine} before it terminates.
Summing, we see that $\sum_i N_i = O(k\sqrt{k})$.
\end{proof}

We now complete the proof of Lemma~\ref{lemma:goldberg_refine_time}.
There $O(\sqrt{k})$ iterations of \textsc{Refine}, each of which executes
\textsc{Hungarian-Search2} and \textsc{DFS}.
By Lemmas~\ref{lemma:goldberg_hs_time} and \ref{lemma:goldberg_dfs_time},
these calls take $O(T_1(n, k) + T_2(n, k)) = O(k\polylog n)$ time per
iteration.
\textsc{Hungarian-Search2} and \textsc{DFS} require some
once-per-\textsc{Refine} preprocessing to initialize data structures
in $P_1(n, k) + P_2(n, k) = O(n\polylog n)$ time.
Outside of these, we need to account for the time spent on flow value updates
and augmentation-induced empty vertex potential updates.
By Lemma~\ref{lemma:goldberg_bf_size}, the former is $O(k\sqrt{k})$ over the
course of \textsc{Refine}.
Combining Lemmas~\ref{lemma:goldberg_bf_size} and \ref{lemma:empty_updates},
the time for the latter is also $O(k\sqrt{k})$.

Filling in the values of $P_1(n, k)$, $P_2(n, k)$, $T_1(n, k)$, and
$T_2(n, k)$, the total time for \textsc{Refine} is
$O((n + k\sqrt{k})\polylog n)$.
Together with Lemmas~\ref{lemma:goldberg_scales} and \ref{lemma:scale_init},
this completes the proof of Theorem~\ref{theorem:gmcm}.


\section{Unbalanced transportation}

% definitions
% introduce the excess scaling algorithm/Orlin's
% time per hungarian search
% handling problem cases (stars, singletons)


{
\bibliographystyle{abbrv}
\bibliography{ref}
}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

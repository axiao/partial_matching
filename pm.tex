%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{latexsym,color,amsmath,amssymb,amsthm}
\usepackage{floatflt}
\usepackage{color}
\usepackage{times}
\usepackage{euscript}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{mathpazo}
\usepackage[top=1.25in,bottom=1.25in,left=1.25in,right=1.25in]{geometry}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{algpseudocode,algorithm,algorithmicx}

\usepackage{fullpage}
\usepackage{xspace}
\usepackage[shortlabels, inline]{enumitem}

\usepackage[title]{appendix}

\usepackage{ifpdf}

%\graphicspath{{fig/}}

%\DeclareGraphicsExtensions{.pdf}

%\ifpdf
%\else
%   \DeclareGraphicsExtensions{.eps}
%\fi

\def\etal{\textsl{et~al.}}
\def\polylog{\mathop{\mathrm{polylog}}}
\def\eps{\varepsilon}
\def\softO{\widetilde{O}}
\def\bd{{\partial}}
\def\reals{\mathbb{R}}
\def\ints{\mathbb{N}}
\def\flr#1{{\lfloor #1\rfloor}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\tsupply{\lambda}
\def\fsupply{\phi}

\def\Refine{\textsc{Refine}}
\def\Update{\textsc{Update}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}{Observation}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}

\newcommand{\lemlab}[1]{\label{lem:#1}}
\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\theolab}[1]{\label{theo:#1}}
\newcommand{\corlab}[1]{\label{cor:#1}}
\def\lemref#1{Lemma~\ref{lemma:#1}}
\def\theoref#1{Theorem~\ref{theo:#1}}
\def\corref#1{Corollary~\ref{cor:#1}}
\def\figref#1{Figure~\ref{fig:#1}} \def\secref#1{Section~\ref{sec:#1}}

\def\parent{\operatorname{par}}
\def\cost{\operatorname{cost}}


\makeatletter
\long\def\@makecaption#1#2{
    \vskip 10pt
    \setbox\@tempboxa\hbox{{\footnotesize {\bf #1.} #2}}
    \ifdim \wd\@tempboxa >\hsize         % IF longer than one line:
        {\footnotesize {\bf #1.} #2\par}% THEN set as ordinary paragraph.
      \else                              %   ELSE  center.
        \hbox to\hsize{\hfil\box\@tempboxa\hfil}
    \fi}
%\dbltextfloatsep 18pt plus 2pt minus 4pt   % was 20pt plus 2pt minus 4pt
%\textfloatsep 18pt plus 2pt minus 4pt      % was 20pt plus 2pt minus 4pt
\makeatother

\title{ Geometric Partial Matching and Unbalanced Transportation %
\date{\today} % replace with date?
\author{
Pankaj K. Agarwal 
\and
Allen Xiao
}
}


\begin{document}

\maketitle

\section{Introduction}

Consider the problem of finding a minimum-cost bichromatic matching between
a set $A$ of red points and $B$ of blue points lying in the plane,
where the cost of a matching edge $(a, b)$ is the Euclidean distance 
$\|a - b\|$ ---
in other words, the minimum-cost bipartite matching problem on the Euclidean 
complete graph $G = (A \cup B, A \times B)$.
Let $|A| = r$ and $|B| = n$, and (without loss of generality) $r \leq n$.
We consider the problem of \emph{partial matching}, where we are tasked with
finding the minimum-cost matching of size $k \leq r \leq n$.
When $k = r = n$, we say the matching instance is \emph{balanced}
and call the problem \emph{perfect matching} or the \emph{assignment problem}.
When $k = r < n$ ($A$ and $B$ have different sizes, but the matching is 
maximal), we say the matching instance is \emph{unblanced}.
Partial matching generalizes both perfect matching and unbalanced matching.
We will refer to the geometric problem as \emph{bichromatic partial matching}.

%TODO previous work
%	on matching
% 	on geometric matching
%	on unbalanced/partial problems (GHKT, the push-relabel one, the tech report)


\subsection{Contributions}

In this paper, we present two algorithms for bichromatic partial matching
that are based on fitting nearest-neighbor (NN) and bichromatic closest pair 
(BCP) oracles into primal-dual algorithms for non-geometric bipartite matching 
and minimum-cost flow.
This pattern is not new, see e.g. %TODO .
Unlike these previous works, we focus on obtaining running time dependencies on 
$k$ or $r$ instead of $n$, i.e. faster for inputs with small $r$ or $k$.
We begin in Section~\ref{section:prelim} by introducing notation for matching 
and minimum-cost flow.

% O((n + k^2)\polylog n)

In our first algorithm, we show that the Hungarian algorithm~\cite{Kuhn55} 
combined with a BCP oracle solves bichromatic partial matching exactly in time 
$O((n + k^2)\polylog(n))$.
Mainly, we show that we can separate the $O(n\polylog(n))$ preprocessing time 
for building the BCP data structure from the augmenting paths' search time,
and update duals in a lazy fashion such that the number of dual updates per
augmenting path is $O(k)$.
This is done in Section~\ref{section:hung}.

\begin{theorem}
\label{theorem:hung}
Given point sets $A, B \subset \mathbb{R}^2$ with $|A| = r \leq |B| = n$ and a 
parameter $k \leq r$, a minimum-cost bichromatic partial matching of size $k$ 
can be computed between $A$ and $B$ in $O((n + k^2)\polylog(n))$ time.
\end{theorem}

% O((n + k\sqrt{k})\polylog(n)\log(n/\eps))

Next, we apply a similar technique to the unit-capacity min-cost circulation 
algorithm of Goldberg, Hed, Kaplan, and Tarjan~\cite{GHKT17}.
The resulting algorithm finds a $(1 + \eps)$-approximation to the optimal 
bichromatic partial matching in $O((n + k\sqrt{k})\polylog(n)\log(n/\eps))$ 
time.
This is presented in Section~\ref{section:goldberg}.

\begin{theorem}
\label{theorem:gmcm}
Given point sets $A, B \subset \mathbb{R}^2$ with $|A| = r \leq |B| = n$ and a 
parameter $k \leq r$, a $(1+\eps)$ bichromatic partial matching of size $k$ 
can be computed between $A$ and $B$ in 
$O((n + k\sqrt{k})\polylog(n)\log(n/\eps))$ time.
\end{theorem}

% add the ~O(rn^{3/2}) version

Our third algorithm solves the transportation problem, in the unbalanced 
setting with $|A| = r$ and $|B| = n$.
This time, we use the strongly polynomial uncapacitated min-cost flow algorithm
by Orlin~%TODO cite
The result is an $O(n^{3/2}r\polylog(n))$ time algorithm for unbalanced 
transportation.
This improves over the $O(n^2 \polylog(n))$ time algorithm of %TODO cite
when $r = o(\sqrt{n})$.

\begin{theorem}
\label{theorem:orlin}
Given point sets $A, B \subset \mathbb{R}^2$ with $|A| = r \leq |B| = n$ with
supplies and demands $\tsupply(\cdot)$ such that 
$\sum_{a \in A} \tsupply(a) = \sum_{b \in B} \tsupply(b)$, an optimal 
transportation map can be computed in $O(n^{3/2}r\polylog(n))$ time.
\end{theorem}

%TODO STILL IN PROGRESS, WE DON'T HAVE A SOLUTION YET. REMOVE IF FAILS.

%Finally, we show that an algorithm by Orlin~\cite{} for uncapacitated min-cost 
%flow can be modified to give an $O(nr\polylog(n))$ time algorithm for the 
%unbalanced transportation problem.
%Unlike matching, vertices may be ``matched'' (transporting) to more than one 
%other vertex.
%As a result, it seems possible that the search procedure reaches vertices which 
%do not contribute to a new augmenting path or towards finding unreached $B$ 
%vertices, forcing the search to backtrack.
%Avoiding the backtracking is the main challenge for this algorithm.
%% how?

By nature of the BCP/NN oracles we use, our results generalize from Euclidean 
distances to any $L_p$ distances.

\section{Preliminaries}
\label{section:prelim}

\subsection{Matching}

Let $G = (A \cup B, E)$ be a bipartite graph between vertex sets $A$ and $B$,
with costs $c(v, w)$ for each $e \in E$.
We use $C = \max_{e \in E} c(e)$, and assume that the problem is scaled such 
that $\min_{e \in E} c(e) = 1$.
A \emph{matching} $M \subseteq E$ is a set of edges where no two share an 
endpoint.
We use $V(M)$ to denote the vertices matched by $M$.
The size of a matching is the number of edges in the set, and the cost of a 
matching is the sum of costs of its edges.
The minimum-cost partial matching problem (MPM) asks to find the size $k$ 
matching $M^*$ of minimum cost.

\subsection{Minimum-cost flow}

For minimum-cost flow, let $G_0 = (V, E_0)$ be a directed graph with 
nonnegative arc capacities $u(v, w)$ and costs $c(v, w)$ for each arc 
$(v, w) \in E_0$. 
We say $G_0$ is \emph{unit-capacity} if all $u(v, w) = 1$.
Let $\fsupply: V \to \ints_{\geq 0}$ be a supply-demand function on $V$,
where positive values of $\fsupply(v)$ we refer to as \emph{supply},
negative values of $\fsupply(v)$ are \emph{demand}, and 
$\sum_{v \in V} \fsupply(v) = 0$.
We augment $G_0$ to make it \emph{symmetric} (for every arc $(v, w) \in E_0$ 
its reverse $(v, w)^R = (w, v)$ is also an arc) and the costs 
\emph{antisymmetric} ($c(v, w) = -c(w, v)$): for each $(v, w) \in E_0$, create 
an arc $(w, v)$ and define $u(w, v) = 0$ and $c(w, v) = -c(v, w)$.
Let this set of new \emph{reverse arcs} be $E^R$.
From here forward, we work with the symmetric multigraph 
$G = (V, E = E_0 \cup E^R)$.
The combination of graph, costs, capacities, and supply-demands is a 
\emph{network} $(G, c, u, \fsupply)$.

A \emph{pseudoflow} $f$ is an antisymmetric function on arcs 
($f(v, w) = -f(w, v)$) such that for each arc $f(v, w) \leq u(v, w)$.
We say that $f$ \emph{saturates} an arc $e \in E$ if $f(v, w) = u(v, w)$.
All our algorithms will handle integer-valued pseudoflows, so in the 
unit-capacity setting an arc has either 0 flow or is saturated.
Given a pseudoflow $f$, we define the \emph{imbalance} of a vertex to be
$e_f(v) := \fsupply(v) + \sum_{(w, v) \in E}{f(w, v)} - \sum_{(v, w) \in E}{f(v, w)}$.
We call positive imbalance \emph{excess} and negative imbalance \emph{deficit},
vertices with $e_f(v) > 0$ we call \emph{excess vertices} and respectively 
those with $e_f(v) < 0$ \emph{deficit vertices}.
If all vertices have $e_f(v) = 0$, the pseudoflow is a \emph{circulation}.
The cost of a pseudoflow is $\cost(f) = \sum_{(v, w) \in E} c(v, w) f(v, w)$.
The minimum-cost flow problem (MCF) asks to find the circulation $f^*$ of 
minimum cost.

For each arc $(v, w) \in E$, the \emph{residual capacity} with respect to 
pseudoflow $f$ is defined to be $u_f(v, w) := u(v, w) - f(v, w)$.
The set of \emph{residual edges} is 
$E_f := \{(v, w) \in E \mid u_f(v, w) > 0\}$.
The \emph{residual graph} is $G_f = (V, E_f)$.
A pseudoflow $f'$ in $G_f$ can be ``added'' or ``augmented'' to $f$ to produce 
a new pseudoflow (i.e. the per-arc addition of $f + f'$ is a valid pseudoflow 
in $G$).
A pseudoflow $f'$ in $G_f$ is an \emph{improving flow} if (1) 
$0 \leq e_{f'}(v) \leq e_f(v)$ for all excess vertices $v \in V$,
(2) $0 \leq -e_{f'}(v) \leq -e_f(v)$ for all deficit vertices $v \in V$,
(3) if $e_f(v) = 0$ then $e_{f'}(v) = 0$, and (4) 
$\sum_{v \in V} |e_{f'}(v)| < \sum_{v \in V} |e_f(v)|$.
If improving flow $f'$ is on a simple path (from an excess vertex to a deficit 
vertex), we call it an \emph{augmenting path flow} and its edges an 
\emph{augmenting path}.
If $f'$ saturates at least one residual arc of every augmenting path in $G_f$,
we call it a \emph{blocking flow}.
In other words, for blocking flow $f'$, there is no augmenting path flow 
$f'' \in G_f$ for which $f + (f' + f'')$ is a feasible pseudoflow in $G$.

\subsection{Primal-dual augmentation algorithms}

The Hungarian algorithm begins with an empty matching and increases it to size 
$k$ using \emph{alternating augmenting paths}.
Given a non-maximal matching $M$, an alternating augmenting path $P$ is a path
between an unmatched $a \in A$ and unmatched $b \in B$.
Then, $M' = M \oplus P$ is a matching of size 1 greater.
By restricting alternating augmenting paths to edges which satisfy a certain 
cost condition (admissibility, defined momentarily), one can prove that each 
intermediate matching, of size $j \leq k$, is minimum-cost for size $j$.
There is a similar augmentation procedure for flows, which sends improving 
flows (e.g. augmenting path flows) to gradually reduce the imbalance in a 
pseudoflow to 0, making it a circulation.
By restricting augmentations to residual arcs satisfying a certain cost 
condition (admissibility), one can prove that the resulting circulation is 
minimum cost.

Formally, we use the linear programming dual problem to define 
\emph{potentials} (dual variables) $\pi(v)$ for each $v \in V$. 
The \emph{reduced cost} of $(v, w) \in E_f$ with respect to $\pi$ is 
$c_\pi(v, w) := c(v, w) - \pi(v) + \pi(w)$.
The \emph{dual feasibility constraint} is that $c_\pi(v, w) \geq 0$ for all 
residual arcs; potentials which satisfy this are said to be \emph{feasible}.
The linear programming \emph{optimality conditions} state that, for an optimal 
circulation $f^*$, there exist feasible potentials $\pi^*$ which satisfy 
$c_\pi(v, w) = 0$ on all arcs with $f^*(v, w) > 0$.
We can similarly define potentials and reduced cost for matchings, using
$c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b)$ for $(a, b) \in A \times B$.

Suppose we relax the dual feasibility constraint to allow for a violation of 
$\eps > 0$.
We say that a pseudoflow $f$ is \emph{$\eps$-optimal} if 
$c_\pi(v, w) \geq -\eps$ for all $(v, w) \in E_f$ with $u_f(v, w) > 0$.
Note that 0-optimality coincides with the optimality conditions.
We say that residual arcs with $c_\pi(v, w) \leq 0$ are \emph{admissible}.
We say that an improving flow $f'$ is \emph{admissible} if $f'(v, w) > 0$
only on admissible arcs $(v, w)$.
For matchings, we say that $M$ is $\eps$-optimal if $c_\pi(a, b) \leq \eps$ for 
$(a, b) \in M$ and $c_\pi(a, b) \geq -\eps$ $(a, b) \in E \setminus M$.
Matching edges are admissible if $c_\pi(a, b) \geq 0$ (resp. nonmatching edges, 
if $c_\pi(a, b) \leq 0$), and an alternating augmenting path is admissible
if all its edges are.
For both matching and flows, 0-optimal $f$ implies the admissibility condition 
is with equality, instead ($c_\pi(v, w) = 0$).

Now, we can concretely state how admissible augmentations lead to a correct 
algorithm for $\eps > 0$.
\begin{lemma}
	Let $f$ be an $\eps$-optimal pseudoflow in $G$ and let $f'$ be an 
	admissible improving flow in $G_f$.
	Then $f + f'$ is also $\eps$-optimal.
\end{lemma}
\begin{proof}
	Augmentation by $f'$ will not change the potentials, but may introduce
	new arcs with $u_{f+f'}(v, w) > 0$.
	We will verify that these arcs satisfy the $\eps$-optimality condition.
	Such an arc $(v, w)$ must have $u(v, w) = f(v, w) > (f+f')(v, w)$, 
	meaning $f'(w, v) > 0$.
	By assumption, $(w, v)$ was an $\eps$-admissible arc, thus
	$c_\pi(w, v) \leq 0$, meaning $c_\pi(v, w) \geq 0$.
	Thus, all such arcs have $c_\pi(v, w) \geq 0 \geq -\eps$, and $f + f'$
	is $\eps$-optimal.
\end{proof}

With a similar argument, we could prove the same for matchings.
Finally, we show that $\eps$-optimality is sufficient to prove when a 
circulation is an approximate MCF solution, when the underlying graph is 
unit-capacity.

\begin{lemma}
\label{lemma:mcf_cost}
	Let $G$ be a unit-capacity graph with $|V| = n$ and $|E| = m$,
	$f$ an $\eps$-optimal circulation in $G$, and $f^*$ an optimal 
	circulation for $G$.
	Then, $\cost(f) \leq \cost(f^*) + m\eps$.
\end{lemma}
\begin{proof}
	By the flow decomposition theorem, there exists a residual pseudoflow 
	$f'$ such that $f + f' = f^*$, and $f'$ can be decomposed into a set of 
	unit flows on edge-disjoint cycles.
	The number of edges used by these cycles is at most $m$.
	The cost of a residual cycle is equal to its reduced cost, since the 
	potentials of telescope.
	Thus, $\cost(f') \geq m(-\eps)$, and therefore 
	$\cost(f) \leq \cost(f^*) + m\eps$.
\end{proof}

This bound can be improved if we have a better upper bound on the number of 
edges used in the cycles of $f'$.
Indeed, the algorithm in Section~\ref{section:goldberg} uses a graph where the
bound is $6k$, and converts the statement from an additive approximation to a 
relative approximation.
Since our matching algorithm uses a 0-optimal (exact) solution, we do not 
include a proof for approximation quality of $\eps$-optimal matchings.


\section{Matching with the Hungarian algorithm}
\label{section:hung}

The Hungarian algorithm maintains a 0-optimal (initially empty) matching $M$, 
and repeatedly augments by alternating augmenting paths of admissible edges 
until $|M| = k$.
To this end, the algorithm maintains a set of feasible potentials $\pi$ and 
updates them to find augmenting paths of admissible edges.
It maintains the invariant that matching edges are admissible.
Since there are $k$ augmentations and each alternating path is length at most 
$2k-1$, the total time spent on bookkeeping the matching is $O(k^2)$.
This leaves the analysis of the subroutine which updates the potentials and 
finds an admissible augmenting path, called the \emph{Hungarian search}.

\begin{algorithm}
\caption{Hungarian Algorithm}
\begin{algorithmic}[1]
\Function{Match}{$G = (A \cup B, E)$, $k$}
	\State $M \gets \emptyset$
	\State $\pi(v) \gets 0, \forall v \in A \cup B$
	\While{$|M| < k$}
		\State $\Pi \gets$ \Call{Hungarian-Search}{$G$, $M$, $\pi$}
		\State $M \gets M \oplus \Pi$
	\EndWhile
	\State\Return $M$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Hungarian algorithm time]
\label{theorem:hung_orig}
	Let $G = (A \cup B, A \times B)$ be an instance of bichromatic partial 
	matching with $|A| = r \geq |B| = n$, and parameter $k \leq r$.
	Suppose the Hungarian search finds each augmenting path in $T(n, k)$ 
	time after a one-time $P(n, k)$ preprocessing time.
	Then, the Hungarian algorithm finds the optimal size $k$ matching in
	time $O(P(n, k) + k T(n, k) + k^2)$.
\end{theorem}

\subsection{Hungarian search}

Let $S$ be the set of vertices that can be reached from an unmatched $a \in A$ 
by admissible residual edges, initially the unmatched vertices of $A$.
The Hungarian search updates potentials in a Dijkstra's algorithm-like manner, 
expanding $S$ until it includes an unmatched $b \in B$ (and thus an admissible 
alternating augmenting path).
The ``search frontier'' of the Hungarian search is 
$(S \cap A) \times (B \setminus S)$.
From the frontier, we \emph{relax} the edge with minimum reduced cost, changing 
the duals such that the edge becomes admissible, and adding the opposite $B$ 
vertex into $S$.

The dual update uniformly decreases the reduced costs of the frontier edges.
Since $(a', b')$ is the minimum-reduced cost frontier edge, the potential 
update in line~\ref{line:hs_update} does not make any reduced cost negative, 
and thus preserves the dual feasibility constraint for all edges.
The algorithm is shown below as Algorithm~\ref{algorithm:hung_hs}.

\begin{algorithm}
\caption{Hungarian Search (matching)}
\label{algorithm:hung_hs}
\begin{algorithmic}[1]
\Require{$\forall (a, b) \in M, c_\pi(a, b) = 0$}
\Statex %newline
\Function{Hungarian-Search}{$G = (A \cup B, E)$, $M$, $\pi$}
	\State $S \gets a \in (A \setminus V(M))$
	\Repeat
		\State $(a', b') \gets \argmin\{c_\pi(a, b) \mid (a, b) \in (S \cap A) \times (B \setminus S)\}$
		\State $\gamma \gets c_\pi(a', b')$
		\State $\pi(v) \gets \pi(v) + \gamma, \forall v \in S$
			\Comment{make $(a', b')$ admissible}
			\label{line:hs_update}
		\State $S \gets S \cup \{b'\}$
		\Statex %newline
		\If{$b' \not\in V(M)$} \Comment{$b'$ unmatched}
			\State $\Pi \gets$ alternating augmenting path in $S$ to $b'$
			\State\Return $\Pi$
		\Else \Comment{$b'$ is matched to some $a'' \in A \cap V(M)$}
			\State $S \gets S \cup \{a''\}$
		\EndIf
	\Until{$S = (A \cup B)$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}

By tracking the forest of relaxed edges (e.g. back pointers), it is 
straightforward to recover the alternating augmenting path $\Pi$ once we reach an
unmatched $b' \in B$.
We make the following observation about the Hungarian search:

\begin{observation}
\label{observation:hungsearch_length}
	There are $\leq k$ edge relaxations before the Hungarian search finds an 
	alternating augmenting path.
\end{observation}
\begin{proof}
	Each edge relaxation either leads to a matched $B$ vertex (of which 
	there are at most $k-1$), or finds an unmatched vertex and ends the 
	search.
\end{proof}

In non-geometric graphs, the minimum edge is typically found by pushing all 
encountered $(S \cap A) \times (B \setminus S)$ edges into a priority queue.
However, in the bipartite complete graph, this may take $\Theta(rn\polylog(n))$
time in each Hungarian search --- edges must be pushed into the queue even if 
they are not relaxed.
We avoid this problem by finding the minimum edge using a \emph{bichromatic 
closest pair} (BCP) query on additively weighted Euclidean distances,
for which there exist fast (poly-logarithmic query and update) dynamic data 
structures.
The BCP problem is to find, between two point sets 
$P, Q \subseteq \mathbb{R}^2$, the $p \in P$ and $q \in Q$ minimizing 
$\|p - q\| - w(p) + w(q)$, for some real-valued vertex weights $w(p)$;
a perfect fit for reduced cost.
The state of the art for dynamic BCP data structures --- from Kaplan, Mulzer, 
Roditty, Seiferth, and Sharir~\cite{KMRSS17} --- inserts and deletes points in 
$O(\polylog(n))$ time, and answers queries in $O(\log^2 n)$ time.
The following lemma, combined with Theorem~\ref{theorem:hung_orig}, completes
the proof of Theorem~\ref{theorem:hung}.

\begin{lemma}
	Using a dynamic BCP, we can implement Hungarian search with 
	$T(n, k) = O(k\polylog(n))$ and $P(n, k) = O(n\polylog(n))$.
\end{lemma}
\begin{proof}
	We will maintain a BCP between $P = (S \cap A)$ and 
	$Q = (B \setminus S)$.
	Changes to the BCP sets are entirely driven by changing $S$,
	i.e. updates to $S$ incur BCP insertions/deletions.
	We first analyze the bookkeeping outside of dual updates, and then
	show how dual updates can be done efficiently.

	\begin{enumerate}
	\item Let $S^t_0$ denote the initial set $S$ at the beginning of the 
		$t$-th Hungarian search, i.e. the set of unmatched $A$ points
		after $t$ augmentations.
		At the very beginning of the Hungarian algorithm, we initialize 
		$S^0_0 \gets A$ (meaning $P = A$ and $Q = B$), which is a 
		one-time addition of $O(n)$ points into BCP.
		On each successive Hungarian search, $S^t_0$ shrinks as more 
		and more $A$ points are matched.
		Assume for now that, at the beginning of the $(t+1)$-th 
		Hungarian search, we are able to construct the $S^t_0$ from the 
		previous iteration.
		To construct $S^{t+1}_0$, we simply remove the $A$ point that 
		was matched by the $t$-th augmenting path.
		Thus, with that assumption, we are able to initialize $S$ in 
		one BCP deletion operation per augmentation.

	\item During each Hungarian search, points are added to $P$ ($A$ points 
		added to $S$) and removed from $Q$ ($B$ points added to $S$)
		--- at most one of each per edge relaxation.
		By Observation~\ref{observation:hungsearch_length} the number 
		of relaxed edges is at most $k$, so the number of these BCP 
		operations is also at most $k$.

	\item To obtain the assumption used in (1), we keep a log of the 
		points added since $S^t_0$ in the last Hungarian search 
		(i.e. those of (2)).
		After the augmentation, we use this log to delete the added 
		vertices from $S$ and recover $S^t_0$.
		By the argument in (2) there are $O(k)$ of such points to 
		delete, so reconstructing $S^t_0$ costs $O(k)$ BCP operations.
		% TODO instead of reversing a log, is persistence an easier solution to this?
	\end{enumerate}
	We spend a one-time fee of $P(n, k) = O(n \polylog(n))$ time to build 
	the initial BCP.
	The number of BCP operations associated with each Hungarian search is 
	$O(k)$, so the time spent on BCP operations in each Hungarian search
	is $O(k \polylog(n))$.
	
	We modify a trick from Vaidya~\cite{Vaidya89} to batch potential 
	updates such that the dual updates we do perform can be charged to a 
	BCP insertion or deletion.
	Throughout the course of the Hungarian algorithm, we maintain a value
	$\delta$ (initially 0) which aggregates dual changes.
	Vertices that are added to $S$ are saved into $P$ with weight 
	$w(p) \gets \pi(p) - \delta$.
	When the points of $S$ have their duals increased in (2), we instead
	raise $\delta \gets \delta + c_\pi(\hat{a}, \hat{b})$.
	Thus, the ``true'' potential for any point in $S$ is $w(p) + \delta$.
	For points outside $S$ (i.e. $B \setminus S$), we simply use the true 
	potential as the BCP weight.
	Since all $S \cap A$ BCP weights are uniformly offset from their 
	potential by $\delta$, this change does not alter the BCP result.
	Once a point is removed from $S$, we update its true potential
	as $\pi(p) \gets w(p) + \delta$.

	Obviously, the number of updates to $\delta$ is equal to the number of 
	edge relaxations, which is $O(k)$ per Hungarian search.
	The number of times we have to save the true potential is bounded by
	the number times we remove a point from $S$.
	By the previous argument (2), this is $O(k)$ per Hungarian search as 
	well.
	The total time for dual updates per Hungarian search is therefore 
	$O(k)$.
	Overall, the time per Hungarian search is $T(n, k) = O(k\polylog(n))$.
\end{proof}


\section{Matching with the Goldberg~{\etal} algorithm}
\label{section:goldberg}

The basis of the algorithm in this section is a \emph{cost-scaling} algorithm 
for unit-capacity min-cost flow from \cite{GHKT17}.
Before describing the algorithm, we first give a linear-time reduction from 
min-cost matching to unit-capacity min-cost flow, which allows us to apply the 
Goldberg~{\etal} algorithm to partial matching.

\subsection{MPM to unit-capacity MCF reduction}
\label{subsection:mcm_mcf_reduction}

For a partial matching problem on $G = (A \cup B, E)$ with parameter $k$, we 
direct the bipartite edges of $E$ from $(A \to B)$, with costs equal to the 
original cost $c(a, b)$ and capacity 1.
Next, we add a dummy vertex $s$ with arcs $(s, a)$ to every $a \in A$,
and respectively a dummy vertex $t$ with arcs $(b, t)$ for each $b \in B$,
all with arc cost 0 and capacity 1.
Set $\fsupply(s) = k$, $\fsupply(t) = -k$, and $\fsupply(v) = 0$ for all other 
vertices.
Call the resulting multigraph $H = (A \cup B \cup \{s, t\}, E')$.

It is straightforward to show that any circulation $f$ on $H$ uses exactly 
$k$ of the $(A \to B)$ arcs, which correspond to the edges of a size $k$ 
matching.
For a circulation $f$ in $H$, we use $M_f \subseteq E$ to denote the 
corresponding matching.
Observe that $\cost(f) = \cost(M_f)$, so an $\alpha$-approximation to the MCF 
problem on $H$ is an $\alpha$-approximation to the matching problem on $G$.

We can improve Lemma~\ref{lemma:mcf_cost} on $H$.
Note that for integer-valued (e.g. unit) capacities, there is always an
integer-valued optimal circulation.
\begin{lemma}
\label{lemma:goldberg_cost_add}
	Let $f$ an $\eps$-optimal integer circulation in $H$, and $f^*$ an 
	optimal integer circulation for $H$.
	Then, $\cost(f) \leq \cost(f^*) + 6k\eps$.
\end{lemma}
\begin{proof}
	We label the arcs of $H_f$ as follows: \emph{forward arcs} directed
	from $s \to A$ or $A \to B$ or $B \to t$, and \emph{reverse arcs} in
	the opposite directions.
	Observe that a residual cycle $\Gamma$ must have exactly half of its 
	edges be reverse arcs.
	The reverse arcs may either be (i) on one of the $M_f$ edges or else
	(ii) between $\{s\} \times A$ or (iii) between $B \times \{t\}$.
	If it is of type (ii) or (iii), there is an adjacent type (i) reverse
	arc.
	Thus, we can charge the reverse arcs of $\Gamma$ to $M_f \cap \Gamma$ 
	edges with at most 3 charge per edge of $M_f \cap \Gamma$.
	We can then charge all arcs of $f' = (f^* - f) = \sum \Gamma_i$ to 
	$M_f$ with at most 6 charge per $M_f$ edge.
	As $|M_f| = k$, the number of arcs in $f'$ is at most $6k$.
	The rest of the argument proceeds as in Lemma~\ref{lemma:mcf_cost}.
\end{proof}

Suppose we scaled arc costs (via uniform scaling of the input points) such that
the minimum cost (closest pair distance) is 1.
Then, $\cost(f^*) \geq k$, and we can turn Lemma~\ref{lemma:goldberg_cost_add}
into a relative approximation.

\begin{corollary}
\label{corollary:flow_approx}
	Let $f$ an $\eps$-optimal integer circulation in $H$, and $f^*$ an 
	optimal integer circulation for $H$.
	Suppose costs are scaled such that $\min \|a - b\| = 1$.
	Then, $\cost(f) \leq (1 + 6\eps) \cost(f^*)$.
\end{corollary}

\begin{corollary}
\label{corollary:match_approx}
	Let $f$ an $(\eps'/6)$-optimal integer circulation in $H$, and $M^*$ an 
	optimal size $k$ matching of $G$.
	Suppose costs are scaled such that $\min \|a - b\| = 1$.
	Then, $\cost(M_f) \leq (1 + \eps') \cost(M^*)$.
\end{corollary}

In other words, a $(\eps'/6)$-optimal circulation is sufficient for a 
$(1 + \eps')$-approximate matching.

\subsection{Algorithm description}

The Goldberg~{\etal}~\cite{GHKT17} algorithm is based on \emph{cost-scaling} or 
\emph{successive approximation}, originally due to Goldberg and 
Tarjan~\cite{GT90}.
The algorithm finds $\eps$-optimal circulations for geometrically shrinking 
values of $\eps$. 
Each period where $\eps$ holds constant is called a \emph{scale}.
Once $\eps$ is sufficiently small, the $\eps$-optimal flow is a suitable 
approximation or even optimal when costs are integer~\cite{GT90,GHKT17}.
We present this algorithm as an approximation is because costs in bichromatic 
matching (i.e. Euclidean distances) are generally not integer.

\begin{algorithm}
\caption{Cost-Scaling MCF}
\begin{algorithmic}[1]
\Function{MCF}{$H$, $\eps'$}
	\State $\eps \gets kC$
	\State $f \gets 0$
	\State $\pi \gets 0$
	\Repeat
		\State $(f, \pi) \gets$ \Call{Scale-Init}{$H$, $f$, $\pi$}
		\State $(f, \pi) \gets$ \Call{Refine}{$H$, $f$, $\pi$}
		\State $\eps \gets \eps/2$
	\Until{$\eps \leq \eps'/6$}
	\State\Return $f$
\EndFunction
\end{algorithmic}
\end{algorithm}

Note that the 0 flow is trivially $kC$-optimal for $H$.
At the beginning of each scale, \textsc{Scale-Init} takes the previous 
circulation (now $2\eps$-optimal) and transforms it into an $\eps$-optimal 
pseudoflow with $O(k)$ excess.
The rest of the scale, in \textsc{Refine}, reduces the excess in this 
pseudoflow to 0, making an $\eps$-optimal circulation.
The bulk of this section will describe and analyze \textsc{Scale-Init} and 
\textsc{Refine}.

For now, we analyze the number of scales (iterations of the outer loop).
Initially $\eps = kC$, and the algorithm is stopped once $\eps \leq \eps'/6$.
Thus, the number of scales is $O(\log(kC/\eps'))$.
For bichromatic matching, there is a simple way to preprocess the point set 
such that $C = O(n^2)$, effectively, by Sharathkumar and Agarwal~\cite{SA12}.
Using this preprocessing gives us $O(\log(n/\eps'))$ scales, instead.
We briefly describe this preprocessing step at the end of this section.

\begin{lemma}
\label{lemma:goldberg_scales}
	The cost-scaling algorithm finds a $(1 + \eps')$-approximate matching
	after $O(\log(n/\eps'))$ scales.
\end{lemma}

\subsection{\textsc{Scale-Init}}

\begin{algorithm}
\caption{Scale Initialization}
\label{algorithm:scale_init}
\begin{algorithmic}[1]
\Function{Scale-Init}{$H$, $f$, $\pi$}
	\State $\forall a \in A, \pi(a) \gets \pi(a) + \eps$
	\State $\forall b \in B, \pi(b) \gets \pi(b) + 2\eps$
	\State $\pi(t) \gets \pi(t) + 3\eps$
	\Statex %newline
	\ForAll{$(v, w) \in E_f$}
		\If{$c_\pi(w, v) < -\eps$}
			\State $f(v, w) \gets 0$
		\EndIf
	\EndFor
	\State\Return $(f, \pi)$
\EndFunction
\end{algorithmic}
\end{algorithm}

The procedure is described in Algorithm~\ref{algorithm:scale_init}.
Let the $H_f$ arcs directed from $s \to A$ or $A \to B$ or $B \to t$ be 
\emph{forward arcs}, and let those in the opposite directions be 
\emph{reverse arcs}.
The first 4 lines of \textsc{Scale-Init} raise the reduced cost of each
forward arc by $\eps$, therefore making all forward arcs $\eps$-optimal.
For example, a forward arc of $A \to B$ now has reduced cost
\begin{equation*}
	c(a, b) - (\pi(a) + \eps) + (\pi(b) + 2\eps) 
	= c_\pi(a, b) + \eps
	\geq -2\eps + \eps
	= -\eps.
\end{equation*}
In the lines after, we deal with the reduced cost of reverse arcs by simply
de-saturating them if they violate $\eps$-optimality.
Note that forward arcs will not be de-saturated in this step, since they are
now $\eps$-optimal.

\begin{lemma}
\label{lemma:scale_init}
	In $O(n)$ time, \textsc{Scale-Init} turns a $2\eps$-optimal circulation 
	into an $\eps$-optimal pseudoflow with $O(k)$ excess.
\end{lemma}
\begin{proof}
	The potential updates affect every vertex except $s$,
	so this takes $O(n)$ time.
	As for the arc de-saturations, every reverse arc is induced by positive 
	flow on a forward arc, and the number of positive flow edges in $f$ is 
	$O(k)$.
	The total number of edges that get examined by the loop is therefore 
	$O(k)$.
	In total, the time taken is $O(n)$.

	For the amount of excess, notice that new excess is only created due
	to the de-saturations of reverse arcs.
	Because the graph is unit-capacity, each de-saturation creates one unit 
	of excess.
	There are $O(k)$ reverse arcs possible, so the total created excess 
	must be $O(k)$.
\end{proof}

\subsection{\textsc{Refine}}

\textsc{Refine} is implemented using a primal-dual augmentation algorithm, 
which sends improving flows on admissible edges like the Hungarian algorithm.
Unlike the Hungarian algorithm, it uses blocking flows instead of augmenting 
paths.

\begin{algorithm}
\caption{Refinement}
\begin{algorithmic}[1]
\Function{Refine}{$H = (V, E)$, $f$, $\pi$}
	\While{$\sum_{v \in V} |e_f(v)| > 0$}
		\State $\pi \gets$ \Call{Hungarian-Search2}{$H$, $f$, $\pi$}
		\State $f' \gets$ \Call{DFS}{$H$, $f$, $\pi$}
			\Comment{$f'$ is an admissible blocking flow}
		\State $f \gets f + f'$
	\EndWhile
	\State\Return $(f, \pi)$
\EndFunction
\end{algorithmic}
\end{algorithm}

Using the properties of blocking flows and the unit-capacity input graph, 
Goldberg~{\etal} prove that there are $O(\sqrt{k})$ blocking flows before 
excess becomes 0.

\begin{lemma}[Goldberg~{\etal}~\cite{GHKT17} Lemma 3.11 and Section 6]
	Let $f$ be a pseudoflow in $H$ with $O(k)$ excess. 
	There are $O(\sqrt{k})$ blocking flows before excess is 0.
\end{lemma}

Each step of \textsc{Refine} finds an admissible blocking flow in two stages.
\begin{enumerate}
\item A \emph{Hungarian search}, which updates duals in a Dijkstra-like 
	manner until there exists an excess-deficit path of admissible edges.
	There are slight differences from the Hungarian algorithm's ``Hungarian 
	search,'' but the final running time is identical.
	We call the procedure \textsc{Hungarian-Search2} to distinguish.

\item A \emph{depth-first search} (\textsc{DFS}) through the set of admissible 
	edges to construct an admissible blocking flow.
	It suffices to repeatedly extract admissible augmenting paths until
	no more admissible excess-deficit paths remain.
	By definition, the union of such paths is a blocking flow.
\end{enumerate}
For \textsc{Hungarian-Search2}, we again use a dynamic BCP data structure to 
accelerate the Hungarian search after a once-per-\textsc{Refine} preprocessing.
To perform \textsc{DFS} quickly, we can use a dynamic \emph{nearest-neighbor} 
(NN) data structure, to discover admissible edges without handling the set of 
admissible edges explicitly. 
This is applied in a similar way as the BCP is for Hungarian search.

\begin{lemma}
	Suppose \textsc{Hungarian-Search2} can be implemented in $T_1(n, k)$ 
	time after a once-per-\textsc{Refine} $P_1(n, k)$ time preprocessing,
	and respectively \textsc{DFS} in $T_2(n, k)$ time after $P_2(n, k)$ 
	preprocessing.
	Then, \textsc{Refine} can be implemented in 
	$O(P_1(n, k) + P_2(n, k) + \sqrt{k}[T_1(n, k) + T_2(n, k)])$ time.
\end{lemma}

As we will show shortly (Lemmas~\ref{lemma:goldberg_hs_time}, 
\ref{lemma:goldberg_dfs_time}), the total running time for \textsc{Refine}
is ultimately $O((n + k\sqrt{k})\polylog(n))$.
Combining with Lemmas~\ref{lemma:goldberg_scales} and \ref{lemma:scale_init}
completes the proof of Theorem~\ref{theorem:gmcm}.

\subsubsection{Hungarian search}

\begin{algorithm}
\caption{Hungarian Search (cost-scaling)}
\begin{algorithmic}[1]
\Function{Hungarian-Search2}{$H = (V, E)$, $f$, $\pi$}
	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
	\Repeat
		\State $(v', w') \gets \argmin\{c_\pi(v, w) \mid (v, w) \in S \times (V \setminus S)\}$
		\State $\gamma \gets c_\pi(v', w')$
		\If{$\gamma > 0$}
			\Comment{make $(v', w')$ admissible if it isn't}
			\State $\pi(v) \gets \pi(v) + \gamma\lceil\frac{\gamma}{\eps}\rceil, \forall v \in S$
		\EndIf
		\State $S \gets S \cup \{w'\}$
		\Statex %newline
		\If{$e_f(w') < 0$} \Comment{$w'$ is a deficit}
			\State\Return $(f, \pi)$
		\EndIf
	\Until{$S = (A \cup B)$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}

%TODO

\begin{lemma}
\label{lemma:goldberg_hs_time}
	Using a dynamic BCP, we can implement \textsc{Hungarian-Search2} with 
	$T_1(n, k) = O(k\polylog(n))$ and $P_1(n, k) = O(n\polylog(n))$.
\end{lemma}


\subsubsection{Depth-first search}

The depth-first search is similar to \textsc{Hungarian-Search2} in that it 
relies on the relaxation of a minimum-reduced cost edge.
$P$ is a stack used for the depth-first search, recording the current path.
%TODO
\begin{algorithm}
\caption{Depth-first search}
\begin{algorithmic}[1]
\Function{DFS}{$H = (V, E)$, $f$, $\pi$}
	\State $f' \gets 0$.
	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
	\State $P \gets \emptyset$
	\Repeat
		\State $v' \gets$ \Call{Pop}{$P$}
		\If{$e_f(v') < 0$}
			\Comment{if we reached a deficit, save the path to $f'$}
			\State add to $f'$ a unit flow on the path $P$
			
			\State $P \gets \emptyset$
		\EndIf

		\Statex %newline
		\State $w' \gets \argmin\{c_\pi(v', w) \mid w \in V \setminus S\}$
		\State $\gamma \gets c_\pi(v', w')$
		\Statex %newline
		\If{$\gamma \leq 0$}
			\Comment{if $(v', w')$ is admissible, extend the current path}
			\State $S \gets S \cup \{w'\}$
			
			\State $P \gets \Call{Push}{$P$, $w'$}$

		\EndIf




		\If{$e_f(w') < 0$} \Comment{$w'$ is a deficit}
			\State\Return $(f, \pi)$
		\EndIf
	\Until{$S = \emptyset$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}

%TODO

\begin{lemma}
\label{lemma:goldberg_dfs_time}
	Using a dynamic NN, we can implement \textsc{DFS} with 
	$T_2(n, k) = O(k\polylog(n))$ and $P_2(n, k) = O(n\polylog(n))$.
\end{lemma}








\subsection{Preprocessing for $C = O(n^2)$}

%TODO
\begin{lemma}[Sharathkumar, Agarwal]
	In $O(n\log n)$ time, we can preprocess $A, B$ by partitioning into
	$(A_1, B_1), \ldots, (A_\ell, B_\ell)$ such that 
	\begin{enumerate}
	\item each $(A_i, B_i)$ has $|A_i| = |B_i|$,
	\item the union of optimal matching solutions on $(A_i, B_i)$
		is an optimal matching for $A, B$, and
	\item the spread of $(A_i, B_i)$ is $O(n^2)$.
	\end{enumerate}
\end{lemma}















{
\bibliographystyle{abbrv}
\bibliography{ref}
}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

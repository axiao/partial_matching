%!TEX encoding = UTF-8 Unicode
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[hmargin=1in,vmargin=1in]{geometry}

\usepackage[dvipsnames,usenames]{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=Blue, citecolor=Green, linkcolor=BrickRed, breaklinks, unicode}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{euscript}
\usepackage{mathpazo}
\usepackage[scaled=.90]{berasans,beramono}

%\usepackage{epsfig}
%\usepackage{floatflt}
\usepackage{graphicx}

\usepackage{microtype}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage[shortlabels, inline]{enumitem}

\usepackage{latexsym,amsmath,amssymb,amsthm}
\usepackage{algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools} % for \coloneqq

\usepackage[title]{appendix}

\usepackage{ifpdf}

%\graphicspath{{fig/}}

%\DeclareGraphicsExtensions{.pdf}

%\ifpdf
%\else
%   \DeclareGraphicsExtensions{.eps}
%\fi

\def\etal{\textsl{et~al.}}
\def\polylog{\mathop{\mathrm{polylog}}}
\def\eps{\varepsilon}
\def\softO{\widetilde{O}}
\def\bd{{\partial}}
\def\reals{\mathbb{R}}
\def\ints{\mathbb{N}}
\def\flr#1{{\lfloor #1\rfloor}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\tsupply{\lambda}
\def\fsupply{\phi}

\def\Refine{\textsc{Refine}}
\def\Update{\textsc{Update}}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{definition}[lemma]{Definition}
\numberwithin{figure}{section}

\newcommand{\lemlab}[1]{\label{lem:#1}}
\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\theolab}[1]{\label{theo:#1}}
\newcommand{\corlab}[1]{\label{cor:#1}}
\def\lemref#1{Lemma~\ref{lemma:#1}}
\def\theoref#1{Theorem~\ref{theo:#1}}
\def\corref#1{Corollary~\ref{cor:#1}}
\def\figref#1{Figure~\ref{fig:#1}} \def\secref#1{Section~\ref{sec:#1}}

\def\cost{\operatorname{cost}}
\def\parent{\operatorname{par}}
\def\short{\operatorname{short}}

\makeatletter
\long\def\@makecaption#1#2{
    \vskip 10pt
    \setbox\@tempboxa\hbox{{\footnotesize {\bf #1.} #2}}
    \ifdim \wd\@tempboxa >\hsize         % IF longer than one line:
        {\footnotesize {\bf #1.} #2\par}% THEN set as ordinary paragraph.
      \else                              %   ELSE  center.
        \hbox to\hsize{\hfil\box\@tempboxa\hfil}
    \fi}
%\dbltextfloatsep 18pt plus 2pt minus 4pt   % was 20pt plus 2pt minus 4pt
%\textfloatsep 18pt plus 2pt minus 4pt      % was 20pt plus 2pt minus 4pt
\makeatother

% for definitions
\def\EMPH#1{\textbf{\emph{\boldmath #1}}}

% ----------------------------------------------------------------------
%  Notes to myself.  The margin flags are broken, thanks to an
%  incompatibility with the geometry package.
% ----------------------------------------------------------------------
\def\n@te#1{\textsf{\boldmath \textbf{$\langle\!\langle$#1$\rangle\!\rangle$}}\leavevmode}
\def\note#1{\textcolor{red}{\n@te{#1}}}
%\renewcommand{\note}[1]{} % use to clear notes


%----------------------------------------------------------------------
% 'cramped' list style, stolen from Jeff Vitter.  Doesn't always work.
%----------------------------------------------------------------------
\def\cramped
  {\parskip\@outerparskip\@topsep\parskip\@topsepadd2pt\itemsep0pt
}


%% METAFILE
\title{ Geometric Partial Matching and Unbalanced Transportation %
\date{\today} % replace with date?
\author{
Pankaj K.\ Agarwal
\and
Allen Xiao
}
}


\begin{document}

\maketitle

\section{Introduction}

Consider the problem of finding a minimum-cost bichromatic matching between
a set of red points $A$ and a set of blue points $B$ lying in the plane,
where the cost of a matching edge $(a, b)$ is the Euclidean distance
$\|a - b\|$;
in other words, the minimum-cost bipartite matching problem on the Euclidean
complete graph $G = (A \cup B, A \times B)$.
Let $r \coloneqq |A|$ and $n \coloneqq |B|$.
Without loss of generality, assume that $r \leq n$.
We consider the problem of \EMPH{partial matching}, where the task is to
find a minimum-cost matching of size $k \leq r$.
When $k = r = n$, we say the matching instance is \EMPH{balanced}.
When $k = r < n$ ($A$ and $B$ have different sizes, but the matching is
maximal), we say the matching instance is \EMPH{unblanced}.
We call the geometric problem of finding a size $k$ matching of point sets $A$
and $B$ \EMPH{geometric partial matching}.

% \begin{TODO}
% Previous work
% \begin{itemize}\itemsep=0pt
% 	\item on matching
% 	\item on geometric matching
% 	\item on unbalanced/partial problems (GHKT, the push-relabel one, the tech report)
% \end{itemize}
% \noindent\textcolor{blue}{[Hsien: State your TODO list explicitly in the pdf file so that it's easier to read.  Make everything that you are planning to do, and put priorities on them.]}
% \end{TODO}


\subsection{Contributions}

In this paper, we present two algorithms for geometric partial matching
that are based on fitting nearest-neighbor (NN) and geometric closest pair
(BCP) oracles into primal-dual algorithms for non-geometric bipartite matching
and minimum-cost flow.
This pattern is not new, see for example 
(\ldots)\note{TODO cite}.
Unlike these previous works, we focus on obtaining running time dependencies on
$k$ or $r$ instead of $n$, that is, faster for inputs with small $r$ or $k$.
We begin in Section~\ref{section:prelim} by introducing notation for matching
and minimum-cost flow.

% O((n + k^2)\polylog n)

First in Section~\ref{section:hung}, we show that the Hungarian
algorithm~\cite{Kuhn55} combined with a BCP oracle solves geometric partial
matching exactly in time $O((n + k^2)\polylog n)$.
Mainly, we show that we can separate the $O(n\polylog n)$ preprocessing time
for building the BCP data structure from the augmenting paths' search time,
and update duals in a lazy fashion such that the number of dual updates per
augmenting path is $O(k)$.

\begin{theorem}
\label{theorem:hung}
Let $A$ and $B$ be two point sets in the plane with $|A| = r$ and $|B| = n$ satisfying $r \le n$, and let $k$ be a
parameter.  A minimum-cost geometric partial matching of size $k$
can be computed between $A$ and $B$ in $O((n + k^2)\polylog n)$ time.
\end{theorem}

% O((n + k\sqrt{k})\polylog n\log(n/\eps))

Next in Section~\ref{section:goldberg}, we apply a similar technique to the
unit-capacity min-cost circulation algorithm of Goldberg, Hed, Kaplan, and
Tarjan~\cite{GHKT17}.
The resulting algorithm finds a $(1 + \eps)$-approximation to the optimal
geometric partial matching in $O((n + k\sqrt{k})\polylog n \log(n/\eps))$ time.

\begin{theorem}
\label{theorem:gmcm}
Let $A$ and $B$ be two point sets in the plane with $|A| = r$ and $|B| = n$
satisfying $r \le n$, and let $k$ be a parameter.
A $(1+\eps)$ geometric partial matching of size $k$ can be computed between
$A$ and $B$ in $O((n + k\sqrt{k})\polylog n \log(n/\eps))$ time.
\end{theorem}

% add the ~O(rn^{3/2}) version

Our third algorithm solves the transportation problem in the unbalanced
setting.
The transportation problem is a weighted generalization of the matching
problem.
Each point of $A$ is weighted with an integer \EMPH{supply} and each point of
$B$ is weighted with integer \EMPH{demand} such that the sum of supply and
demand are equal.
The goal of the transportation problem is to find a minimum-cost mapping of
all supplies to demands, where the cost of moving a unit of supply at $a \in A$
to satsify a unit of demand at $b \in B$ is $\|a - b\|$.
For this, we use the strongly polynomial uncapacitated min-cost flow algorithm
by Orlin~\cite{O93}.
The result is an $O(n^{3/2} r \polylog n)$ time algorithm for unbalanced
transportation.
This improves over the $O(n^2 \polylog n)$ time algorithm of
Agarwal~\etal~\cite{AFPVX17} when $r = o(\sqrt{n})$.

\begin{theorem}
\label{theorem:orlin}
Let $A$ and $B$ be two point sets in the plane with $|A| = r$ and $|B| = n$
satisfying $r \le n$, with supplies and demands given by the function
$\tsupply: (A \cup B) \to \ints$ such that
$\sum_{a \in A} \tsupply(a) = \sum_{b \in B} \tsupply(b)$.
An optimal transportation map can be computed in $O(n^{3/2}r\polylog n)$ time.
\end{theorem}

%TODO STILL IN PROGRESS, WE DON'T HAVE A SOLUTION YET. REMOVE IF FAILS.

%Finally, we show that an algorithm by Orlin~\cite{} for uncapacitated min-cost
%flow can be modified to give an $O(nr\polylog n)$ time algorithm for the
%unbalanced transportation problem.
%Unlike matching, vertices may be ``matched'' (transporting) to more than one
%other vertex.
%As a result, it seems possible that the search procedure reaches vertices which
%do not contribute to a new augmenting path or towards finding unreached $B$
%vertices, forcing the search to backtrack.
%Avoiding the backtracking is the main challenge for this algorithm.
%% how?

By nature of the BCP/NN oracles we use, these results generalize to when
$\|a - b\|$ is any $L_p$ distance, and not just the Euclidean distance between
$a$ and $b$.


\section{Preliminaries}
\label{section:prelim}

\subsection{Matching}

Let $G$ be a bipartite graph between vertex sets $A$ and $B$ and edge set $E$,
with costs $c(v, w)$ for each edge $e$ in $E$.
We use $C \coloneqq \max_{e \in E} c(e)$, and assume that the problem is scaled such
that $\min_{e \in E} c(e) = 1$.
A \EMPH{matching} $M \subseteq E$ is a set of edges where no two edges share an
endpoint.
We use $V(M)$ to denote the vertices matched by $M$.
The \EMPH{size} of a matching is the number of edges in the set, and the
\EMPH{cost} of a matching is the sum of costs of its edges.
The \EMPH{minimum-cost partial matching problem (MPM)} asks to find a size-$k$
matching $M^*$ of minimum cost.

\subsection{Minimum-cost flow}

\note{Collect definitions into paragraphs.}

\paragraph{Network.}
For minimum-cost flow, let $G_0 = (V, E_0)$ be a directed graph with
nonnegative arc capacities $u(v, w)$ and costs $c(v, w)$ for each arc
$(v, w) \in E_0$.
We say $G_0$ is \EMPH{unit-capacity} if $u(v, w) = 1$ holds for all arc $(v, w)$.
Let $\fsupply: V \to \ints_{\geq 0}$ be a supply-demand function on $V$, satisfying $\sum_{v \in V} \fsupply(v) = 0$.
The positive values of $\fsupply(v)$ are referred to as \EMPH{supply}, and the negative values of $\fsupply(v)$ as \EMPH{demand}.

We augment $G_0$ to make it \EMPH{symmetric}
%for every arc $(v, w) \in E_0$ its reverse $(w, v)$ is also an arc;
and the costs
\EMPH{antisymmetric}
%$c(v, w) = -c(w, v)$).
by creating an arc $(w, v)$ for each $(v, w) \in E_0$ and define $u(w, v) = 0$
and $c(w, v) = -c(v, w)$.
Denote this set of new \EMPH{reverse arcs} by $E^R$.
\note{How do you feel about using darts and arcs to describe the distinction?}
Let $E = E_0 \cup E^R$.
From here forward, we work with the symmetric multigraph $G = (V, E)$.
A \EMPH{network} $N = (G, c, u, \fsupply)$ is a graph $G$ augmented with arc
costs, capacities, and a supply-demand function on vertices of $G$.

\paragraph{Pseudoflows.}
A \EMPH{pseudoflow} $f:E \to \ints$ is an antisymmetric function on arcs
satisfying $f(v, w) \leq u(v, w)$ for all arcs $(v, w)$.
We say that $f$ \EMPH{saturates} an arc $e$ if $f(v, w) = u(v, w)$.
The \EMPH{support} of $f$ is $E_{>0}(f) \coloneqq \{(v, w) \mid f(v, w) > 0\}$.
All our algorithms will handle integer-valued pseudoflows, so in the
unit-capacity setting an arc is either saturated or has zero flow.
Given a pseudoflow $f$, we define the \EMPH{imbalance} of a vertex to be
\[
e_f(v) \coloneqq \fsupply(v) + \sum_{(w, v) \in E}{f(w, v)} - \sum_{(v, w) \in E}{f(v, w)}.
\]
We call positive imbalance \EMPH{excess} and negative imbalance \EMPH{deficit};
and vertices with positive and negative imbalance \EMPH{excess vertices} and
\EMPH{deficit vertices}, respectively.
A vertex is \EMPH{balanced} if it has zero imbalance.
If all vertices are balanced, the pseudoflow is a \EMPH{circulation}.
The cost of a pseudoflow is
\[
\cost(f) \coloneqq \sum_{(v, w) \in E} c(v, w) \cdot f(v, w).
\]
The \EMPH{minimum-cost flow problem (MCF)} asks to find the circulation $f^*$ of
minimum cost.

\paragraph{Residual network.}
For each arc $(v, w)$, the \EMPH{residual capacity} with respect to
pseudoflow $f$ is defined to be $u_f(v, w) \coloneqq u(v, w) - f(v, w)$.
The set of \EMPH{residual arcs} is defined as
\[
E_f \coloneqq \{(v, w) \in E \mid u_f(v, w) > 0\}.
\]
We call $G_f = (V, E_f)$ the \EMPH{residual graph} with respect to pseudoflow $f$.
A pseudoflow $f'$ in $G_f$ can be ``added'' or ``augmented'' to $f$ to produce
a new pseudoflow (that is, the arc-wise addition $f + f'$ is a valid pseudoflow
in $G$).
A pseudoflow $f'$ in $G_f$ is an \EMPH{improving flow} if
\begin{enumerate}[(1)]\itemsep=0pt
\item
$0 \leq e_{f'}(v) \leq e_f(v)$ for all excess vertices $v$,
\item
$0 \leq -e_{f'}(v) \leq -e_f(v)$ for all deficit vertices $v$, and
%\item
% if $e_f(v) = 0$ then $e_{f'}(v) = 0$, \note{this follows from (1) or (2)} and
\item $\sum_{v \in V} |e_{f'}(v)| < \sum_{v \in V} |e_f(v)|$ holds.
\end{enumerate}
If improving flow $f'$ is on a simple path (from an excess vertex to a deficit
vertex), we call it an \EMPH{augmenting-path flow} \note{path flow for short? only used twice throughout the paper} and its underlying support path \note{support undefined} an
\EMPH{augmenting path}.
If $f'$ saturates at least one residual arc in every augmenting path in $G_f$,
we call $f'$ a \EMPH{blocking flow}.
In other words, for blocking flow $f'$, there is no augmenting-path flow
$f''$ in $G_f$ for which $f + (f' + f'')$ is a feasible pseudoflow in $G$.
\note{Move to Section 4.4 where blocking flow is first used.}

\subsection{Primal-dual augmentation algorithms}

The Hungarian algorithm begins with an empty matching and gradually increases its size to $k$ using \EMPH{alternating augmenting paths}.
Given a non-maximal matching $M$, an alternating augmenting path $P$ is a path
between an unmatched vertex $a \in A$ and unmatched $b \in B$.
\note{What is $A$ and $B$? Remind the readers about the bipartite graph again.}
Then, $M' = M \oplus P$ is a matching of size 1 greater.
\note{$\oplus$ undefined.  Personally I believe it's easier to define in words; using notation is fine too.}
By restricting alternating augmenting paths to edges \note{when do you use edges and when do you use arcs?}
which satisfy a certain cost condition, we can prove that each intermediate matching of size $j \leq k$ is of minimum-cost among matchings of size $j$.

There is a similar augmentation procedure for flows, which sends improving
flows to gradually reduce the imbalance for all vertices to 0, making it a
circulation.
By restricting augmentations to residual arcs satisfying a certain cost
condition (admissibility), one can prove that the resulting circulation is
minimum cost.

\note{The above paragraph might be clearer if you put it after the definition of admissibility; because you can actually provide a formal proof.  Sketch of ideas are not that useful because for experts they don't need to read it, for beginners they won't understand without former definitions.}

\paragraph{LP-duality and admissability.}
Formally, the
\EMPH{potentials} $\pi(v)$ are the variables of the linear program dual to \note{which primal problem? State the correspodning linear problems explicitly}.
The \EMPH{reduced cost} of an arc $(v, w)$ in $E_f$ with respect to $\pi$ is
\[
c_\pi(v, w) \coloneqq c(v, w) - \pi(v) + \pi(w).
\]
Note that reduced costs are antisymmetric: $c_\pi(v, w) = -c_\pi(w, v)$.
The \EMPH{dual feasibility constraint} is that $c_\pi(v, w) \geq 0$ holds for all
residual arcs \note{naturally, not the reverse arcs; thus the distinction between arcs and darts}; potentials which satisfy this constraint are said to be \EMPH{feasible}.
The linear programming \EMPH{optimality conditions} state that, for an optimal
circulation $f^*$, there are feasible potentials $\pi^*$ which satisfy
$c_{\pi^*}(v, w) = 0$ on all arcs with $f^*(v, w) > 0$.
We can similarly define potentials and reduced costs for matchings, using
$c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b)$ for edges $(a, b) \in A \times B$.

Suppose we relax the dual feasibility constraint to allow for a violation of
$\eps > 0$.
We say that a pseudoflow $f$ is \EMPH{$\eps$-optimal} \note{with respect to $\pi$} if
$c_\pi(v, w) \geq -\eps$ for all arcs $(v, w)$ in $E_f$ with $u_f(v, w) > 0$.
We say that a residual arc $(v ,w)$ satisfying $c_\pi(v, w) \leq 0$ is
\EMPH{admissible}.
We say that an improving flow $f'$ is \EMPH{admissible} if $f'(v, w) > 0$
only on admissible arcs $(v, w)$.

For matchings, we say that matching $M$ is $\eps$-optimal if $c_\pi(a, b) \leq \eps$ for
$(a, b) \in M$ and $c_\pi(a, b) \geq -\eps$ $(a, b) \in E \setminus M$.
Matching edges (resp.\ nonmatching edges) are \EMPH{admissible} if $c_\pi(a, b) \geq 0$ (resp.\ $c_\pi(a, b) \leq 0$); and an alternating augmenting path is \EMPH{admissible}
if all its edges are.
For both matching and flows, 0-optimal $f$ implies the admissibility condition
is with equality, instead ($c_\pi(v, w) = 0$).

\note{I got the sense that it might be helpful to define admissibility at the start of the flow section and the matching section separately.}

%Now, we can concretely state how admissible augmentations lead to a correct algorithm for $\eps > 0$.
\begin{lemma}
Let $f$ be an $\eps$-optimal pseudoflow in $G$ and let $f'$ be an
admissible improving flow in $G_f$.
Then $g = f + f'$ is also $\eps$-optimal.
\end{lemma}
\begin{proof}
Augmentation by $f'$ will not change the potentials, so any previously
$\eps$-optimal arcs remain $\eps$-optimal.
However, it may introduce new arcs with $u_g(v, w) > 0$, that previously had
$u_f(v, w) = 0$.
We will verify that these arcs satisfy the $\eps$-optimality condition.

If an arc is newly introduced this way, then by defintion of residual
capacities $f(v, w) = u(v, w)$.
At the same time, $u_g(v, w) > 0$ implies that $g(v, w) < u(v, w)$.
This means that $f'$ augmented flow in the reverse direction of $(v, w)$
($f'(w, v) > 0$).
By assumption, the arcs of $E_{>0}(f')$ are admissible, so $(w, v)$ was an
admissible arc ($c_\pi(w, v) \leq 0$).
By antisymmetry of reduced costs, this implies $c_\pi(v, w) \geq 0$.
Finally, $c_\pi(v, w) \geq 0 \geq -\eps$.
Thus, all arcs with $u_g(v, w) > 0$ respect the $\eps$-optimality condition,
and $g$ is $\eps$-optimal.
\end{proof}

In Section~\ref{section:goldberg}, we use $\eps$-optimality to prove the
approximation quality of an $\eps$-optimal circulation.


\section{Matching with the Hungarian algorithm}
\label{section:hung}

The Hungarian algorithm maintains a 0-optimal (initially empty) matching $M$,
and repeatedly augments by alternating augmenting paths of admissible edges
until $|M| = k$.
To this end, the algorithm maintains a set of feasible potentials $\pi$ and
updates them to find augmenting paths of admissible edges. \note{admissible augmenting path?}
It maintains the invariant that matching edges are admissible.
Since there are $k$ augmentations and each alternating path has length at most
$2k-1$, the total time spent on bookkeeping the matching is $O(k^2)$.
This leaves the analysis of the subroutine that updates the potentials and
finds an admissible augmenting path; we call this subroutine the
\EMPH{Hungarian search}.

\begin{figure*}
\centering
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Hungarian algorithm}
\begin{algorithmic}[1]
\Function{Match}{$G = (A \cup B, E)$, $k$}
	\State $M \gets \emptyset$
	\State $\pi(v) \gets 0$ for all $v \in A \cup B$
	\While{$|M| < k$}
		\State $\Pi \gets$ \Call{Hungarian-Search}{$G$, $M$, $\pi$}
		\State $M \gets M \oplus \Pi$
	\EndWhile
	\State\Return $M$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}


\begin{theorem}[Time for Hungarian algorithm]
\label{theorem:hung_orig}
Let $G = (A \cup B, A \times B)$ be an instance of geometric partial matching
with $r \coloneqq |A|$, $n \coloneqq |B|$, $r \leq n$, and parameter $k \leq r$.
Suppose the Hungarian search finds each augmenting path in $T(n, k)$ time after
a one-time $P(n, k)$ preprocessing time.
Then, the Hungarian algorithm finds the optimal size $k$ matching in time
$O(P(n, k) + k T(n, k) + k^2)$.
\end{theorem}

\subsection{Hungarian search}

Let $S$ be the set of vertices that can be reached from an unmatched $a \in A$
by admissible residual edges, initially the unmatched vertices of $A$.
The Hungarian search updates potentials in a Dijkstra's algorithm-like manner,
expanding $S$ until it includes an unmatched $b \in B$ (and thus an admissible
alternating augmenting path).
The ``search frontier'' of the Hungarian search is
$(S \cap A) \times (B \setminus S)$.
We \emph{relax} the minimum-reduced cost edge in the frontier, changing the
potentials of vertices $S$ such that the edge becomes admissible, and adding
the head of the edge into $S$. 
\note{Well, either you add both endpoints into $S$, or an admissible augmenting path is found.}

The potential update uniformly decreases the reduced costs of the frontier
edges.
Since $(a', b')$ is the minimum reduced cost frontier edge, the potential
update in line~\ref{line:hs_update} does not make any reduced cost negative,
and thus preserves the dual feasibility constraint for all edges.
The algorithm is shown below as Algorithm~\ref{algorithm:hung_hs}.

\begin{figure*}
\centering
\begin{minipage}{.8\linewidth}
\begin{algorithm}[H]
\caption{Hungarian Search (matching)}
\label{algorithm:hung_hs}
\begin{algorithmic}[1]
\Require{$c_\pi(a, b) = 0$ for all $(a, b) \in M$}
%\Statex %newline
\Function{Hungarian-Search}{$G = (A \cup B, E)$, $M$, $\pi$}
	\State $S \gets a \in (A \setminus V(M))$ \note{arbitrary $a$?}
	\Repeat
		\State $(a', b') \gets \argmin\{c_\pi(a, b) \mid (a, b) \in (S \cap A) \times (B \setminus S)\}$
		\State $\gamma \gets c_\pi(a', b')$
		\State $\pi(v) \gets \pi(v) + \gamma, \forall v \in S$
			\Comment{make $(a', b')$ admissible}
			\label{line:hs_update}
		\State $S \gets S \cup \{b'\}$
		% \Statex %newline
		\If{$b' \not\in V(M)$} \Comment{$b'$ unmatched}
			\State $\Pi \gets$ alternating augmenting path from $a$ to $b'$
			\State\Return $\Pi$
		\Else \Comment{$b'$ is matched to some $a'' \in A \cap V(M)$}
			\State $S \gets S \cup \{a''\}$
		\EndIf
	\Until{$S = A \cup B$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

By tracking the forest of relaxed edges (e.g. back pointers), it is
straightforward to recover the alternating augmenting path $\Pi$ once we reach
an unmatched $b' \in B$.
We make the following observation about the Hungarian search:

\begin{lemma}
\label{lemma:hungsearch_length}
There are at most $k$ edge relaxations before the Hungarian search finds an
alternating augmenting path.
\end{lemma}

\begin{proof}
Each edge relaxation either leads to a matched vertex in $B$ (there are at most
$k-1$ such vertices), or finds an unmatched vertex and ends the search.
\end{proof}

In general graphs, the minimum edge is typically found by pushing all
encountered $(S \cap A) \times (B \setminus S)$ edges into a priority queue.
However, in the bipartite complete graph, this may take $\Theta(rn\polylog n)$
time for each Hungarian search --- edges are being pushed into the queue even when
they are not relaxed.
We avoid this problem by finding an edge with minimum cost using \EMPH{bichromatic
closest pair} (BCP) queries on an additively weighted Euclidean distances,
for which there exist fast dynamic data structures.
Given two point sets $P$ and $Q$ in the plane, the BCP is the pair of points
$p \in P$ and $q \in Q$ minimizing the (adjusted) distance
$\|p - q\| - \omega(p) + \omega(q)$, for some real-valued vertex weights
$\omega(p)$.
In our setting, the vertex weights will mostly be set as the potentials; the
adjusted distance is equal to the reduced cost.

\note{Short history on BCP?}
The state of the art dynamic BCP data structure from Kaplan, Mulzer,
Roditty, Seiferth, and Sharir~\cite{KMRSS17} supports point insertions and deletions in
$O(\polylog n)$ time, and answers queries in $O(\log^2 n)$ time.
The following lemma, combined with Theorem~\ref{theorem:hung_orig}, completes
the proof of Theorem~\ref{theorem:hung}.

\begin{lemma}
\label{lemma:hs_time}
Using the dynamic BCP data structure from Kaplan \etal, we can implement
Hungarian search with $T(n, k) = O(k\polylog n)$ and
$P(n, k) = O(n\polylog n)$.
\end{lemma}

\begin{proof}
Recall that we maintain a BCP data structure between $P = (S \cap A)$ and
$Q = (B \setminus S)$.
Changes to the $P$ and $Q$ are entirely driven by changing $S$; that is,
updates to $S$ incur BCP insertions/deletions.
We first analyze the bookkeeping besides the potential updates, and then
show how potential updates can be implemented efficiently.

\begin{enumerate}
\item Let $S^t_0$ \note{Is there a reason why you want the subscript?  Do you ever define $S^t$?} denote the initial set $S$ at the beginning of the
	$t$-th Hungarian search, that is, the set of unmatched points in $A$
	after $t$ augmentations.
	At the very beginning of the Hungarian algorithm, we initialize
	$S^0_0 \gets A$ (meaning that $P = A$ and $Q = B$), which is a
	one-time insertion of $O(n)$ points into BCP, attributed to $P(n, k)$.
	On each successive Hungarian search, $S^t_0$ shrinks as more
	and more points in $A$ are matched.
	Assume for now that, at the beginning of the $(t+1)$-th
	Hungarian search, we are able to construct $S^t_0$ from the
	previous iteration.
	To construct $S^{t+1}_0$, we simply remove the point in $A$ that was
	matched by the $t$-th augmenting path.
	Thus, with that assumption, we are able to initialize $S$ using
	one BCP deletion operation per augmentation.

\item During each Hungarian search, points are added to $P$ (that is, some points in $A$ are
	added to $S$) and removed from $Q$ (points in $B$ added to $S$), which will happen at most once per edge relaxation.
	By Lemma~\ref{lemma:hungsearch_length} the number of relaxed
	edges is at most $k$, so the number of such BCP operations is
	also at most $k$.

\item To obtain $S^t_0$, we keep track \note{give a name to such points} of the
	points added since $S^t_0$ in the last Hungarian search
	(i.e.\ those of (2)). \note{Unclear}
	After the augmentation, we use this log \note{use the name} to delete the added
	vertices from $S$ and recover $S^t_0$.
	By the argument in (2) there are $O(k)$ of such points to
	delete, so reconstructing $S^t_0$ takes $O(k)$ BCP operations.
	\note{TODO instead of reversing a log, is persistence an easier solution to this?}
\end{enumerate}

We spend $P(n, k) = O(n \polylog n)$ time to build the initial BCP.
The number of BCP operations associated with each Hungarian search is
$O(k)$, so the time spent on BCP operations in each Hungarian search
is $O(k \polylog n)$.

As for the potential updates, we modify a trick from Vaidya~\cite{Vaidya89} to
batch potential updates.
Potentials have a \EMPH{stored value}, i.e. the current value of $\pi(v)$,
and a \EMPH{true value}, which may have changed from $\pi(v)$.
The algorithm uses the true value when dealing with reduced costs and updates
the stored value rarely; we explain the mechanism shortly.

Throughout the course of the algorithm, we maintain a nonnegative value
$\delta$ (initially 0) which aggregates potential changes.
Vertices that are added to $S$ are immediately added to a BCP data structure
with weight $\omega(p) \gets \pi(p) - \delta$, for whatever value $\delta$ is
at the time of insertion.
When the points of $S$ have potentials increased by $\gamma$ in (2), we instead
raise $\delta \gets \delta + \gamma$.
Thus, true value for any potential of a point in $S$ is $\omega(p) + \delta$.
For points of $(A \cup B) \setminus S$, the true potential is equal to the
stored potential.

Since potentials for $S$ points are uniformly offsetted by $\delta$, the
minimum edge returned by the BCP oracle does not change.
Once a point is removed from $S$, we update its stored potential
to be $\pi(p) \gets \omega(p) + \delta$, for the current value of $\delta$.
Importantly, $\delta$ is not reset at the end of a Hungarian search, and
persists throughout the entire algorithm.
This way, the unmatched points in each $S^t_0$ have their true potentials
accurately represented by $\delta$ and $\omega(p)$.

The number of updates to $\delta$ is equal to the number of edge relaxations,
which is $O(k)$ per Hungarian search.
We update stored potentials when removing a point from $S$ (by the rewind
mechanism, or due to an augmentation) which occurs $O(k)$ times per Hungarian
search.
The time spent on potential updates per Hungarian search is therefore $O(k)$.
Overall, the time spent per Hungarian search is $T(n, k) = O(k\polylog n)$.

\note{The proof gets more handwavy as the paragraph progresses.  Consider a revision after this round.}
\end{proof}


\section{Matching with the Goldberg~{\etal} algorithm}
\label{section:goldberg}

\note{Remind the readers what you want to achieve in this section.}
In this section, we describe a $(1+\eps)$-approximation algorithm for geometric
partial matching which proves Theorem~\ref{theorem:gmcm}.
We build atop a \EMPH{cost-scaling} algorithm for unit-capacity min-cost flow
from Goldberg~\etal~\cite{GHKT17}.
First, we give a cost-preserving near-linear time reduction from geometric
partial matching to unit-capacity min-cost flow, which allows us to apply the
cost-scaling algorithm to partial matching.

\subsection{MPM to unit-capacity MCF reduction}
\label{subsection:mcm_mcf_reduction}

For a partial matching problem on a bipartite graph $G = (A \cup B, E_0)$ with parameter $k$, we
direct each bipartite edge in $E_0$ from $A$ to $B$, with cost equal to the
original cost $c(a, b)$ and capacity $1$.
Next, we add a dummy vertex $s$ with arcs $(s, a)$ to every vertex $a$ in $A$,
and a dummy vertex $t$ with arcs $(b, t)$ for every vertex $b$ in $B$,
all with cost $0$ and capacity $1$.
For each of the above arcs $(v, w)$, we also add a reverse arc $(w, v)$ with
cost $c(w, v) = -c(v, w)$ and capacity $0$. \note{is this consistent with the other residual graph description?}
Let the complete set of arcs be $E$, and $V = A \cup B \cup \{s, t\}$.
Set $\fsupply(s) = k$, $\fsupply(t) = -k$, and $\fsupply(v) = 0$ for all other
vertices. \note{What is $\phi$?  Undefined in this section.}
Let the resulting graph be $H = (V, E)$, and the network
$N_H = ((V, E), c, u, \fsupply)$.
We call $H$ the \EMPH{reduction graph} of the partial matching instance on
$A, B$ with parameter $k$, and $N_H$ the \EMPH{reduction network}.

\begin{observation}
\label{observation:dag}
	The arcs of $H$ with positive capacity form a directed acyclic graph.
\end{observation}

In other words, there will be no cycles of positive flow in circulations on
$H$.
With this, we can show that the number of arcs used by any integer pseudoflow
in $H$ is asymptotically bounded by the excess of the pseudoflow.

\begin{lemma}
\label{lemma:reduction_count}
Let $f$ be an integer pseudoflow in $H$ with $O(k)$ excess.
Then, $|E_{>0}(f)| = O(k)$.
\end{lemma}

\begin{proof}
By Observation~\ref{observation:dag}, the positive-flow edges of $f$ do not
contain a cycle.
Thus, $E_{>0}(f)$ can be decomposed into a set of inclusion-maximal paths,
each of which creates a single unit of excess if it does not terminate at $t$.
A path may also create excess at $t$ if there are at least $k$ other paths
terminating at $t$.
By assumption, there are $O(k)$ units of excess to which we can associate
paths, and at most $k$ paths that we cannot associate with a unit of excess.
The maximum length of any path with positive-flow arcs in $H$ is $3$ by
construction.
We conclude that the number of positive flow arcs in $f$ is $O(k)$.
\end{proof}

It is straightforward to show that any integer circulation on $H$ uses exactly
$k$ of the $A$-to-$B$ arcs, which correspond to the edges of a size-$k$
matching.
For a circulation $f$ in $H$, we use $M_f$ to denote the
corresponding matching.

\begin{observation}
\label{observation:reduction_cost}
	Let $f$ be an integer circulation on $H$; $f$ uses exactly $k$ of the
	$A$-to-$B$ arcs which correspond to a size $k$ matching on $A, B$.
	Call this matching $M_f$, then $\cost(f) = \cost(M_f)$
\end{observation}

Observe that $\cost(f) = \cost(M_f)$, so an $\alpha$-approximation to the MCF
problem on $H$ is an $\alpha$-approximation to the matching problem on $G$.
In the next lemma, we show how $\eps$-optimality implies approximation on $H$.

\note{Move the definitions of $\eps$-optimality and admissibility for flows here?}

\begin{lemma}
\label{lemma:goldberg_cost_add}
Let $f$ an $\eps$-optimal integer circulation in $H$, and $f^*$ an optimal
integer circulation for $H$.
Then, $\cost(f) \leq \cost(f^*) + 6k\eps$.
\end{lemma}

\begin{proof}
We label the arcs of the residual network $H_f$ as follows:
\begin{itemize}\itemsep=0pt
\item \EMPH{forward arcs}: arcs directed from $s$-to-$A$ or $A$-to$B$ or
	$B$-to-$t$, and 
\item \EMPH{reverse arcs}: arcs point in the opposite directions.
\end{itemize}
Reverse arcs must be induced by positive flow in the opposite direction
(forward arc between the same points), since $H$ only has arcs in the forward
direction.
Since $f$ is a circulation, $E_{>0}(f)$ can be decomposed into $k$ paths from
$s$ to $t$.
Each $s$-to-$t$ path in $H$ is length 3, so the total number of reverse arcs
is $3k$.

There exists a residual flow $g$ in $H_f$ such that $f + g = f^*$.
Since both $f$ and $f^*$ are both circulations and $H$ is unit-capacity, $g$ is
comprised of unit flows on a collection edge-disjoint residual cycles,
$\Gamma_1, \ldots \Gamma_\ell$.
Observe that each residual cycle $\Gamma_i$ must have exactly half of its arcs 
being reverse arcs, thus $\sum_i |\Gamma_i| \leq 6k$.

Let $\pi$ be a set of potentials which certify that $f$ is $\eps$-optimal.
For residual cycles, we have that $c_\pi(\Gamma_i) = c(\Gamma_i)$, since the
potential terms telescope.
We then see that
\begin{equation*}
	\cost(f) - \cost(f^*) 
	= \sum_i c(\Gamma_i) 
	= \sum_i c_\pi(\Gamma_i)
	\geq \sum_i |\Gamma_i|(-\eps)
	\geq -6k\eps,
\end{equation*}
where the second-to-last inequality follows from the $\eps$-optimality of $f$
with respect to $\pi$.
Rearranging, we have that $\cost(f) \leq \cost(f^*) + 6k\eps$.
\end{proof}

\begin{corollary}
\label{corollary:goldberg_cost_add}
Let $f$ an $(\eps/6k)$-optimal integer circulation in $H$, and $f^*$ an optimal
integer circulation for $H$.
Then, $\cost(f) \leq \cost(f^*) + \eps$.
\end{corollary}

We use a technique from Sharathkumar and Agarwal~\cite{SA12} to transform this
additive $\eps$ approximation into a relative $(1+\eps)$ approximation
for geometric matching.
Let $\EuScript{T}$ be the minimum spanning tree (MST) of $A \cup B$ and order
its edges by decreasing length as $e_1, \ldots, e_{r+n-1}$.
Let $T_i$ be the connected components of $\EuScript{T}$ induced by
$e_{i+1}, \ldots, e_{r+n-1}$, i.e. from removing all MST edges of length at
least $c(e_i)$.
For each component $T \in T_i$, let $A_T = T \cap A$ and $B_T = T \cap B$
respectively.

Let $j_1$ be minimum index such that there exists a component $T \in T_{j_1}$
with $|A_T| \neq |B_T|$.
Choose $j_2$ to be the maximum index such that
$c(e_{j_2}) \geq n^2 c(e_{j_1})$.
We partition $A, B$ into the sets $A_T, B_T$ for each $T \in T_{j_2}$.
Since $j_2 < j_1$, $|A_T| = |B_T|$ for all $T \in T_{j_2}$.

\begin{lemma}[\cite{SA12}, Section 3.5]
\label{lemma:sa_partition}
Consider the partition of $A, B$ into sets $A_T, B_T$ for each $T \in T_{j_2}$.
Let $M^*$ be the optimal matching on $A, B$ and $M^*_T$ be the optimal matching
on $T \in T_{j_2}$, and let the diameter of a component $T$ be
$C_T \coloneqq \max_{p, q \in A_T \cup B_T} \|p - q\|$.
Then,
\begin{enumerate}[(i)]
\item $M^* = \bigcup_{T \in T_{j_2}} M^*_T$,
\item $C_T \leq kn^2 c(e_{j_1})$ for all $T \in T_{j_2}$, and
\item $c(e_{j_1}) \leq \cost(M^*)$.
\end{enumerate}
Furthermore, this partition can be constructed in $O(n\polylog n)$ time
using a dynamic data structure for bichromatic closest pair.
\end{lemma}

The proof of these properties can be found in the original paper
\cite[Section 3.5]{SA12}, but we reproduce the rest of their proof below
(tailored for $\eps$-optimality).
Given this lemma, we can construct the MCF reduction network $N_H$ for each
component $T = A_T \cup B_T$, find an $(\eps c(e_{j_1})/6kn)$-optimal
circulation $f_T$ for each, and then $\bigcup_{T \in T_{j_2}}M_{f_T}$ is a 
$(1+\eps)$-approximate partial matching for $A, B$
Let $f^*_T$ be the optimal flow on $H$ for the component $T$.
By combining Corollary~\ref{corollary:goldberg_cost_add},
Observation~\ref{observation:reduction_cost}, and
Lemma~\ref{lemma:sa_partition},
\begin{equation*}
\begin{aligned}
	\cost(\bigcup_{T \in T_{j_2}} M_{f_T})
		&= \sum_{T \in T_{j_2}} \cost(M_{f_T}) \\
		&= \sum_{T \in T_{j_2}} \cost(f_T) \\
		&\leq \sum_{T \in T_{j_2}} \cost(f^*_T) + \eps c(e_{j_1})/n \\
		&= \sum_{T \in T_{j_2}} \cost(M^*_T) + \eps c(e_{j_1})/n \\
		&\leq \cost(M^*) + \eps c(e_{j_1}) \\
		&\leq (1 + \eps) \cost(M^*).
\end{aligned}
\end{equation*}

\begin{corollary}
\label{corollary:cost_scale_approx}
If an algorithm can compute $(\eps c(e_{j_1})/6kn)$-optimal circulation for the
reduction network $N_H$ of a point set with diameter
$C \leq kn^2 \cdot c(e_{j_1})$, then we can find a $(1+\eps)$-approximate
partial matching of $A, B$, after $O(n\polylog n)$ extra preprocessing time.
\end{corollary}

\subsection{Algorithm description}

Pseudocode for the cost-scaling algorithm is given in
Algorithm~\ref{algorithm:cost-scaling}.
The Goldberg~{\etal}~\cite{GHKT17} algorithm is based on \EMPH{cost-scaling} or
\EMPH{successive approximation}, originally due to Goldberg and
Tarjan~\cite{GT90}.
The algorithm finds $\eps$-optimal circulations for geometrically shrinking
values of $\eps$.
Each iteration of the outer loop (where $\eps$ holds single value) is called a
\EMPH{cost scale}.
Once $\eps$ is sufficiently small, the $\eps$-optimal flow is a suitable
approximation (or even an optimal flow itself when costs are integers)~\cite{GT90,GHKT17}.
We present this algorithm as an approximation is because costs in the geometric
partial matching settings (with respect to Euclidean distances) are generally not integers.

\begin{figure*}[h]
\centering
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Cost-Scaling MCF}
\label{algorithm:cost-scaling}
\begin{algorithmic}[1]
\Function{MCF}{$H$, $\eps^*$}
	\State $\eps \gets kC$,
	$f \gets 0$,
	$\pi \gets 0$
	\While{$\eps > \eps^*/6$}
		\State $(f, \pi) \gets$ \Call{Scale-Init}{$H$, $f$, $\pi$}
		\State $(f, \pi) \gets$ \Call{Refine}{$H$, $f$, $\pi$}
		\State $\eps \gets \eps/2$
	\EndWhile
	\State\Return $f$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Note that the zero flow is trivially $kC$-optimal for $H$.
At the beginning of each scale, \textsc{Scale-Init} takes the previous
circulation (now $2\eps$-optimal) and transforms it into an $\eps$-optimal
pseudoflow with $O(k)$ excess.
For the rest of the scale, the procedure \textsc{Refine}, reduces the excess in
the newly constructed pseudoflow to zero, making it an $\eps$-optimal
circulation.
Thus, the algorithm produces an $\eps^*$-optimal circulation after
$O(\log(kC/\eps^*))$ scales.

\begin{lemma}
\label{lemma:goldberg_scales}
For each subproblem in Corollary~\ref{corollary:cost_scale_approx},
the cost-scaling algorithm requires $O(\log(kn/\eps^*))$ scales.
\end{lemma}

\begin{proof}
Recall that the subproblems in Corollary~\ref{corollary:cost_scale_approx} hav
diameter $C \leq kn^2 \cdot c(e_{j_1})$ and ask for an
$(\eps^* c(e_{j_1})/6kn)$-optimal circulation.
The number of cost scales is bounded above by
\begin{equation*}
	O(\log(kC/\eps^*))
	= O\left(\log\left(\frac{kn^2 \cdot c(e_{j_1})}{\eps^* c(e_{j_1})/6kn}\right)\right)
	= O(\log(kn/\eps^*)).
\end{equation*}
\end{proof}

\subsection{\textsc{Scale-Init}}

The procedure is described in Algorithm~\ref{algorithm:scale_init}.

\begin{figure*}[h]
\centering
\begin{minipage}{.5\linewidth}
\begin{algorithm}[H]
\caption{Scale Initialization}
\label{algorithm:scale_init}
\begin{algorithmic}[1]
\Function{Scale-Init}{$H$, $f$, $\pi$}
	\State $\forall a \in A, \pi(a) \gets \pi(a) + \eps$
	\State $\forall b \in B, \pi(b) \gets \pi(b) + 2\eps$
	\State $\pi(t) \gets \pi(t) + 3\eps$
	%\Statex %newline
	\ForAll{$(v, w) \in E_{>0}(f)$}
		\If{$c_\pi(w, v) < -\eps$}
			\State $f(v, w) \gets 0$
		\EndIf
	\EndFor
	\State\Return $(f, \pi)$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Let the $H_f$ arcs directed from $s \to A$ or $A \to B$ or $B \to t$ be
\EMPH{forward arcs}, and let those in the opposite directions be
\EMPH{reverse arcs}.
\note{Already did so in Lemma 4.3; might want to move this definition to somewhere before lemma 4.3.}
The first four lines \note{try not use use specific numbers, in case you change the pseudocode later} of \textsc{Scale-Init} raise the reduced cost of each
forward arc by $\eps$, therefore making all forward arcs $\eps$-optimal.
\note{Instead of an example, mention that at the start of the iteration, every forward arc is $2\eps$-optimal.}
% For example, a forward arc of $A \to B$ now has reduced cost
% \begin{equation*}
% 	c(a, b) - (\pi(a) + \eps) + (\pi(b) + 2\eps)
% 	= c_\pi(a, b) + \eps
% 	\geq -2\eps + \eps
% 	= -\eps.
% \end{equation*}
In the lines after \note{the for-loop}, we deal with the reduced cost of reverse arcs by simply
de-saturating them if they violate $\eps$-optimality.
Note that forward arcs will not be de-saturated in this step, since they are
now $\eps$-optimal.

\begin{lemma}
\label{lemma:scale_init}
\textsc{Scale-Init} turns a $2\eps$-optimal circulation into an
$\eps$-optimal pseudoflow with $O(k)$ excess in $O(n)$ time.
\end{lemma}

\begin{proof}
The potential updates affect every vertex except $s$, so this takes $O(n)$
time.
As for the arc de-saturation, every reverse arc is induced by positive flow on
a forward arc, and the number of positive flow edges in $f$ is $O(k)$ by
Lemma~\ref{lemma:reduction_count}.
The total number of edges examined by the loop is $O(k)$.
In total, this takes $O(n)$ time.

%For the amount of excess,
Notice that new excess vertex is only created due to the
de-saturation of reverse arcs.
Because the arcs in the graph has unit capacity, each de-saturation creates one unit of
excess.
By Lemma~\ref{lemma:reduction_count}, there are $|E_{>0}(f)| = O(k)$ reverse
arcs, so the total excess created must be $O(k)$.
\end{proof}


\subsection{\textsc{Refine}}

\textsc{Refine} is implemented using a primal-dual augmentation algorithm,
which sends improving flows on admissible edges like the Hungarian algorithm.
Unlike the Hungarian algorithm, it uses blocking flows instead of augmenting
paths.

\begin{figure*}[ht]
\centering
\begin{minipage}{.8\linewidth}
\begin{algorithm}[H]
\caption{Refinement}
\label{algorithm:refine}
\begin{algorithmic}[1]
\Function{Refine}{$H = (V, E)$, $f$, $\pi$}
	\While{$\sum_{v \in V} |e_f(v)| > 0$}
		\State $\pi \gets$ \Call{Hungarian-Search2}{$H$, $f$, $\pi$}
		\State $f' \gets$ \Call{DFS}{$H$, $f$, $\pi$}
			\Comment{$f'$ is an admissible blocking flow}
		\State $f \gets f + f'$
	\EndWhile
	\State\Return $(f, \pi)$
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Using the properties of blocking flows and the unit-capacity input graph,
Goldberg~{\etal}~\cite{GHKT17} prove that there are $O(k)$ blocking
flows before excess becomes 0, but on a slightly different reduction graph
and under a slightly different model of minimum-cost flow.
We provide a sketch of their proof technique adapted for the reduction network
$N_H$.

\note{HC: I am not familiar enough with their algorithm; do you think it's a straightforward application of the technique, or are there something subtle that requires a complete proof?}

\begin{lemma}[Goldberg~{\etal}~{\cite[Lemma~3.11 and \S{6}]{GHKT17}}]
\label{lemma:goldberg_refine_iterations}
Let $f$ be a pseudoflow in $H$ with $O(k)$ excess.
There are $O(\sqrt{k})$ blocking flows before excess is 0.
\end{lemma}

\begin{proof}
\note{TODO: proof sketch}%TODO needs a proof sketch at least
\end{proof}

An \EMPH{iteration} of \textsc{Refine} is a complete execution of the main loop
in Algorithm~\ref{algorithm:refine}.
Each iteration of \textsc{Refine} finds an admissible blocking (improving) flow
in two stages, and then augments the current pseudoflow by the blocking flow.
\begin{enumerate}
\item A \EMPH{Hungarian search}, which updates duals in a Dijkstra-like
	manner until there is an excess-deficit path of admissible edges.
	This is different from the procedure \textsc{Hungarian-Search} used for
	matching.
	We call the procedure \textsc{Hungarian-Search2} to distinguish.
\item A \EMPH{depth-first search} (\textsc{DFS}) through the set of admissible
	edges to construct an admissible blocking flow.
	It suffices to repeatedly extract admissible augmenting paths until
	no more admissible excess-deficit paths remain.
	By definition, the union of such paths is a blocking flow.
\end{enumerate}
Both procedures traverse the residual graph using admissible arcs from the set
of excess vertices.
Each step of these procedures \EMPH{relaxes} a minimum-reduced cost arc from a
visited vertex to an unvisited vertex, until a deficit vertex is visited.
We associate each relaxation step with its newly-visited vertex.

\begin{lemma}
\label{lemma:goldberg_refine_time}
Suppose \textsc{Hungarian-Search2} can be implemented in $T_1(n, k)$ time after
a once-per-\textsc{Refine} $P_1(n, k)$ time preprocessing, and
\textsc{DFS} can be implemented in $T_2(n, k)$ time after $P_2(n, k)$ preprocessing.
Then, \textsc{Refine} can be implemented in time
\[
O(P_1(n, k) + P_2(n, k) + \sqrt{k}T_1(n, k) + \sqrt{k}T_2(n, k) + k\sqrt{k}).
\]
\end{lemma}

As we will show shortly (Lemmas~\ref{lemma:goldberg_hs_time} and \ref{lemma:goldberg_dfs_time}), the total running time for \textsc{Refine} is
$O((n + k\sqrt{k})\polylog n)$.
Combining with Lemmas~\ref{lemma:goldberg_scales} and \ref{lemma:scale_init}
completes the proof of Theorem~\ref{theorem:gmcm}.
At a high level, our analysis strategy is to charge relaxation events in
the search to arcs of $E_{>0}(f)$.
We first extend Lemma~\ref{lemma:reduction_count} to bound the size of
$E_{>0}(f)$ throughout \textsc{Refine}, by observing that the amount of excess
decreases in each iteration of \textsc{Refine}.

\begin{corollary}
\label{corollary:support_size_during}
Let $f$ be the pseudoflow before or after any iteration of \textsc{Refine}.
Then, $|E_{>0}(f)| = O(k)$.
\end{corollary}

We discuss some challenges of our analysis and resolve them, before giving
the details of \textsc{Hungarian-Search2} and \textsc{DFS}.

\subsubsection{Empty vertices and the shortcut graph}

As it turns out, there are some vertices whose relaxation events we cannot
charge to the support size.
However, we can replace $H_f$ with an equivalent graph that excludes them,
and run \textsc{Hungarian-Search2} and \textsc{DFS} on the resulting graph.

We say $v \in A \cup B$ is an \EMPH{empty vertex} if $e_f(v) = 0$ and no edges
of $E_{>0}(f)$ adjoin $v$.
\note{Hmm. How do you feel about calling them "irrelavant vertices"?}
We are unable to charge relaxation steps involving empty vertices to
$|E_{>0}(f)|$, so the algorithm must deal with them separately.
Namely, there is no edge with $f(e) > 0$ adjacent to an empty vertex,
reaching an empty vertex does not terminate the search, and there may be
$\Omega(n)$ empty vertices at once (consider $H_{f = 0}$, the residual graph
of the empty flow).
We use $A_\emptyset$ and $B_\emptyset$ to denote the empty vertices of $A$ and
$B$ respectively.
Vertices that are not empty are called \EMPH{non-empty vertices}.

For an empty vertex $v$, either residual in-degree ($v \in A_\emptyset$) or
residual out-degree ($v \in B_\emptyset$) is 1.
Call a length 2 paths through $v$ to/from non-empty vertices an
\EMPH{empty 2-path}.
For example, if $v \in A_\emptyset$ (resp. $v \in B_\emptyset$), then its empty
2-paths have the form $(s, v, b)$ (resp. $(a, v, t)$) for each
$b \in B \setminus B_\emptyset$ (resp. $a \in A \setminus A_\emptyset$).
We say that $(s, v, b)$ is an empty 2-path \EMPH{surrounding} empty vertex $v$.
Separately, we define the length 3 $s$-$t$ paths that pass through two empty
vertices to be \EMPH{empty 3-paths}.
As with 2-paths, we say an empty 3-path $(s, v_1, v_2, t)$ surrounds
$v_1 \in A_\emptyset$ and $v_2 \in B_\emptyset$.

As for the costs of empty paths, consider an empty 2-path $(s, v, b)$ that
surrounds $v \in A_\emptyset$.
Because reduced costs telescope for residual paths, the reduced cost of
$(s, v, b)$ does not depend on the potential of $v$.
\begin{equation*}
	c_\pi((s, v, b)) = c_\pi(s, v) + c_\pi(v, b) = c(v, b) - \pi(s) + \pi(b)
\end{equation*}
Something similar holds for empty 2-paths surrounding $B_\emptyset$ vertices,
and empty 3-paths.

We construct the \EMPH{shortcut graph} $\tilde{H}_f$ from $H_f$ by removing all
empty vertices and their adjacent edges, and then inserting a direct arc
between the end points of each empty path $\Pi$ of equal cost.
We call this direct edge the \EMPH{shortcut} $\short(\Pi)$ of empty path $\Pi$.
For example, the empty 2-path $(s, v, b)$ for $v \in A_\emptyset$ is replaced
with a shortcut $(s, b)$ of cost $c(\short(s, v, b)) \coloneqq c(v, b)$.
Similarly, the empty 3-path $(s, v_1, v_2, t)$ would be replaced with a
shortcut $(s, t)$ of cost $c(\short((s, v_1, v_2 t))) \coloneqq c(v_1, v_2)$.

The resulting multigraph $\tilde{H}_f$ contains only the non-empty vertices of
$V$, and has the same connectivity between non-empty vertices as $H_f$.
Consider a path $\Pi$ from non-empty $v$ to non-empty $w$ in $H_f$.
Any empty vertex in $\Pi$ is surrounded by an empty 2- or 3-path contained
in $\Pi$, since the only nontrivial residual paths through an empty vertex are
its surrounding empty paths.
Thus, there is a corresponding $v$-to-$w$ path $\tilde{\Pi}$ in $\tilde{H}_f$
by replacing each empty path contained in $\Pi$ with its shortcut.
Furthermore, we have $c(\Pi) = c(\tilde{\Pi})$.
We argue now that $\tilde{H}_f$ is fine as a surrogate for $H_f$, by showing
that we can recover $\eps$-optimal potentials for the non-empty vertices.

\begin{lemma}
\label{lemma:empty_correct}
Let $\tilde{\pi}$ be a $\eps$-optimal set of potentials for non-empty
vertices of $H_f$.
Construct potentials $\pi$, extending $\tilde{\pi}$ to empty vertices, by
setting $\pi(a) \gets \tilde{\pi}(s)$ for $a \in A_\emptyset$ and
$\pi(b) \gets \tilde{\pi}(t)$ for $b \in B_\emptyset$.
Then,
\begin{enumerate}
\item $\pi$ is a set of $\eps$-optimal potentials for $H_f$, and
\item if a shortcut $\short(\Pi)$ is admissible under $\tilde{\pi}$,
	then every arc of $\Pi$ is admissible under $\pi$.
\end{enumerate}
\end{lemma}

\begin{proof}
Reduced costs for non-empty to non-empty arcs are unchanged between
$\tilde{\pi}$ and $\pi$, so $\eps$-optimality are preserved for these.
Recall that an empty path is comprised of one $A$-to-$B$ arc, and 1 or 2
zero-cost arcs (connecting the empty vertex/vertices to $s$ and $t$).
With our choice of empty vertex potentials, we observe that the zero-cost arcs
have reduced cost 0:
for an empty $a \in A_\emptyset$, $c_\pi(s, a) = 0$, for an empty
$b \in B_\emptyset$, $c_\pi(b, t) = 0$.
These arcs are both $\eps$-optimal ($\geq -\eps$) and admissible ($\leq 0$), so
it remains to prove $\eps$-optimality and admissibility for arcs $(a, b)$ where
either $a$ or $b$ is an empty vertex.

Let $(a, b) \in A \times B$ such that at least one of $a$ or $b$ is empty.
There exists an empty path $\Pi$ that contains $(a, b)$.
Observe that $c_\pi(a, b) = c_\pi(\Pi)$,
which we can prove for all varieties of empty paths.
\begin{itemize}
\item If $\Pi = (s, a, b)$ for $a \in A_\emptyset$:
	\begin{equation*}
	c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b) = c(a, b) - \pi(s) + \pi(b) = c_\pi(\Pi)
	\end{equation*}
\item If $\Pi = (a, b, t)$ for $b \in B_\emptyset$:
	\begin{equation*}
	c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b) = c(a, b) - \pi(a) + \pi(t) = c_\pi(\Pi)
	\end{equation*}
\item If $\Pi = (s, a, b, t)$ for $a \in A_\emptyset$ and $b \in B_\emptyset$:
	\begin{equation*}
	c_\pi(a, b) = c(a, b) - \pi(a) + \pi(b) = c(a, b) - \pi(s) + \pi(t) = c_\pi(\Pi)
	\end{equation*}
\end{itemize}
By construction, $c_\pi(\Pi) = c_{\tilde{\pi}}(\short(\Pi))$, so we have
$c_\pi(a, b) = c_{\tilde{\pi}}(\short(\Pi)) \geq -\eps$ and $(a, b)$ is
$\eps$-optimal.
Additionally, if $\short(\Pi)$ is admissible under $\tilde{\pi}$, then so is
$(a, b)$ under $\pi$.
Empty paths cover all arcs adjoining empty vertices, so we have proved both
parts of the lemma for all arcs in $H_f$.
\end{proof}

In \textsc{Refine}, we do not explicitly construct $\tilde{H}_f$ for running
\textsc{Hungarian-Search2} or \textsc{DFS}, but query its edges using BCP/NN
oracles and min/max heaps on elements of $H_f$.
Potentials for empty vertices are only required at the end of \textsc{Refine}
(for the next scale), and right before an augmentation sends flow through an
empty path, making its surrounded vertices non-empty.
During these occasions, we use the procedure in Lemma~\ref{lemma:empty_correct}
to find feasible, $\eps$-optimal potentials for empty vertices which
also preserve the structure of admissibility.

\begin{lemma}
\label{lemma:empty_updates}
The number of end-of-\textsc{Refine} empty vertex potential updates is $O(n)$.
The number of augmentation-induced empty vertex potential updates in each
invocation of \textsc{Refine} is $O(\sum_i N_i)$ where $N_i$ is the number
of positive flow arcs in the $i$-th blocking flow.
\end{lemma}

\begin{proof}
The number of end-of-\textsc{Refine} potential updates is $O(n)$.
Each update due to flow augmentation involves a blocking flow sending positive
flow through an empty path, causing a potential update on the surrounded
empty vertex.
We charge this potential update to the edges of that empty path, which are in
turn arcs with positive flow in the blocking flow.
For each blocking flow, no positive arc is charged more than twice.
It follows that the number of augmentation-induced updates is $O(N_i)$ for the
$i$-th blocking flow, and $O(\sum_i N_i)$ over the course of \textsc{Refine}.
\end{proof}

Ultimately, we prove that $\sum_i N_i = O(k\sqrt{k})$, but this requires that
we explain the process creating each blocking flow.
We revisit this lemma after analyzing \textsc{DFS}.

\subsubsection{Hungarian search}

\begin{figure*}
\centering
\begin{minipage}{.8\linewidth}
\begin{algorithm}[H]
\caption{Hungarian Search (cost-scaling)}
\begin{algorithmic}[1]
\Function{Hungarian-Search2}{$H = (V, E)$, $f$, $\pi$}
	\State $\tilde{H}_f \gets$ the shortcut graph of $H_f$
	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
	\Repeat
		\State $(v', w') \gets \argmin\{c_\pi(v', w') \mid v' \in S, w' \not\in S, (v', w') \in \tilde{H}_f)\}$
			\label{line:hs_relaxation}
		\State $\gamma \gets c_\pi(v', w')$
		\If{$\gamma > 0$}
			\Comment{make $(v', w')$ admissible if it isn't}
			\State $\pi(v) \gets \pi(v) + \gamma\lceil\frac{\gamma}{\eps}\rceil, \forall v \in S$
		\EndIf
		\State $S \gets S \cup \{w'\}$
		\If{$e_f(w') < 0$} \Comment{reached a deficit}
			\State\Return $\pi$
		\EndIf
	\Until{$S = (A \setminus A_\emptyset) \cup (B \setminus B_\emptyset)$}
	\State\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure*}

Logically, we are executing the Hungarian search (``raise prices'') from
\cite[Section 3.2]{GHKT17} on the shortcut graph $\tilde{H}_f$.
We describe how we can query the minimum-reduced cost arc leaving $S$ in
$O(\polylog n)$ time, for the shortcut graph, without constructing
$\tilde{H}_f$ explicitly.
For this purpose, let $S'$ be a set of ``reached'' vertices maintained
alongside $S$, identical except whenever a shortcut is relaxed, we add its
surrounded empty vertices to $S'$ in addition to its (non-empty) endpoints.
Observe that the arcs of $\tilde{H}_f$ leaving $S$ fall into $O(1)$ categories.
\begin{enumerate}
\item Non-shortcut reverse arcs $(v, w)$ with $(w, v) \in E_{>0}(f)$.
	For these, we can maintain a min-heap on $E_{>0}(f)$ arcs as each $v$
	arrives in $S$.
\item Non-shortcut $A$-to-$B$ forward arcs.
	For these, we can use a BCP data structure between
	$(A \setminus A_\emptyset) \cap S$ and
	$(B \setminus B_\emptyset) \setminus S$, weighted by potential.
\item Non-shortcut forward arcs from $s$-to-$A$ and from $B$-to-$t$.
	For $s$, we can maintain a min-heap on the potentials of
	$B \setminus S$, queried while $s \in S$.
	For $t$, we can maintain a max-heap on the potentials of
	$A \cap S$, queried while $t \not\in S$.

\item Shortcut arcs $(s, b)$ corresponding to empty 2-paths from $s$ to
	$b \in (B \setminus B_\emptyset) \setminus S'$.
	For these, we maintain a BCP data structure with $P = A_\emptyset$,
	$Q = (B \setminus B_\emptyset) \setminus S')$ with weights
	$\omega(p) = \pi(s)$ for all $p \in P$, and $\omega(q) = \pi(q)$ for
	all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 2-path $(s, a, b)$.
	This is only queried while $s \in S'$.
\item Shortcut arcs $(a, t)$ corresponding to empty 2-paths from
	$a \in (A \setminus A_\emptyset) \cap S'$ to $t$.
	For these, we maintain a BCP data structure with
	$P = (A \setminus A_\emptyset) \cap S'$,
	$Q = B_\emptyset \setminus S'$ with weights $\omega(p) = \pi(p)$ for
	all $p \in P$, and $\omega(q) = \pi(t)$ for all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 2-path $(a, b, t)$.
	This is only queried while $t \not\in S'$.
\item Shortcut arcs $(s, t)$ corresponding to empty 3-paths.
	For these, we maintain in a BCP data structure with
	$P = A_\emptyset \setminus S'$, $Q = B_\emptyset \setminus S'$ with
	weights $\omega(p) = \pi(s)$ for all
	$p \in P$, and $\omega(q) = \pi(t)$ for all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 3-path $(s, a, b, t)$.
	This is only queried while $s \in S'$ and $t \not\in S'$.
\end{enumerate}

By construction, the BCP distance of each datastructure in (4-6) is equal to
the reduced cost of the shortcut, which is equal to the reduced cost of the
corresponding empty path.
Each of the above data structures requires one query per relaxation, and an
insertion/deletion operation whenever a new vertex moves into $S$.
The data structures above can perform both in $O(\polylog n)$ time each, so the
running time of \textsc{Hungarian-Search2} outside of potential updates can be
bounded in the number of relaxation steps.

\begin{lemma}
\label{lemma:goldberg_hs_length1}
There are $O(k)$ non-shortcut relaxations in \textsc{Hungarian-Search2} before
a deficit vertex is reached.
\end{lemma}

\begin{proof}
Each edge relaxation adds a new vertex to $S$, and non-shortcut relaxations
only add non-empty vertices.
The vertices of $V \setminus S$ fall into several categories:
(i) $s$ or $t$, (ii) $A$ or $B$ vertex with 0 imbalance, and (iii) $A$ or $B$
vertex with deficit ($S$ contains all excess vertices).
The number of vertices in (i) and (iii) is $O(k)$, leaving us to bound the
number of (ii) vertices.

An $A$ or $B$ vertex with 0 imbalance must have an even number of $E_{>0}(f)$
edges.
There is either only one positive-capacity incoming edge (for $A$) or outgoing
edge (for $B$), so this quantity is either 0 or 2.
Since the vertex is non-empty, this must be 2.
We charge 0.5 to each of the two $E_{>0}(f)$ edges; the edges of $E_{>0}(f)$
have no more than 1 charge each.
Thus, the number of (ii) vertex relaxations is $O(|E_{>0}(f)|)$.
By Corollary~\ref{corollary:support_size_during}, $O(|E_{>0}(f)|) = O(k)$.
\end{proof}

\begin{lemma}
\label{lemma:goldberg_hs_length2}
There are $O(k)$ shortcut relaxations in \textsc{Hungarian-Search2} before a
deficit vertex is reached.
\end{lemma}

\begin{proof}
Recall the categories of shortcuts from the list of datastructures above.
We have shortcuts corresponding to (i) empty 2-paths surrounding
$a \in A_\emptyset$, (ii) empty 2-paths surrounding $b \in B_\emptyset$, and
(iii) empty 3-paths, which go from $s$ to $t$.

There is only one relaxation of type (iii), since $t$ can only be added to $S$
once.
The same argument holds for type (ii).

Each type (i) relaxation adds some non-empty $b \in B \setminus B_\emptyset$
into $S$.
Since $b$ is non-empty, it must either have deficit or an adjacent edge of
$E_{>0}(f)$.
We charge this relaxation to $b$ if it is deficit, or the adjacent $E_{>0}(f)$
edge otherwise.
No vertex is charged more than once, and no $E_{>0}(f)$ edge is charged more
than twice, therefore the total number of type (i) relaxations is
$O(|E_{>0}(f)|)$.
By Corollary~\ref{corollary:support_size_during}, $O(|E_{>0}(f)|) = O(k)$.
\end{proof}

\begin{corollary}
\label{corollary:goldberg_hs_length}
There are $O(k)$ relaxations in \textsc{Hungarian-Search2} before a deficit
vertex is reached.
\end{corollary}

In the following lemma, we complete the time analysis of
\textsc{Hungarian-Search2} by proving that potentials can be maintained in
$O(k)$ time over the course of the search.

\begin{lemma}
\label{lemma:goldberg_hs_time}
Using a dynamic BCP, we can implement \textsc{Hungarian-Search2} with
$T_1(n, k) = O(k\polylog n)$ and $P_1(n, k) = O(n\polylog n)$.
\end{lemma}

\begin{proof}
The initial sets for each data structure can be constructed in
$O(n\polylog n)$ time.
For each of the $O(1)$ data structures that are queried during a relaxation,
the new vertex moved into $S$ as a result of the relaxation causes $O(1)$
insertion/deletion operations.
For each of the data structures mentioned above, insertions and deletions
can be performed in $O(\polylog n)$ time.
Using Lemma~\ref{lemma:hs_time} as a basis, we first analyze the number of BCP
operations over the course of \textsc{Hungarian-Search2}.

\begin{enumerate}
\item Let $S^t_0$ denote the initial set $S$ at the beginning of the
	$t$-th Hungarian search, i.e. the set of $v \in V$ with
	$e_f(v) > 0$ after $t$ blocking flows.
	Assume for now that, at the beginning of the $(t+1)$-th
	Hungarian search, we have on hand the $S^t_0$ from the
	previous iteration.
	To construct $S^{t+1}_0$, we remove the vertices that had
	excess decreased to 0 by the $t$-th blocking flow.
	Thus, with that assumption, we are able to initialize $S$ at
	the cost of one BCP deletion per excess vertex, which sums to
	$O(k)$ over the entire course of \textsc{Refine}.
\item During each Hungarian search, a vertex entering $S$ may cause $P$
	or $Q$ to update and incur one BCP insertion/deletion.
	Like before, we can charge these to the number of edge
	relaxations over the course of \textsc{Hungarian-Search2}.
	The number of these is $O(k)$ by
	Corollary~\ref{corollary:goldberg_hs_length}.
\item Like before, we can meet the assumption in (1) by rewinding a log
	of point additions to $S$, and recover $S^t_0$.
\end{enumerate}

For potential updates, we use the same trick as in Lemma~\ref{lemma:hs_time} to
lazily update potentials after vertices leave $S$, but only for non-empty
vertices.
Non-empty vertices are stored in each data structure with weight
$\omega(v) = \pi(v) - \delta$, and $\delta$ is increased in lieu of increasing
the potential of all $S$ vertices.
When vertices leave $S$ (through the rewind mechansim above), we restore
their potentials as $\pi(v) \gets \omega(v) + \delta$.
With lazy updates, the number of potential updates on non-empty vertices is
bounded by the number of relaxations in the Hungarian search, which is $O(k)$
by Corollary~\ref{corollary:goldberg_hs_length}.
Note that empty vertex potentials are not handled in
\textsc{Hungarian-Search2}.
\end{proof}

\subsubsection{Depth-first search}

\begin{algorithm}
\caption{Depth-first search}
\label{algorithm:goldberg_dfs}
\begin{algorithmic}[1]
\Function{DFS}{$H = (V, E)$, $f$, $\pi$}
	\State $\tilde{H}_f \gets$ the shortcut graph of $H_f$
	\State $f' \gets 0$.
	\State $S \gets \{v \in V \mid e_f(v) > 0\}$
	\State $S_0 \gets \{v \in V \mid e_f(v) > 0\}$
		\Comment{stack of excess vertices}
	\State $P \gets$ \Call{Pop}{$S_0$}
		\Comment{current path; stack}
	\Repeat
		\State $v' \gets$ \Call{Peek}{$P$}
		\If{$e_f(v') < 0$}
			\Comment{if we reached a deficit, save the path to $f'$}
			\State add to $f'$ a unit flow on the path $P$
			\State $P \gets$ \Call{Pop}{$S_0$}
		\Else
			\State $w' \gets \argmin\{c_\pi(v', w') \mid w' \not\in S, (v', w') \in \tilde{H}_f\}$
			\State $\gamma \gets c_\pi(v', w')$

			\If{$\gamma \leq 0$}
				\Comment{if $(v', w')$ is admissible, extend the current path}
				\State $S \gets S \cup \{w'\}$
				\State $P \gets$ \Call{Push}{$P$, $w'$}
			\Else
				\Comment{No admissible arcs leaving $v'$, remove from $P$}
				\State \Call{Pop}{$P$}
			\EndIf
		\EndIf
	\Until{$S_0 = \emptyset$}
	\State\Return $f'$
\EndFunction
\end{algorithmic}
\end{algorithm}

The depth-first search is similar to \textsc{Hungarian-Search2} in that it
uses the relaxation of minimum-reduced cost arcs/empty paths, this time to
identify admissible arcs/empty paths in a depth-first manner.
This requires some adjustments to the data structures for finding the
minimum-reduced cost arc leaving $v' \in S$.
Given $v' \in S$, we would like to query:
\begin{enumerate}
\item Non-shortcut reverse arcs $(v', w')$ with $(w', v') \in E_{>0}(f)$.
	For these, we can maintain a min-heap on $(w', v') \in E_{>0}(f)$ arcs
	for each non-empty $v' \in V$.
\item Non-shortcut $A$-to-$B$ forward arcs.
	For these, we maintain a NN data structure over
	$P = (B \setminus B_\emptyset) \setminus S$, with weights
	$\omega(p) = \pi(p)$ for each $p \in P$.
	We subtract $\pi(v')$ from the NN distance to recover the reduced cost
	of the arc from $v'$.
\item Non-shortcut forward arcs from $s$-to-$A$ and from $B$-to-$t$.
	For $s$, we can maintain a min-heap on the potentials of
	$B \setminus S$, queried only if $v' = s$.
	For $B$-to-$t$ arcs, there is only one arc to check if $v' \in B$,
	which we can examine manually.

\item Shortcut arcs $(s, b)$ corresponding to empty 2-paths from $s$ to
	$b \in (B \setminus B_\emptyset) \setminus S'$.
	For these, we maintain a BCP data structure with $P = A_\emptyset$,
	$Q = (B \setminus B_\emptyset) \setminus S')$ with weights
	$\omega(p) = \pi(s)$ for all $p \in P$, and $\omega(q) = \pi(q)$ for
	all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 2-path $(s, a, b)$.
	This is only queried if $v' = s$.
\item Shortcut arcs $(a, t)$ corresponding to empty 2-paths from
	$a \in (A \setminus A_\emptyset) \cap S'$ to $t$.
	For these, we maintain a NN data structure over
	$P = B_\emptyset \setminus S'$ with weights $\omega(p) = \pi(t)$ for
	each $p \in P$.
	A response $(v', b)$ corresponds to th empty 2-path $(v', b, t)$.
	We subtract $\pi(v')$ from the NN distance to recover the reduced cost
	of the arc from $v'$.
	This is not queried if $t \in S$.
\item Shortcut arcs $(s, t)$ corresponding to empty 3-paths.
	For these, we maintain in a BCP data structure with
	$P = A_\emptyset \setminus S'$, $Q = B_\emptyset \setminus S'$ with
	weights $\omega(p) = \pi(s)$ for all
	$p \in P$, and $\omega(q) = \pi(t)$ for all $q \in Q$.
	A response $(a, b)$ corresponds to th empty 3-path $(s, a, b, t)$.
	This is only queried while $v' = s$ and $t \not\in S'$.
\end{enumerate}

Each data structure above performs $O(\polylog n)$ time worth of query and
insertion/deletion per relaxation, so the running time is again bounded by
$O(\polylog n)$ times the number of relaxations.
Since the pseudoflow is not changed within \textsc{DFS} (since the end of
\textsc{Hungarian-Search2}), the proofs from
Lemmas~\ref{lemma:goldberg_hs_length1} and \ref{lemma:goldberg_hs_length2} can
be recycled for \textsc{DFS}.

\begin{lemma}
\label{lemma:goldberg_dfs_length1}
There are $O(k)$ non-shortcut relaxations in \textsc{DFS} before a deficit
vertex is reached.
\end{lemma}

\begin{lemma}
\label{lemma:goldberg_dfs_length2}
There are $O(k)$ shortcut relaxations in \textsc{DFS} before a deficit vertex
is reached.
\end{lemma}

\begin{corollary}
\label{corollary:goldberg_dfs_length}
There are $O(k)$ relaxations in \textsc{DFS} before a deficit vertex is
reached.
\end{corollary}

There are no potentials to update within \textsc{DFS}, so the running time of
\textsc{DFS} boils down to the time spent to querying and updating the data
structures.

\begin{lemma}
\label{lemma:goldberg_dfs_time}
Using a dynamic NN, we can implement \textsc{DFS} with
$T_2(n, k) = O(k\polylog n)$ and $P_2(n, k) = O(n\polylog n)$.
\end{lemma}

\begin{proof}
At the beginning of \textsc{Refine}, we can initialize the $O(1)$ data
structures used in \textsc{DFS} in $P_2(n, k) = O(n\polylog n)$ time.
We use the same rewinding mechanism as \textsc{Hungarian-Search2}
(Lemma~\ref{lemma:goldberg_hs_time}) to avoid reconstructing the data
structures across iterations of \textsc{Refine}, so the total time spent
is bounded by the $O(\polylog n)$ times the number of relaxations.
By Corollary~\ref{corollary:goldberg_dfs_length}, we obtain
$T_2(n, k) = O(k\polylog n)$.
\end{proof}

\subsubsection{Size of the blocking flow and completing time analysis}

With Lemmas~\ref{lemma:goldberg_hs_time} and \ref{lemma:goldberg_dfs_time},
we can complete the proof of Lemma~\ref{lemma:goldberg_refine_time}
(time per \textsc{Refine}) by bounding the total number of arcs whose flow is
updated by a blocking flow during \textsc{Refine}.
This bounds both the time spent updating the flow value of these arcs, and
also the time spent on empty vertex potential updates
(Lemma~\ref{lemma:empty_updates}).

\begin{lemma}
\label{lemma:goldberg_bf_size}
Let $N_i$ be the number of positive flow arcs in the $i$-ith blocking flow
of \textsc{Refine}.
Then, $\sum_i N_i = O(k\sqrt{k})$.
\end{lemma}

\begin{proof}
Let $i$ be fixed and consider the invocation of \textsc{DFS} which produces the
$i$-th blocking flow $f_i$.
\textsc{DFS} constructs $f_i$ as a sequence of admissible excess-deficit paths,
which appear as path $P$ in Algorithm~\ref{algorithm:goldberg_dfs}.
Every arc in $P$ is an arc relaxed by \textsc{DFS}, so $N_i$ is bounded by the
number of relaxations performed in \textsc{DFS}.
Using Corollary~\ref{corollary:goldberg_dfs_length}, we have $N_i = O(k)$.

By Lemma~\ref{lemma:goldberg_refine_iterations}, there are $O(\sqrt{k})$
iterations of \textsc{Refine} before it terminates.
Summing, we see that $\sum_i N_i = O(k\sqrt{k})$.
\end{proof}

We now complete the proof of Lemma~\ref{lemma:goldberg_refine_time}.
There $O(\sqrt{k})$ iterations of \textsc{Refine}, each of which executes
\textsc{Hungarian-Search2} and \textsc{DFS}.
By Lemmas~\ref{lemma:goldberg_hs_time} and \ref{lemma:goldberg_dfs_time},
these calls take $O(T_1(n, k) + T_2(n, k)) = O(k\polylog n)$ time per
iteration.
\textsc{Hungarian-Search2} and \textsc{DFS} require some
once-per-\textsc{Refine} preprocessing to initialize data structures
in $P_1(n, k) + P_2(n, k) = O(n\polylog n)$ time.
Outside of these, we need to account for the time spent on flow value updates
and augmentation-induced empty vertex potential updates.
By Lemma~\ref{lemma:goldberg_bf_size}, the former is $O(k\sqrt{k})$ over the
course of \textsc{Refine}.
Combining Lemmas~\ref{lemma:goldberg_bf_size} and \ref{lemma:empty_updates},
the time for the latter is also $O(k\sqrt{k})$.

Filling in the values of $P_1(n, k)$, $P_2(n, k)$, $T_1(n, k)$, and
$T_2(n, k)$, the total time for \textsc{Refine} is
$O((n + k\sqrt{k})\polylog n)$.
Together with Lemmas~\ref{lemma:goldberg_scales} and \ref{lemma:scale_init},
this completes the proof of Theorem~\ref{theorem:gmcm}.


\section{Unbalanced transportation}

% definitions
% introduce the excess scaling algorithm/Orlin's
% time per hungarian search
% handling problem cases (stars, singletons)


{
\bibliographystyle{abbrv}
\bibliography{ref}
}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

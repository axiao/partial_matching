%!TEX encoding = UTF-8 Unicode
%!TeX spellcheck = en-US
\documentclass[a4paper,UKenglish]{socg-lipics-v2018}
\usepackage[utf8]{inputenc}

\usepackage{graphicx,wrapfig}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

%\usepackage[charter]{mathdesign}
%\usepackage{berasans,beramono}
\usepackage{microtype}

\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=Blue, citecolor=Green, linkcolor=BrickRed, breaklinks, unicode}

\usepackage[dvipsnames,usenames]{xcolor}
\usepackage[nocompress]{cite}
\usepackage{amsmath,mathtools,stmaryrd}
%\usepackage[shortlabels]{enumitem}

% \usepackage[title]{appendix}
\usepackage[bibliography=common]{apxproof}
%\usepackage[appendix=inline,bibliography=common]{apxproof}
\renewcommand{\appendixsectionformat}[2]{Missing Details and Proofs from Section~#1}

% \renewcommand\theenumi{\arabic{enumi}}
% \renewcommand\labelenumi{\theenumi.}
%
% \renewcommand\theenumii{\Alph{enumii}}
% \renewcommand\labelenumii{\theenumii}
%
% \renewcommand\theenumiii{\roman{enumiii}}
% \renewcommand\labelenumiii{\theenumiii.}
%
% \renewcommand\theenumiv{(\alph{enumiv})}
% \renewcommand\labelenumiv{\theenumiv}

\usepackage{arydshln}

\usepackage{todonotes}
\def\etal{\emph{et~al.}}

\dashlinedash 0.75pt
\dashlinegap 1.5pt

\usepackage{latexsym,amsmath}
\usepackage{amssymb,stmaryrd}
\usepackage{algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Requirement:}}

\usepackage{mathtools} % for \coloneqq

% \usepackage{etoolbox}
% \makeatletter
% \setbool{@fleqn}{false}
% \makeatother

\def\etal{\textit{et~al.}}
\def\poly{\mathop{\mathrm{poly}}}
\def\polylog{\mathop{\mathrm{polylog}}}
\def\eps{\varepsilon}
\def\softO{\widetilde{O}}
\def\bd{{\partial}}
\def\reals{\mathbb{R}}
\def\ints{\mathbb{Z}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% ---- DELIMITER PAIRS ----
\def\floor#1{\lfloor #1 \rfloor}
\def\ceil#1{\lceil #1 \rceil}
\def\seq#1{\langle #1 \rangle}
\def\set#1{\{ #1 \}}
\def\abs#1{\mathopen| #1 \mathclose|}		% use instead of $|x|$
\def\norm#1{\mathopen\| #1 \mathclose\|}	% use instead of $\|x\|$
\def\indic#1{\big[#1\big]}			% indicator variable; Iverson notation
							% e.g., Kronecker delta = [x=0]

% --- Self-scaling delmiter pairs ---
\def\Floor#1{\left\lfloor #1 \right\rfloor}
\def\Ceil#1{\left\lceil #1 \right\rceil}
\def\Seq#1{\left\langle #1 \right\rangle}
\def\Set#1{\left\{ #1 \right\}}
\def\Abs#1{\left| #1 \right|}
\def\Norm#1{\left\| #1 \right\|}
\def\Paren#1{\left( #1 \right)}		% need better macro name!
\def\Brack#1{\left[ #1 \right]}		% need better macro name!
\def\Indic#1{\left[ #1 \right]}		% indicator variable; Iverson notation

\def\tsupply{\lambda}
\def\fsupply{\phi}

\def\arcto{\mathord\shortrightarrow}
\def\arc#1#2{#1\arcto#2}

\def\Refine{\textsc{Refine}}
\def\Update{\textsc{Update}}

\def\cost{\operatorname{cost}}
\def\parent{\operatorname{par}}
\def\short{\operatorname{short}}
\def\supp{\operatorname{supp}}
\def\intracount{\operatorname{count}}


\theoremstyle{plain}
\newtheoremrep{lemma}{Lemma}[section]
\newtheoremrep{theorem}[lemma]{Theorem}
\newtheoremrep{corollary}[lemma]{Corollary}
% \newtheorem{observation}[lemma]{Observation}
% \newtheorem{claim}[lemma]{Claim}
% \newtheorem{definition}[lemma]{Definition}
\numberwithin{figure}{section}
\renewcommand{\paragraph}{\subparagraph}


% for definitions
%\definecolor{DarkRed}{rgb}{0.50,0.00,0.00}
\def\EMPH#1{\textcolor{BrickRed}{{\emph{#1}}}}
%\def\EMPH#1{\textbf{\boldmath #1}}
%\def\EMPH#1{\textbf{\emph{\boldmath #1}}}
\pdfstringdefDisableCommands{\let\boldmath\relax} % allow \boldmath in section titles

% ----------------------------------------------------------------------
%  Notes to myself.  The margin flags are broken, thanks to an
%  incompatibility with the geometry package.
% ----------------------------------------------------------------------
\def\n@te#1{\textsf{\boldmath \textbf{$\langle\!\langle$#1$\rangle\!\rangle$}}\leavevmode}
\def\note#1{\textcolor{red}{\n@te{#1}}}
%\renewcommand{\note}[1]{} % use to clear notes


%----------------------------------------------------------------------
% 'cramped' list style, stolen from Jeff Vitter.  Doesn't always work.
%----------------------------------------------------------------------
\def\cramped
  {\parskip\@outerparskip\@topsep\parskip\@topsepadd2pt\itemsep0pt
}


%% METAFILE
% \title{ Geometric Partial Matching and Unbalanced Transportation %
% \date{\today} % replace with date?
% \author{
% Pankaj K.\ Agarwal
% \and
% Hsien-Chih Chang
% \and
% Allen Xiao
% }
% }

\title{Efficient Algorithms for Geometric Partial Matching}
\titlerunning{Efficient Algorithms for Geometric Partial Matching}
\author{Pankaj K.\ Agarwal}{Duke University, USA}{pankaj@cs.duke.edu}{}{}
\author{Hsien-Chih Chang}{Duke University, USA}{hsienchih.chang@duke.edu}{}{}
\author{Allen Xiao}{Duke University, USA}{axiao@cs.duke.edu}{}{}
\date{\today}

\authorrunning{P.\ K.\ Agarwal, H.-C.\ Chang, A.\ Xiao}
\Copyright{Pankaj K.\ Agarwal, Hsien-Chih Chang, Allen Xiao}

\subjclass{Theory of computation $\rightarrow$ Design and analysis of algorithms}
\keywords{partial matching, transportation, optimal transport, minimum-cost flow, bichromatic closest pair}
\acknowledgements{
We thank Haim Kaplan for discussion and suggestion to use
Goldberg~\etal \cite{GHKT17} algorithm.
We thank Debmalya Panigrahi for helpful discussions.
}
\funding{
Work on this paper was supported by NSF under grants CCF-15-13816,
CCF-15-46392, and IIS-14-08846, by an ARO grant W911NF-15-1-0408, and by
BSF Grant 2012/229 from the U.S.-Israel Binational Science Foundation.
}

\EventEditors{}
\EventNoEds{2}
\EventLongTitle{The 35th International Symposium on Computational Geometry (SOCG 2019)}
\EventShortTitle{SOCG 2019}
\EventAcronym{SOCG}
\EventYear{2019}
\EventDate{June 18--21, 2019}
\EventLocation{Portland, USA}
\EventLogo{eatcs}
\SeriesVolume{}
\ArticleNo{1} %Set article-no=1


\begin{document}

\maketitle

\begin{abstract}
Let $A$ and $B$ be two point sets in the plane of sizes $r$ and $n$ respectively (assume $r \leq n$), and let $k$ be a parameter.
A matching between $A$ and $B$ is a family $M \subseteq A \times B$ of pairs so that any point of $A \cup B$ appears in at most one pair.
Given two integers $p, q \geq 1$, we define the cost of $M$ to be $\cost(M) = \sum_{(a, b) \in M}\norm{a-b}_p^q$ where $\norm{\cdot}_p$ is the $L_p$-norm.
The geometric partial matching problem asks to find the minimum-cost size-$k$ matching between $A$ and $B$.

We present efficient algorithms for geometric partial matching that work for any powers of $L_p$-norm matching objective:
An exact algorithm that runs in $O((n + k^2)\polylog n)$ time, and a $(1 + \eps)$-approximation algorithm that runs in $O((n + k\sqrt{k})\polylog n \cdot \log\eps^{-1})$ time.
Both algorithms are based on the primal-dual flow augmentation scheme; the main improvements are obtained by using dynamic data structures to achieve efficient flow augmentations.
Using similar techniques, we give an exact algorithm for the planar transportation problem that runs in $O((r^2\sqrt{n} + rn^{3/2})\polylog n)$ time.
For $r = o(\sqrt{n})$, this algorithm is faster than the state-of-art near-quadratic time algorithm by Agarwal~\etal\ [SOCG 2017].
\end{abstract}


% ----------------------------------------------------------------------------
\section{Introduction}

Given two point sets $A$ and $B$ in $\reals^2$, we consider the problem of finding
the minimum-cost partial matching between $A$ and $B$.
Formally, suppose $A$ has size $r$ and $B$ has size $n$ where $r \leq n$.
Let $G(A, B)$ be the undirected complete bipartite graph between
$A$ and $B$, and let the cost of edge $(a, b)$ be
$\EMPH{$c(a, b)$} = \norm{a-b}_p$, for some $1 \leq p < \infty$.
%Define $\EMPH{$C$} \coloneqq \max_{(a, b) \in A \times B} c(a, b)$.
A \EMPH{matching} $M$ in $G(A, B)$ is a set of edges sharing no endpoints.
The \EMPH{size} of $M$ is the number of edges in $M$.
Given $q \geq 1$, the cost of $M$ is defined to be
\begin{equation*}
	\EMPH{$\cost(M)$} \coloneqq \sum_{(a, b) \in M} \norm{a-b}_p^q.
\end{equation*}
For a parameter $k \leq r$, the problem of finding the minimum-cost
size-$k$ matching in $G(A, B)$ is called the \EMPH{geometric partial matching problem}.
We call the corresponding problem in general bipartite graphs (with arbitrary
edge costs) the \EMPH{partial matching} problem.%
\footnote{Partial matching is also called \EMPH{imperfect matching} or \EMPH{imperfect assignment} \cite{RT12,GHKT17}.}

We also consider the following generalization of bipartite matching.
Let $\tsupply:A \cup B \to \ints$ be a \EMPH{supply-demand function} with
positive value on points of $A$ and negative value on points of $B$, satisfying
$\sum_{a \in A} \tsupply(a) = - \sum_{b \in B} \tsupply(b)$.
%Define $\EMPH{$U$} \coloneqq \max_{p \in A \cup B} \abs{\tsupply(p)}$.
A \EMPH{transportation map} is a function $\tau: A \times B \to \reals_{\geq 0}$
such that $\sum_{b \in B} \tau(a, b) = \tsupply(a)$ for all $a \in A$ and
$\sum_{a \in A} \tau(a, b) = -\tsupply(b)$ for all $b \in B$.
We define the cost of $\tau$ to be
\begin{equation*}
	\EMPH{$\cost(\tau)$} \coloneqq \sum_{(a, b) \in A \times B} \norm{a-b}_p^q \cdot \tau(a, b).
\end{equation*}
The \EMPH{transportation problem} asks to compute a transportation map of minimum cost.

%Both matching and transportation problems arise in a broad range of applications,
%for example, in
%computer vision and graphics~\cite{RTG98,SGPCBNDG15,P15},
%machine learning~\cite{BBR06,ACB17},
%economics~\cite{G16},
%engineering~\cite{O87,STTP14},
%and medical imaging~\cite{GPC15}.
%More recently, computational/numerical interest has expanded greatly due to
%the development of fast approximate solvers \cite{C13,AWR17,DGK18,ABRW18}; \note{remove \cite{ABRW18}?}
%see the books by Villani~\cite{V03,V08}, by Cuturi and Peyr{\'e}~\cite{PC18},
%and the survey by Solomon~\cite{S18}.

\paragraph{Related work}
Maximum-size bipartite matching is a classical problem in graph algorithms.
Upper bounds include the $O(m\sqrt{n})$ time algorithm by
Hopcroft and Karp~\cite{HK73} and the $O(m \min\{\sqrt{m}, n^{2/3}\})$ time
algorithm by Even and Tarjan~\cite{ET75}, where $n$ is the
number of vertices and $m$ is the number of edges.
The first improvement in over thirty years was made by M{\k a}dry~\cite{M13},
which uses an interior-point algorithm and runs in $O(m^{10/7}\polylog n)$ time.

The Hungarian algorithm~\cite{Kuhn55} computes a minimum-cost maximum matching
in a bipartite graph in roughly $O(mn)$ time.
Faster algorithms have been developed,
%for minimum-weight bipartite matching,
such as the $O(m\sqrt{n}\log(nC))$ time algorithms by Gabow and
Tarjan~\cite{GT89} and the improved $O(m\sqrt{n}\log C)$ time algorithm by
Duan~\etal~\cite{DPS11} assuming that the edge costs are integral;
here $C$ is the maximum cost of an edge.
Ramshaw and Tarjan~\cite{RT12} showed that the Hungarian algorithm can be extended to compute a minimum-cost partial
matching of size $k$ in time $O(km + k^2\log r)$ time.
They also proposed a cost-scaling algorithm for partial
matching that runs in time $O(m\sqrt{n}\log(nC))$, again assuming that costs
are integral.
By reduction to unit-capacity min-cost flow, Goldberg~\etal~\cite{GHKT17}
developed a cost-scaling algorithm for partial matching with running time
$O(m\sqrt{k}\log(kC))$,
%$= O(nr\sqrt{k}\log(kC))$
again only for integral edge costs.

In geometric settings, the Hungarian algorithm can be implemented to compute
an optimal perfect matching between $A$ and $B$ (assuming equal size)
in time $O(n^2\polylog n)$~\cite{KMRSS17} (see also \cite{Vaidya89,AES99}).
This algorithm computes an optimal size-$k$ matching in time $O(kn\polylog n)$.
Faster approximation algorithms have been developed for computing a perfect
matching in geometric settings \cite{Vaidya89,V98,AV04,SA12}.
For $q=1$, the best algorithm to date by Sharathkumar and Agarwal~\cite{SA12m}
computes a $(1+\eps)$-approximation to the optimal perfect matching in
$O(n\polylog n \cdot \eps^{-O(1)})$ expected time with high probability.
Their algorithm can also compute a $(1+\eps)$-approximate partial
matching within the same time bound.
For $q > 1$, the best known approximation algorithm to compute a perfect
matching runs in $O(n^{3/2}\polylog n \log(n/\eps))$ time \cite{SA12};
it is not obvious how to extend this algorithm to the partial matching setting.

There is also some work on computing an optimal or near-optimal partial
matching when $B$ is fixed but $A$ is allowed to translate and/or rotate~\cite{CGKR08,R10,AHJKRST18,AKKMRSX18}.
Here, the goal is to (i) compute a (near-)optimal matching over all possible
transformations of $A$, or (ii) to compute a set $\mathcal{M}$ of matchings,
such that for any translation/rotation $\EuScript{T}$ of $A$, a (near-)optimal
matching of $\EuScript{T}(A)$ and $B$ in $\mathcal{M}$.
%Often in these case a partial matching algorithm on static points is used as a black box.
%so our results \note{what results? haven't presented yet} improve the final time bounds in~\cite{R10,AHJKRST18,AKKMRSX18}.

The transportation problem can also be formulated as a minimum-cost flow
problem in a graph.
The strongly polynomial uncapacitated min-cost flow algorithm by
Orlin~\cite{O93} solves the transportation problem in
$O((m + n\log n) n\log n)$ time.
Lee and Sidford~\cite{LS13b} give a weakly polynomial algorithm that runs in
$O(m\sqrt{n}\polylog(n, U))$ time, where $U$ is the maximum amount of vertex supply-demand.
Agarwal~\etal~\cite{AFPVX17} showed that Orlin's algorithm can be
implemented to solve 2D transportation in time $O(n^2\polylog n)$.
It is not known whether the 2D transportation problem can be solved in
$O(rn\polylog n)$ time.
By adapting the Lee-Sidford algorithm, they developed a
$(1+\eps)$-approximation algorithm that runs in $O(n^{3/2}\eps^{-2}\polylog(n, U))$ time.
They also gave a Monte-Carlo algorithm that computes an
$O(\log^2(1/\eps))$-approximate solution in $O(n^{1+\eps})$ time with
high probability.

\paragraph{Our results}
There are three main results in this paper.
First in Section~\ref{section:hung} we present an efficient algorithm for
computing an optimal partial matching in $\reals^2$.

\begin{theorem}
\label{theorem:hung}
Given two point sets $A$ and $B$ in $\reals^2$ each of size at most $n$,
a minimum-cost matching of size $k$ between $A$ and $B$ can be computed in
$O((n + k^2)\polylog n)$ time.
\end{theorem}

We use \emph{bichromatic closest pair (BCP)} data structures to implement the Hungarian algorithm efficiently, similar to Agarwal~\etal\ and Kaplan~\etal~\cite{KMRSS17,AES99}.
But unlike their algorithms which take $\Omega(n)$ time to find an
augmenting path, we show that after $O(n\polylog n)$ preprocessing,
an augmenting path can be found in $O(k\polylog n)$ time.
The key is to recycle (rather than rebuild) our data structures from one
augmentation to the next.
We refer to this idea as the \emph{rewinding mechanism}.
%and it will be used in our other algorithms as well.

\medskip

Next in Sections~\ref{section:goldberg} and \ref{S:implementation},
we obtain a $(1+\eps)$-approximation algorithm for the geometric partial
matching problem in $\reals^2$ by providing an efficient implementation of the
unit-capacity min-cost flow algorithm by Goldberg~\etal~\cite{GHKT17}.

\begin{theorem}
\label{theorem:gmcm}
Given two point sets $A$ and $B$ in $\reals^2$ each of size at most $n$,
a $(1+\eps)$-approximate min-cost matching of size $k$ between $A$
and $B$ can be computed in $O((n + k\sqrt{k})\polylog n \cdot \log\eps^{-1})$ time.
\end{theorem}

The main challenge here is the set of \emph{null vertices}
which do not play any role in the augmentations, but still contribute to the size of the graph.
Instead, we run the unit-capacity min-cost flow algorithm on a
\emph{shortcut graph}, circumventing all null vertices.
The shortcut graph itself may have $\Omega(n^2)$ edges,
but we can represent it implicitly and use a data structure to explore this graph.
As such, we can implement each augmentation in the Goldberg~\etal algorithm in
time proportional to the size of the \emph{flow support}, which turns out to be of size $O(k)$.

\medskip

Finally in Section~\ref{section:orlin} we present a faster algorithm for the
transportation problem in $\reals^2$ when the two point sets are unbalanced.

\begin{theorem}
\label{theorem:orlin}
Given two point sets $A$ and $B$ in $\reals^2$ of sizes $r$ and $n$ respectively
along with supply-demand function $\tsupply:A \cup B \to \ints$,
an optimal transportation map between $A$ and $B$ can be computed in
$O((r^2\sqrt{n} + rn^{3/2})\polylog n)$ time.
\end{theorem}

Our result improves over the $O(n^2\polylog n)$ time algorithm in
\cite{AFPVX17} when $r = o(\sqrt{n})$.
The algorithm uses the strongly polynomial uncapacitated minimum-cost
flow algorithm by Orlin~\cite{O93}, adapted for geometric costs as in
Agarwal~\etal~\cite{AFPVX17}.
%\note{``If needed, we can remove this part.''}
Unlike in the case of matchings, the flow support for the transportation problem may have size $\Omega(n)$
even when $r$ is a constant; so na\"ively we can no longer charge the execution time to flow support size.
However, we show that most of the support arcs are of degree one and thus can be partitioned
into \emph{stars} centered at vertices of $A$.
We describe a data structure that processes these stars in amortized
$O((r^2/\sqrt{n} + r\sqrt{n})\polylog n)$ time per augmentation.

% ----------------------------------------------------------------------------
\section{Minimum-Cost Partial Matchings using Hungarian Algorithm}
\label{section:hung}

% The Hungarian algorithm~\cite{Kuhn55} is a primal-dual algorithm for min-cost
% bipartite matching in general graphs that can be adapted to solve the partial matching problem exactly if
% one terminates the algorithm after $k$ iterations (see e.g.~\cite{RT12}).
In this section, we solve the geometric partial matching problem and prove Theorem~\ref{theorem:hung} by implementing the Hungarian algorithm for partial matching in $O((n + k^2)\polylog n)$ time.

%\subsection{Matching Terminologies}

% \note{Move some to intro}
% Let $G$ be a bipartite graph between vertex sets $A$ and $B$ and edge set $E$,
% with costs $c(v, w)$ for each edge $(v, w)$ in $G$.
% A \EMPH{matching} $M \subseteq E$ is a set of edges where no two edges share an
% endpoint.
% A vertex $v$ is \EMPH{matched} by $M$ if $v$ is the endpoint of some matching edge in $M$;
% otherwise $v$ is \EMPH{unmatched}.
% %We use $V(M)$ to denote the vertices matched by $M$.
% The \EMPH{size} of a matching is the number of edges in the set, and the
% \EMPH{cost} of a matching is the sum of costs of its edges.
% For a parameter $k$, the \EMPH{minimum-cost partial matching problem (MPM)}
% asks to find a size-$k$ matching of minimum cost.
% In the geometric partial matching setting, we have $E = A \times B$
% and $c(a, b) = \norm{a-b}_p^q$ for every edge $(a, b)$ in $G$.

A vertex $v$ is \EMPH{matched} by matching $M \subseteq E$ if $v$ is the endpoint of some edge in $M$;
otherwise $v$ is \EMPH{unmatched}.
Given a matching $M$, an \EMPH{augmenting path}
$\Pi = (a_1, b_1, \ldots, a_\ell, b_\ell)$ is an odd-length path with unmatched
endpoints ($a_1$ and $b_\ell$) that alternates between edges outside and inside of $M$.
The symmetric difference $M \oplus \Pi$ creates a new matching of size $\abs{M}+1$, called the \EMPH{augmentation} of $M$ by $\Pi$.
%
The dual to the standard linear program for partial matching has dual variables
for each vertex, called \EMPH{potentials $\pi$}.
Given potentials $\pi$, we can define the \EMPH{reduced cost} on the edges to be
$\EMPH{$c_\pi(v, w)$} \coloneqq c(v, w) - \pi(v) + \pi(w)$.
Potentials $\pi$ are \EMPH{feasible} if the reduced costs are nonnegative for all edges in $G$.
We say that an edge $(v, w)$ is \EMPH{admissible} under potentials $\pi$ if $c_\pi(v, w) = 0$.

\paragraph{Fast implementation of Hungarian search.}

The Hungarian algorithm is initialized with $M = \emptyset$ and $\pi = 0$.
% It maintains the following invariants: (see, for example, \cite{})
% %\begin{enumerate}[(i)]\itemsep=0pt
% (i) $\pi$ is feasible,
% (ii) all edges in $M$ are admissible,
% (iii) unmatched vertices of $A$ all have the same potential $\alpha$ satisfying $\alpha \geq \pi(a)$ for any matched vertex $a \in A$, and
% (iv) unmatched vertices of $B$ all have the same potential $\beta$ satisfying $\beta \leq \pi(b)$ for any matched vertex $b \in B$.
% %\end{enumerate}
% Ramshaw and Tarjan~\cite{RT12} show that these conditions are sufficient to
% guarantee that $M$ is a minimum-cost matching.
Each iteration of the Hungarian algorithm augments $M$ with an admissible
augmenting path $\Pi$, discovered using a procedure called the
\EMPH{Hungarian search}.
The algorithm terminates after $k$ augmentations, when $\abs{M} = k$;
Ramshaw and Tarjan~\cite{RT12} showed that $M$ is guaranteed to be an optimal partial matching.
%\note{For the specific implementation of Hungarian search where in each iteration the roots are all unmatched vertices in $A$.  Picking one as source does not guarantee min-cost partial matching.}

The Hungarian search grows a set of \EMPH{reachable vertices $S$} \emph{from}
all unmatched $v \in A$ \emph{using} augmenting paths of admissible edges.
Initially, $S$ is the set of unmatched vertices in $A$.
Let the \EMPH{frontier} of $S$ be the edges in $(A \cap S) \times (B \setminus S)$.
$S$ is grown by \EMPH{relaxing} an edge $(a, b)$ in the frontier:
adding $b$ into $S$, and also modifying potentials to make $(a, b)$ admissible,
preserve $c_\pi$ on other edges within $S$, and keep $\pi$ feasible on edges outside of $S$.
Specifically, the algorithm relaxes the minimum-reduced-cost frontier edge $(a, b)$,
and then raises $\pi(v)$ by $c_\pi(a, b)$ for all $v \in S$.
%It is easy to verify that this potential change preserves feasibility.
%
% \note{Compress details...}
% As $b \in B$ is added into $S$, we can store a backpointer to $a$, which can be
% used later to recover the admissible augmenting path through $b$.
% If $b$ is matched, say to vertex $a'$, then we also relax $(a', b)$ by adding $a'$
% into $S$ (no potential change needed, by invariant) with backpointer to $b$.
% If $b$ is unmatched, the search finishes and we can recover an admissible
% augmenting path to $b$ by following backpointers to an unmatched vertex $a \in A$.
% %Once $S$ contains an unmatched $b \in B$, an admissible augmenting path exists.
% \note{... to here.}
If $b$ is already matched, then we also relax the matching edge $(a',b)$ and add $a'$ into $S$.
The search finishes when $b$ is unmatched, and an admissible augmenting path now can be recovered.

%Each augmenting path has length $O(k)$, as every other edge is a matching edge and $|M| \leq k$. \note{not needed here?}
%Additionally, there are $k$ augmentations throughout the Hungarian algorithm.
%so the total time spent on updating the matching (during augmentations) is $O(k^2)$.

%\subsection{Fast implementation of Hungarian search}
%\label{SS:fast-hungarian-matching}

% Observe that the Hungarian search makes $O(k)$ relaxations, as each
% relaxation either leads to an unmatched vertex (ending the search) or
% adds both vertices of a matching edge.
% We will implement each relaxation step in $O(\polylog n)$ time, after
% preprocessing.

%In general graphs,
%The most expensive step in augmentation is to find the minimum-reduced-cost frontier edge that needs to be relaxed --- the search must ``look at every edge''.
%e.g.\ by pushing them into a priority queue even if they are not relaxed.
In the geometric setting, we find the min-reduced-cost frontier edge using a dynamic
\EMPH{bichromatic closest pair} (BCP) data structure, as observed
in~\cite{AFPVX17,Vaidya89}.
Given two point sets $P$ and $Q$ in the plane and a weight function
$\omega: P\cup Q \to \reals$, the BCP is two points $a \in P$ and $b \in Q$
minimizing the additively weighted distance $c(a, b) - \omega(a) + \omega(b)$.
Thus, a minimum reduced-cost frontier edge is precisely the BCP of point sets
$P = A \cap S$ and $Q = B \setminus S$, with $\omega = \pi$.
Note that the ``state'' of this BCP is parameterized by $S$ and $\pi$.

%\note{Short history on BCP?}
%\note{Under the assumption ... on the metric,}
The dynamic BCP data structure by Kaplan \etal~\cite{KMRSS17} supports point insertions and deletions in
$O(\polylog n)$ time and answers queries in $O(\log^2 n)$ time for our setting.
Outside of potential updates, each relaxation in the Hungarian search requires
one query, one deletion, and at most one insertion.
As $\abs{M} \leq k$ throughout, there are at most $2k$ relaxations in each
Hungarian search, and the BCP can be used to implement each Hungarian search
in $O(k\polylog n)$ time.
Explained shortly, there is an existing technique to handle potential updates
without performing BCP updates for each one.

\subsection{Rewinding mechanism}
\label{SS:rewinding}

% In the beginning of each iteration,
% $S$ is initialized to the set of unmatched vertices in $A$,
% and therefore $Q = B \setminus S$ has size $n$.
We cannot afford to take $O(n\polylog n)$ time to initialize the BCP data structure at the
beginning of every Hungarian search beyond the first one.
To resolve the issue, observe that exactly one vertex of $A$ is newly matched after an augmentation.
Thus (modulo potential changes), given the initial state of the BCP at the $i$-th Hungarian search,
we can obtain the initial state for the $(i+1)$-th with a single BCP deletion operation.
Suppose we log the sequence of points added to $S$ in the $i$-th Hungarian search.
Then, at the start of the $(i+1)$-th Hungarian search, we can \emph{rewind} this
log by applying the opposite insert/delete operation for each BCP update in
reverse order of the log to obtain the initial state of the $i$-th BCP.
With one additional BCP delete, we have the initial state for the $(i+1)$-th BCP.
The number of points in the log is $O(k)$, bounded by the number of relaxations
per Hungarian search.
Thus, in $O(k\polylog n)$ time we can recover the initial BCP data structure
for each Hungarian search beyond the first.
We refer to this procedure as the \EMPH{rewinding mechanism}.

As for potential updates, we adapt an update batching trick from Vaidya~\cite{Vaidya89}
to work under the rewinding mechanism, so that the time spent on potential
updates per Hungarian search is $O(k)$.
See the full version for details.
%
Putting everything together, our implementation of the Hungarian algorithm runs
in $O((n + k^2)\polylog n)$ time.
This proves Theorem~\ref{theorem:hung}.
%\begin{lemma}
%\label{L:fast-hungarian}
%Each Hungarian search can be implemented
%in $O(k\polylog n)$ time after a one-time $O(n\polylog n)$ preprocessing.
%\end{lemma}

\subsection{Potential updates}
\label{SS:potential-update}

We modify a trick from Vaidya~\cite{Vaidya89} to batch potential updates.
Potentials have a \EMPH{stored value}, i.e.\ the currently recorded value of
$\pi(v)$, and a \EMPH{true value}, which may have changed from $\pi(v)$.
The resulting algorithm queries the minimum-reduced-cost under the true values
of $\pi$ and updates the stored value occasionally.

Throughout the entire Hungarian algorithm, we maintain a nonnegative scalar
$\EMPH{$\delta$}$ (initially set to $0$) which aggregates potential changes.
Vertices $a \in A$ that are added to $S$ are inserted into BCP with weight
$\omega(a) \gets \pi(a) - \delta$, for whatever value $\delta$ is at the time
of insertion.
Similarly, vertices $b \in B$ that are added to $S$ have $\omega(b) \gets \pi(b) - \delta$
recorded ($B \cap S$ points aren't added into a BCP set).
When the Hungarian search wants to raise the potentials of points in $S$,
$\delta$ is increased by that amount instead.
Thus, true value for any potential of a point in $S$ is always $\omega(p) + \delta$.
For points of $(A \cup B) \setminus S$, the true potential is equal to the
stored potential.
Since all the points of $A \cap S$ have weights uniformly offset from their
true potentials, the minimum edge returned by the BCP does not change. \note{why?}

Once a point is removed from $S$ (i.e.\ by an augmentation or the rewinding
mechanism), we update its stored potential $\pi(p) \gets \omega(p) + \delta$,
again for the current value of $\delta$.
Most importantly, $\delta$ is not reset at the end of a Hungarian search and
persists through the entire algorithm.
Thus, the initial BCP sets constructed by the rewinding mechanism have true
potentials accurately represented by $\delta$ and $\omega(p)$.

We update $\delta$ once per edge relaxation; thus $O(k)$ times in total per Hungarian search.
There are $O(k)$ stored values updated per Hungarian search during the rewinding process.
The time spent on potential updates per Hungarian search is therefore $O(k)$.


% ----------------------------------------------------------------------------
\section{Approximating Min-Cost Partial Matching through Cost-Scaling}
\label{section:goldberg}

In this section we describe an approximation algorithm for computing a min-cost
partial matching.
%in the graph $G(A, B)$ defined earlier.
We reduce the problem to computing a min-cost circulation in a flow network
(Section~\ref{SS:match-flow-red}).
We adapt the cost-scaling algorithm by Goldberg~\etal~\cite{GHKT17} for
computing min-cost flow of a unit-capacity network (Section~\ref{SS:cost-scale}).
Finally, we show how their algorithm can be implemented in
$O\Paren{(n + k^{3/2})\polylog(n)\log(1/\eps)}$ time in our setting (Section~\ref{SS:fast_refine}).

\subsection{From matching to circulation}
\label{SS:match-flow-red}

Given a bipartite graph $G$ with vertex sets $A$ and $B$, we construct a flow network
$N = (V, \vec{E})$ in a standard way \cite{RT12}
so that a min-cost matching in $G$ corresponds to a min-cost integral
circulation in $N$.

\paragraph{Flow network.}
Each vertex in $G$ becomes a node in $N$ and each edge
$(a, b)$ in $G$ becomes an arc $\arc{a}{b}$ in $N$;
we refer to these nodes (resp.\ arcs) as \EMPH{bipartite nodes} (resp.\ \EMPH{bipartite arcs}).
We also include a \EMPH{source} node $s$ and \EMPH{sink} node $t$ in $N$.
For each $a \in A$, we add a \EMPH{left dummy arc} $\arc{s}{a}$ and for each
$b \in B$ we add a \EMPH{right dummy arc} $\arc{b}{t}$.
The cost $c(\arc{v}{w})$ of each arc $\arc{v}{w}$ in $N$ is equal to $c(v, w)$ if
$\arc{v}{w}$ is a bipartite arc and $0$ if $\arc{v}{w}$ is a dummy arc.
All arcs in $N$ have unit capacity.

Let $\fsupply: V \to \ints$ be an integral supply/demand function on nodes of $N$.
The positive values of $\fsupply(v)$ are referred to as \EMPH{supply}, and the
negative values of $\fsupply(v)$ as \EMPH{demand}.
A \EMPH{pseudoflow} $f: \vec{E} \to [0, 1]$ is a function on arcs of $N$.
The \EMPH{support} of $f$ in $N$, denoted as \EMPH{$\supp(f)$}, is the set of arcs with positive flows:
$\supp(f) \coloneqq \Set{\arc vw \in \vec{E} \mid f(\arc vw) > 0}$.
Given a pseudoflow $f$, the \EMPH{imbalance} of a vertex (with respect to $f$) is
\[
\EMPH{$\fsupply_f (v)$} \coloneqq \fsupply(v) + \sum_{\arc wv \in \vec{E}}{f(\arc wv)} - \sum_{\arc vw \in \vec{E}}{f(\arc vw)}.
\]
We call positive imbalance \EMPH{excess} and negative imbalance \EMPH{deficit};
and vertices with positive (resp.\ negative) imbalance excess (resp.\ deficit) vertices.
A vertex is \EMPH{balanced} if it has zero imbalance.
If all vertices are balanced, the pseudoflow is a \EMPH{circulation}.
The \EMPH{cost} of a pseudoflow is defined to be
\[
 \EMPH{$\cost(f)$} \coloneqq \sum_{\arc vw \in \supp(f)} c(\arc vw) \cdot f(\arc vw).
\]
The \EMPH{minimum-cost flow problem} (MCF) asks to find a circulation of minimum cost inside a given network.

If we set $\fsupply(s) = k$, $\fsupply(t) = k$, and $\fsupply(v) = 0$ for all
$v \in A \cup B$, then an integral circulation $f$ corresponds to a partial
matching $M$ of size $k$ and vice versa.
Moreover, $\cost(M) = \cost(f)$.
%
Hence, the problem of computing a min-cost matching of size $k$ in $G(A, B)$
transforms to computing an integral circulation in $N$.
The following lemma will be useful for our algorithm.

\begin{lemma}
\label{lemma:supp_size}
Let $N$ be the network constructed from $G(A, B)$ above.
\begin{enumerate}[(i)]
\item For any integral circulation $g$ in $N$, the size of $\supp(g)$ is at most $3k$.
\item For any integral pseudoflow $f$ in $N$ with $O(k)$ excess, the size of $\supp(f)$ is $O(k)$.
\end{enumerate}
\end{lemma}

\subsection{A cost-scaling algorithm}
\label{SS:cost-scale}

Before describing the algorithm, we need to introduce a few more concepts.

\paragraph{Residual network and admissibility.}
If $f$ is an integral pseudoflow
(that is, $f(\arc{v}{w}) \in \{0, 1\}$ for every arc in $\vec{E}$), then each arc
$\arc{v}{w}$ in $N$ is either \EMPH{idle} with $f(\arc{v}{w}) = 0$ or
\EMPH{saturated} with $f(\arc{u}{v}) = 1$. \note{define earlier?}

Given a pseudoflow $f$, the \EMPH{residual network} $N_f = (V, \vec{E}_f)$ is
defined as follows.
For each idle arc $\arc{v}{w}$ in $\vec{E}$, we add a \EMPH{forward} residual
arc $\arc{v}{w}$ in $N_f$.
For each saturated arc $\arc{v}{w}$ in $\vec{E}$, we add a \EMPH{backward}
residual arc $\arc{w}{v}$ in $N_f$.
The set of residual arcs in $N_f$ is therefore
\[
\vec{E}_f \coloneqq \{\arc{v}{w} \mid f(\arc{v}{w}) = 0\} \cup \{\arc{w}{v} \mid f(\arc{v}{w}) = 1\}.
\]
The cost of a forward residual arc $\arc{v}{w}$ is $c(\arc{v}{w})$,
while the cost of a backward residual arc is $\arc{w}{v}$ is $-c(\arc{v}{w})$.
Each arc in $N_f$ also has unit capacity.
By Lemma~\ref{lemma:supp_size}, $N_f$ has $O(k)$ backward arcs if $f$ has $O(k)$ excess.

A \EMPH{residual pseudoflow} $g$ in $N_f$ can be used to change $f$ into a
different pseudoflow on $N$ by a process called \EMPH{augmentation}.
For simplicity, we only describe augmentation for the case where $f$, $g$ are integer.
Specifically, augmenting $f$ by $g$ produces a pseudoflow $f'$ in $N$ where
\[
f'(\arc vw) = \begin{cases}
	0 & {\arc wv} \in \vec{E}_f \text{ and } g(\arc wv) = 1 \\
	1 & {\arc vw} \in \vec{E}_f \text{ and } g(\arc vw) = 1 \\
	f(\arc vw) & \text{otherwise.}
\end{cases}
\]

Using LP duality for min-cost flow, we assign a \EMPH{potential $\pi(v)$} to each node $v$ in $N$.
The \EMPH{reduced cost} of an arc $\arc{v}{w}$ in $N$ with respect to $\pi$ is
defined as
\[
c_\pi(\arc vw) \coloneqq c(\arc vw) - \pi(v) + \pi(w).
\]
Similarly we define the reduced cost of arcs in $N_f$: the reduced cost of a
forward residual arc $\arc vw$ in $N_f$ is $c_\pi(\arc vw)$, and the reduced cost of a
backward residual arc $\arc wv$ in $N_f$ is $-c_\pi(\arc vw)$.
Abusing the notation, we also use $c_\pi$ to denote the reduced cost of arcs in
$N_f$. \note{Unclear.  Does $c_\pi(\arc wv)$ have positive or negative value for a backward arc $\arc wv$?}

The \EMPH{dual feasibility constraint} asks that $c_\pi(\arc vw) \geq 0$ holds
for every arc $\arc vw$ in $\vec{E}$;
potentials $\pi$ that satisfy this constraint are said to be \EMPH{feasible}.
Suppose we relax the dual feasibility constraint to allow some small violation
in the value of $c_\pi(\arc vw)$.
We say that a pair of pseudoflow $f$ and potential $\pi$ is
\EMPH{$\theta$-optimal}~\cite{T85,BE87}
%\footnote{This is commonly written as $\eps$-optimality in the min-cost flow literature.}
if $c_\pi(\arc vw) \geq -\theta$ for every residual arc $\arc vw$ in $\vec{E}_f$.
Pseudoflow $f$ is \emph{$\theta$-optimal} if it is $\theta$-optimal with
respect to some potentials $\pi$;
potential $\pi$ is \emph{$\theta$-optimal} if it is $\theta$-optimal with
respect to some pseudoflow $f$.
Given a pseudoflow $f$ and potentials $\pi$, a residual arc $\arc vw$ in
$\vec{E}_f$ is \EMPH{admissible} if $c_\pi(\arc vw) \leq 0$.
We say that a pseudoflow $g$ in $G_f$ is \EMPH{admissible} if all support arcs
of $g$ on $G_f$ are admissible; in other words, $g(\arc vw) > 0$ holds only on
admissible arcs $\arc vw$.
We will widely use the following well-known property of $\theta$-optimality.

\begin{lemma}
\label{lemma:eps_opt_preserve}
Let $f$ be an $\theta$-optimal pseudoflow in $N$ and let $g$ be an admissible
pseudoflow in $N_f$.
Then $f$ augmented by $g$ is also $\theta$-optimal in $N$.
\end{lemma}

%\begin{proof}
%Augmentation by $f'$ will not change the potentials, so any previously
%$\theta$-optimal arcs remain $\theta$-optimal.
%However, it may introduce new arcs $\arc vw$ with $u_{f+f'}(\arc vw) > 0$, that previously had
%$u_f(\arc vw) = 0$.
%We will verify that these arcs satisfy the $\theta$-optimality condition.
%
%If an arc $\arc vw$ is newly introduced this way, then by definition of residual
%capacities $f(\arc vw) = u(\arc vw)$.
%At the same time, $u_{f+f'}(\arc vw) > 0$ implies that $(f+f')(\arc vw) < u(\arc vw)$.
%This means that $f'$ augmented flow in the reverse direction of $\arc vw$,
%that is, $f'(\arc wv) > 0$.
%By assumption, the arcs of $\supp(f')$ are admissible, so $\arc wv$ was an
%admissible arc ($c_\pi(\arc wv) \leq 0$).
%By antisymmetry of reduced costs, this implies $c_\pi(\arc vw) \geq 0 \geq -\theta$.
%Therefore, all arcs with $u_{f+f'}(v, w) > 0$ respect the $\theta$-optimality condition,
%and thus $f+f'$ is $\theta$-optimal.
%\end{proof}
%\end{toappendix}

Using Lemma~\ref{lemma:supp_size}, the following lemma can be proved about
$\theta$-optimality:

\begin{lemma}
\label{lemma:goldberg_cost_add}
Let $f$ be a $\theta$-optimal integer circulation in $N$,
and $f^*$ be an optimal integer circulation for $N$.
Then, $\cost(f) \leq \cost(f^*) + 6k\theta$.
\end{lemma}

%\begin{proof}
%By Lemma~\ref{lemma:support_size}, the total number of backward arcs in the residual network $N_f$ is at most $3k$.
%
%Consider the residual flow in $N_f$ defined by the difference between $f^*$ and $f$.
%Since both $f$ and $f^*$ are both circulations and $N_H$ has unit-capacity,
%the flow $f - f^*$ is comprised of unit flows on a collection of edge-disjoint residual cycles $\Gamma_1, \ldots, \Gamma_\ell$.
%Observe that each residual cycle $\Gamma_i$ must have exactly half of its arcs being backward arcs, and thus we have $\sum_i |\Gamma_i| \leq 6k$.
%
%Let $\pi$ be some potential certifying that $f$ is $\theta$-optimal.
%Because $\Gamma_i$ is a residual cycle, we have $c_\pi(\Gamma_i) = c(\Gamma_i)$ since the potential terms telescope.
%We then see that
%\[
%	\cost(f) - \cost(f^*)
%	= \sum_i c(\Gamma_i)
%	= \sum_i c_\pi(\Gamma_i)
%	\geq \sum_i (-\theta) \cdot |\Gamma_i|
%	\geq -6k\theta,
%\]
%where the second-to-last inequality follows from the $\theta$-optimality of $f$
%with respect to $\pi$.
%Rearranging the terms we have that $\cost(f) \leq \cost(f^*) + 6k\theta$.
%\end{proof}

\paragraph{Estimating the value of \textbf{$\boldmath \cost(f^*)$}.}
We now describe a procedure for estimating $\cost(f^*)$ within a polynomial factor,
which will be useful in setting the scaling parameters of the cost-scaling algorithm.

Let \EMPH{$T$} be a minimum spanning tree of $A \cup B$ under the $L_p^q$-metric.
Let $e_1, e_2, \ldots, e_{n-1}$ be the edges of $T$ sorted in nondecreasing order
of length; in other words, $c(e_i) \leq c(e_{i+1})$ where $c(e) = \norm{e}_p^q$.
Let \EMPH{$T_i$} be the forest consisting of the vertices of $A \cup B$ and
$e_1, \ldots, e_i$.
We call a matching $M$ of $G(A, B)$ \EMPH{intra-cluster} if both endpoints of
every edge in $M$ lie in the same connected component of $T_i$.
We define \EMPH{$i^*$} to be the smallest index $i$ such that there exists an
intra-cluster matching of size $k$ in $T_{i^*}$.
Set \EMPH{$\overline{\theta}$} $\coloneqq n^q \cdot c(e_{i^*})$.
The following lemma will be used by our cost-scaling algorithm:

\begin{lemma}
\label{lemma:starting_scale}
\begin{enumerate}[(i)]
\item \label{item:starting_scale1}
	The value of $i^*$ can be computed in $O(n\log n)$ time.
\item \label{item:starting_scale2}
	$c(e_{i^*}) \leq \cost(f^*) \leq \overline{\theta}$.
\item \label{item:starting_scale3}
	There is a $\overline{\theta}$-optimal circulation in the network $N$ with
	respect to the 0 potential $\pi = 0$, assuming $\fsupply(s) = k$,
	$\fsupply(t) = -k$, and $\fsupply(v) = 0$ for all $v \in A \cup B$.
\end{enumerate}
\end{lemma}

Set $\underline{\theta} \coloneqq \frac{\eps}{6k} \cdot c(e_{i^*})$.
As a consequence of Lemmas~\ref{lemma:starting_scale}\ref{item:starting_scale2}
and \ref{lemma:goldberg_cost_add}, we have:

\begin{corollary}
\label{corollary:goldberg_approx}
The cost of a $\underline{\theta}$-optimal integral circulation in $N$ is at
most $(1+\eps) \cost(f^*)$.
\end{corollary}

We are now ready to describe our algorithm.

\paragraph{Overview of the algorithm.}
We closely follow the algorithm of Goldberg \etal~\cite{GHKT17}.
The algorithm works in rounds.
In the beginning of each round, we fix a \EMPH{cost scaling parameter}
\EMPH{$\theta$} and maintain potentials $\pi$ with the following property:

\note{would like to use a better/neater/cleaner environment to represent invariant}%TODO
\begin{enumerate}[(P1)]
\item \label{item:scale_inv}
	There exists a $2\theta$-optimal integral circulation in $N$ with respect to $\pi$.
\end{enumerate}

For the initial round, we set $\theta \gets \overline{\theta}$ and $\pi \gets 0$.
By Lemma~\ref{lemma:starting_scale}\ref{item:starting_scale3},
(P\ref{item:scale_inv}) is satisfied initially.
Each round of the algorithm consists of two stages.
In the first stage, called \EMPH{scale initialization} (\textsc{Scale-Init})
computes a $\theta$-optimal pseudoflow $f$.
The second stage, called \EMPH{refinement} (\textsc{Refine}) converts $f$ into
a $\theta$-optimal (integral) circulation $g$.
In both stages, $\pi$ is updated as necessary.
If $\theta \leq \underline{\theta}$, we return $g$.
Otherwise, we set $\theta \gets \theta/2$ and start the next round.
Note that (P\ref{item:scale_inv}) is satisfied in the beginning of each round.

By Corollary~\ref{corollary:goldberg_approx}, when the algorithm terminates,
it returns an integral circulation $\tilde{f}$ in $N$ of cost at most
$(1+\eps) \cost(f^*)$, which corresponds to a $(1+\eps)$-approximate min-cost
matching of size $k$ in $G$.
The algorithm terminates in
$\log_2(\overline{\theta}/\underline{\theta}) = O(\log(n/\eps))$ rounds.

Next, we describe the two stages in detail.

\paragraph{Scale initialization.}
In the first round, we compute a $\overline{\theta}$-optimal pseudoflow by
simply setting $f(\arc vw) = 0$ for all arcs in $\vec{E}$.
For subsequent rounds, we adjust the potential and flow in $N$ as follows:
we raise the potential of all nodes in $A$ by $\theta$, those in $B$ by $2\theta$,
and of $t$ by $3\theta$.
The potential of $s$ remains unchange.
%
Since the reduced cost of every forward arc in $N_f$ after the previous round
is at least $-2\theta$, the above step increases the reduced cost of all
forward arcs by $\theta$, and the reduced cost of all forward arcs is at least
$-\theta$.

Next, for each backward arc $\arc wv$ in $N_f$ with $c_\pi(\arc wv) < -\theta$,
we set $f(\arc vw) = 0$ (that is, make arc $\arc vw$ idle), which replaces the
backward arc $\arc wv$ in $N_f$ with a forward arc of postive reduced cost.
After this step, the resulting pseudoflow must be $\theta$-optimal as all arcs
of $N_f$ have reduced cost at least $-\theta$.

The desaturation of each backward arc creates one unit of excess.
Since there are at most $3k$ backward arcs, the total excess in the resulting
pseudoflow is at most $3k$.
There are $O(n)$ potential updates and $O(k)$ arcs on which flow might change,
therefore the time needed for \textsc{Scale-Init} is $O(n)$.

\paragraph{Refinement.}
The procedure \textsc{Refine} converts a $\theta$-optimal pseudoflow with
$O(k)$ excess into a $\theta$-optimal circulation, using a primal-dual
augmentation algorithm.
A path in $N_f$ is an \EMPH{augmenting path} if it begins at an excess vertex
and ends at a deficit vertex.
We call an admissible pseudoflow $g$ in the residual network $N_f$ an
\EMPH{admissible blocking flow} if $g$ saturates at least one arc in every
admissible augmenting path in $N_g$.
In other words, there is no admissible excess-deficit path in the residual
network after augmentation by $g$.
Each iteration of \textsc{Refine} finds an admissible blocking flow to be added
to the current pseudoflow in two steps:
\begin{enumerate}
\item
\EMPH{Hungarian search}: a Dijkstra-like search that begins at the set of
excess vertices and raises potentials until there is an excess-deficit path
of admissible arcs in $N_f$.
\item
\EMPH{Augmentation}: using depth-first search through the set of admissible
arcs of $N_f$, construct an admissible blocking flow.
It suffices to repeatedly extract admissible augmenting paths until no more
admissible excess-deficit paths remain.
\end{enumerate}
The algorithm repeats these steps until the total excess becomes zero.
The following lemma bounds the number of iterations in the \textsc{Refine}
procedure at each scale.

\begin{lemma}
\label{lemma:refine_iters}
Let $\theta$ be the scaling parameter and $\pi_0$ the potential function at the
beginning of a round, such that there exists an integral $2\theta$-optimal
circulation with respect to $\pi_0$.
Let $f$ be a $\theta$-optimal pseudoflow with excess $O(k)$.
Then \textsc{Refine} terminates within $O(\sqrt{k})$ iterations.
\end{lemma}
\begin{proof}
We sketch the proof, which is adapted from \cite{GHKT17}.
Let $f_0$ be the assumed $2\theta$-optimal integral circulation with respect to $\pi_0$,
and let $\pi$ be the potentials maintained and changing during \textsc{Refine}.
Let $d(v) = (\pi(v) - \pi_0(v))/\theta$, i.e. the increase in potential
at $v$ in units of $\theta$.
We divide the iterations of \textsc{Refine} into two phases: before and after
every (remaining) excess vertex has $d(v) \geq \sqrt{k}$.
Each Hungarian search raises excess potentials by at least $\theta$,
since we use blocking flows.
Thus, the first phase lasts at most $\sqrt{k}$ iterations.

At the start of the second phase, consider the set of arcs
$E^+ = \{\arc vw \in \vec{E} \mid f(\arc vw) < f_0(\arc vw)\}$.
One can argue that the remaining excess with respect to $f$ is bounded above by
the size of any cut separating the excess and deficit vertices.
The proof examines cuts $Y_i = \{v \mid d(v) > i\}$ for $0 \leq i \leq \sqrt{k}$.
By $\theta$-optimality of $f$ and $2\theta$-optimality of $f_0$, one can show
that each arc in $E^+$ crosses at most 3 cuts.
Furthermore, the size of $E^+$ is $O(k)$, bounded by the support size of $f$ and $f_0$.
Averaging, there is a cut among the $Y_i$ of size at most $3k/\sqrt{k}$,
so the total excess remaining is $O(\sqrt{k})$.
Each iteration of $\textsc{Refine}$ eliminates at least one unit of excess,
so the number of second phase iterations is also at most $O(\sqrt{k})$.
\end{proof}

In the next subsection we show that after $O(n\polylog n)$ preprocessing,
an iteration of \textsc{Refine} can be performed in $O(k\polylog n)$ time
(cf. Lemma~\ref{lemma:refine_iter_time}).
By Lemma~\ref{lemma:refine_iters} and the fact the algorithm terminates in
$O(\log(n/\eps))$ rounds, the overall running time of the algorithm is
$O((n + k^{3/2})\polylog n \log(1/\eps))$, as claimed in Theorem~\ref{theorem:gmcm}.

\subsection{Fast implementation of refinement}
\label{SS:fast_refine}

We now describe a fast implementation of the refinement stage.
The Hungarian search and augmentation steps are similar:
each traversing through the residual network using admissible arcs starting
from the excess vertices.
Due to lack of space, we only describe the Hungarian search process.

At a high level, let $X$ be the subset of nodes visited by the Hungarian search
so far.
Initially $X$ is the set of excess nodes.
At each step, the algorithm finds a minimum reduced cost arc $\arc vw$ in $N_f$
from $X$ to $V \setminus X$.
If $\arc vw$ is not admissible, the potential of all nodes in $X$ is increased
by $\Ceil{c_\pi(\arc vw)/\theta}$ to make $\arc vw$ admissible.
If $w $ is a deficit node, the search terminates.
Othwerise, $w$ is added to $X$ and the search continues.

Implementing the Hungarian search efficiently is more difficult than in
Section~\ref{section:hung} because (a) excess nodes may show up in $A$ as well as in $B$,
(b) a balanced node may become imbalanced later in the rounds,
and (c) the potential of excess nodes may be non-uniform.
We therefore need a more complex data structure.

We call a node $v$ of $N$ \EMPH{inactive} if $\fsupply_f(v) = 0$ and no arc of
$\supp(f)$ is incident to it; otherwise $v$ is \EMPH{active}.
We note that $s$ and $t$ are always active.
Let $\overline{A}$ (resp.\ $\overline{B}$, $\overline{X}$) denote the set of
active nodes of $A$ (resp.\ $B$, $X$).
There are only $O(k)$ active nodes, as each can be charged to its
adjoining $\supp(f)$ arcs or its excess/deficit.
We treat active and inactive nodes separately to implement the Hungarian search
efficiently.
By definition, inactive vertices only adjoin forward arcs in $N_f$.
Thus, the in-degree (resp.\ out-degree) of $\overline{A}$ (resp.\ $\overline{B}$)
is 1, and any path passing through an inactive vertex has a subpath of the form
$s \arcto v \arcto b$ for some $b \in B$ or $a \arcto v \arcto t$ for some $a \in A$.
Consequently, a path in $N_f$ may have at most two consecutive inactive nodes,
and in the case of two consecutive inactive nodes there is a subpath of the
form $s \arcto v \arcto w \arcto t$ where $v \in \overline{A}$ and $w \in \overline{B}$.
We will call such paths, from an active node to an active node
with only inactive interior nodes, \EMPH{active-active paths}.
We extend the notions of reduced cost and admissibility to active-active paths,
where the reduced cost of the path is the sum of its edges.
Since reduced costs telescope, the reduced cost of an active-active path
depends only on the potentials at its (active) endpoints.

Using this observation, we implement the Hungarian search to ``skip over''
inactive nodes, while logically exploring the same active nodes in the same order.
Active-active paths may have length 1 (no inactive interior nodes), 2, or 3.
At each step, we find a minimum reduced cost active-active path $\Pi$ from an
active node of $X$ to an active node of $V \setminus X$, and add the nodes of
this path into $X$ in a single step.
We update potentials in $X$ according to the reduced cost of the path.
There are $O(k)$ active nodes, so the number of minimization queries per
Hungarian search is $O(k)$.

We find the minimum reduced cost active-active path of length $1$, $2$, and $3$,
and then choose the cheapest among them.
We now describe a data structure for each path length.
For each data structure, our ``time budget'' per Hungarian search is $O(k\polylog n)$.

\paragraph{Finding length-1 paths.}
This data structure finds a minimum reduced cost arc from an active node of
$X$ to an active node of $V \setminus X$.
There are $O(k)$ backward arcs, so the minimum among backward arcs can be
maintained explicitly in a priority queue on $c_\pi$ and retrieved in $O(1)$ time.

There are three types of forward arcs: $\arc sa$ for some $a \in \overline{A}$,
$\arc bt$ for some $b \in \overline{B}$, and bipartite arc $\arc ab$ with two
active endpoints.
Edges of the first (resp.\ second) type can be found by maintaining
$\overline{A} \setminus X$ (resp.\ $\overline{B} \cap X$) in a priority queues
on $\pi$, but should only be queried if $s \in X$ (resp.\ $t \not\in X$).
%
The cheapest arc of the third type is an (additively weighted) bichromatic
closest pair (BCP) between $\overline{A} \cap X$ and $\overline{B} \setminus X$,
with reduced cost as the pair distance \note{potentials?}.
%Indeed, since each arc has unit capacity, it can be shown that there is no
%backward arc in $N_f$ from $\overline{B} \setminus X$ to $\overline{A} \cap X$.
We thus maintain $\overline{A} \cap X$, $\overline{B} \setminus X$ in a dynamic
BCP data structure \cite{KMRSS17} on which insertions/deletions can be performed
in $O(\polylog k)$ time.

\paragraph{Finding length-2 paths.}
We describe how to find the cheapeast path of the form $s \arcto v \arcto b$ where
$v$ is inactive and $b \in \overline{B}$.
A cheapest path of the form $a \arcto v \arcto t$ can be found similarly.
As for length-1 paths, we only query paths originating from $s$ if $s \in X$,
and only query paths ending at $t$ if $t \not\in X$.

Note that $c_\pi(s \arcto v \arcto b) = c(v, b) + \pi(b) - \pi(s)$.
Since $\pi(s)$ is common in all such paths, it suffices to find the pair minimizing
\[
\min_{v \in A \setminus \overline{A}, w \in \overline{B} \setminus X} c(v, w) + \pi(w)
\]
This is done by maintaining a dynamic BCP data structure between
$A \setminus \overline{A}$ and $\overline{B} \setminus X$ with
the cost of a pair $(v, w)$ being $c(v, w) + \pi(w)$.
We may require an update operation for each active node added to $X$ during the
Hungarian search, of which there are $O(k)$, so the time spent during a search
is $O(k\polylog n)$.

Since the size of $A \setminus \overline{A}$ is at least $r-k$, we cannot
construct this BCP from scratch at the beginning of each iteration of Hungarian search.
To resolve this, we use the idea of rewinding from Section~\ref{SS:rewinding},
with a slight twist.
There are now \emph{two} ways that the initial BCP may change across
consecutive Hungarian searches: (1) the initial set $X$ may change as vertices
lose excess through augmentation, and (2) the set of inactive $A$ vertices may
change; that is, when flow is augmented across a vertex of $A \setminus \overline{A}$.
The first is identical to the situation in Section~\ref{SS:rewinding};
the number of excess depletions is $O(k)$ over the course of \textsc{Refine}.
For the second, the active/inactive status of a node can change only if the
blocking flow found in the augmentation process passes through it.
By Lemma~\ref{lemma:blocking_incidences} below, there are $O(k)$ such changes.
Thus, updating $A \setminus \overline{A}$ for the BCP (after augmentation)
can be done in $O(k\polylog n)$ time for each Hungarian search.

\paragraph{Finding length-3 paths.}
We now describe how to find the cheapest path of the form $s \arcto v \arcto w \arcto t$
where $v \in A \setminus \overline{A}$ and $w \in B \setminus \overline{B}$.
Note that $c_\pi(s \arcto v \arcto w \arcto t) = c(\arc vw) - \pi(s) + \pi(t)$.
Hence, we need to find a BCP between $A \setminus \overline{A}$ and $B \setminus \overline{B}$,
where the cost of a pair $(v, w)$ is simply $c(v, w)$.
This can be done by maintaining a dynamic BCP data structure similar to the
case of length-2 paths.

The BCP sets have no dependence on $X$ --- the only updates required correspond
to changes to $\overline{A}$ or $\overline{B}$, after an augmentation.
Applying Lemma~\ref{lemma:blocking_incidences} again, there are $O(k)$
active/inactive updates caused by an augmentation, so the time for these updates
per Hungarian search is $O(k\polylog n)$.

\paragraph{Updating potentials.}
The Hungarian search periodically raises the potentials for all nodes in $X$,
and we need to implement this efficiently for the data structures above.
Note that the data structures above do not utilize potentials of inactive nodes.
Potentials for active nodes can be updated in a batched fashion using the
method in Section~\ref{SS:potential-update}.

For inactive nodes, we ignore their potentials entirely and instead recover
``valid'' potentials for them once they switch from inactive to active
(and additionally, at the end of a round).
Specifically, for a newly active $a \in A$ we set $c_\pi(a) \gets c_\pi(s)$
and for newly-active $b \in B$ we set $c_\pi(b) \gets c_\pi(t)$.
Formally, we will say an active-active path is \EMPH{strongly admissible}
under $\pi$ if all of its arcs are admissible under $\pi$.
For correctness, we need to show that our choice of recovered potentials
(1) preserves $\theta$-optimality and (2) makes any admissible active-active
path from before augmentation strongly admissible.
It is a straightforward calculation to verify that both properties hold.

The following lemmas are crucial to analyzing the running time of the Hungarian search.

\begin{lemma}
\label{lemma:cost_scale_relaxations}
The Hungarian search explores $O(k)$ nodes before a deficit vertex is reached,
and each augmentation explores $O(k)$ nodes before finding a blocking flow.
\end{lemma}
\begin{proof}
To sketch the proof, both procedures perform a search over the set of active
nodes, of which there are $O(k)$.
Nodes are not explored more than once (added to $X$, or a stack) by either search.
\end{proof}

\begin{lemma}
\label{lemma:blocking_incidences}
The blocking flow found by augmentation is incident to $O(k)$ vertices.
\end{lemma}
\begin{proof}
Each excess-deficit path that composes the blocking flow is a sequence of
active-active paths.
Each active-active path used is the result of the depth-first search in
augmentation exploring a non-explored active node.
Applying Lemma~\ref{lemma:cost_scale_relaxations}, the total number of nodes
crossed by the flow is $O(k)$, since active-active paths have length at most 3.
\end{proof}

Augmentation can also be implemented in $O(k\polylog n)$ time, after
$O(n\polylog n)$ preprocessing, using a similar set of data structures but
for nearest neighbor instead of BCP.
Putting everything together, we obtain the following:

\begin{lemma}
\label{lemma:refine_iter_time}
After $O(n\polylog n)$ preprocessing, each iteration of \textsc{Refine} can be
implemented in $O(k\polylog n)$ time.
\end{lemma}


% ----------------------------------------------------------------------------
\section{Unbalanced Transportation}
\label{section:orlin}

In this section, we give an exact algorithm which solves the planar transportation
problem in $O((r^2\sqrt{n} + rn^{3/2})\polylog n)$ time, proving Theorem~\ref{theorem:orlin}.
Our strategy is to use the standard reduction to the uncapacitated
min-cost flow problem, and provide a fast implementation under the geometric setting for the uncapacitated min-cost flow algorithm by Orlin~\cite{O93}, combined with some of the tools developed in Sections~\ref{section:hung} and~\ref{section:goldberg}.
%Mainly, we batch potential updates and use the rewinding mechanism to initialize each Hungarian search in time proportional to the previous Hungarian search.

% There is a simple reduction from the transportation problem to the uncapacitated
% min-cost flow problem.
% Consider the complete bipartite graph $G$ between $A$ and $B$ (with all edges
% directed from $A$ to $B$).
% Set the costs $c(a, b)$ to be  $\norm{a-b}_p^q$, all capacities $u(a, b)$ to
% infinity, and the supply-demand function $\fsupply = \tsupply$.
% Any circulation $f$ in the network $N = (G, c, u, \fsupply)$ can be converted
% into a feasible transportation map $\tau_f$ by taking
% $\tau_f(a, b) \coloneqq f(\arc ab)$ for every edges $(a, b)$.
% One simply has $\cost(f) = \cost(\tau_f)$.


For lack of space, we only sketch Orlin's strongly polynomial-time algorithm for uncapacitated min-cost flow problem~\cite{O93}.
See Appendix~\ref{SSA:orlin} for a brief introduction, as well as the original paper.
%
In short, Orlin's algorithm follows the \EMPH{excess-scaling} paradigm under the primal-dual framework:
Maintain a \EMPH{scale parameter $\Delta$}, initially set to $U$.
A vertex $v$ is \EMPH{active} if $\abs{\fsupply_f(v)} \geq \alpha\Delta$ for a fixed parameter $\alpha \in (0.5, 1)$.
Repeatedly run a \emph{Hungarian search} that raises potentials (while maintaining dual
feasibility) to create an admissible augmenting excess-deficit path between active vertices, on which
we perform flow augmentations.
Once there are no more active excess or deficit vertices, $\Delta$ is halved.
Each sequence of augmentations where $\Delta$ holds a constant value is called
an \EMPH{excess scale}.
On top of that, the algorithm performs contraction on arcs with flow value at least $3n\Delta$ at the beginning of a scale, in which case the flow and potentials are no longer tracked, as well as aggressive $\Delta$-lowering under circumstances.

\begin{toappendix}
\subsection{Uncapacitated MCF by excess scaling}
\label{SSA:orlin}

We give an outline of the strongly polynomial-time algorithm for uncapacitated min-cost flow problem
from Orlin~\cite{O93}.
Orlin's algorithm follows an \EMPH{excess-scaling} paradigm originally due to
Edmonds and Karp~\cite{EK72}.
Consider the basic primal-dual framework used in the previous sections:
The algorithm begins with both flow $f$ and potentials $\pi$ set to zero.
Repeatedly run a \emph{Hungarian search} that raises potentials (while maintaining dual
feasibility) to create an admissible augmenting excess-deficit path, on which
we perform flow augmentations.
%If supplies/demands are integral and at least one unit of flow is pushed every time, then such an algorithm terminates.
In terms of cost, $f$ is maintained to be $0$-optimal with respect to $\pi$
and each augmentation over admissible edges preserves $0$-optimality (by Lemma~\ref{lemma:eps_opt_preserve}).
Thus, the final circulation must be optimal.
The excess-scaling paradigm builds on top of this skeleton by specifying (i) between which
excess and deficit vertices we send flows, and (ii) how much flow is sent by the
augmentation.

The excess-scaling algorithm maintains a \EMPH{scale parameter $\Delta$},
initially set to $U$.
A vertex $v$ with $\abs{\fsupply_f(v)} \geq \Delta$ is called \EMPH{active}.
Each augmenting path is chosen between an active excess vertex and an active
deficit vertex.
Once there are no more active excess or deficit vertices,
$\Delta$ is halved.
Each sequence of augmentations where $\Delta$ holds a constant value is called
an \EMPH{excess scale}.
There are $O(\log U)$ excess scales before $\Delta < 1$ and, by integrality of
supplies/demands, $f$ is a circulation.

With some modifications to the excess-scaling algorithm, Orlin~\cite{O93}
obtains a strongly polynomial bound on the number of
augmentations and excess scales.
First, an \EMPH{active} vertex is redefined to be one satisfying
$\abs{\fsupply_f(v)} \geq \alpha\Delta$, for a fixed parameter $\alpha \in (0.5, 1)$.
Second, arcs with flow value at least $3n\Delta$ at the beginning of a scale
are \EMPH{contracted} to create a new vertex, whose supply-demand is
the sum of those on the two endpoints of the contracted arc.
%$\fsupply(\hat{v}) \gets \fsupply(\hat{v}) + \fsupply(\hat{w})$.
We use $\hat{G} = (\hat{V}, \hat{E})$ to denote the resulting
\EMPH{contracted graph}, where each $\hat{v} \in \hat{V}$ is a contracted
component of vertices from $V$.
Intuitively, the flow is so high on contracted arcs that no set of future
augmentations can remove the arc from $\supp(f)$.
Third, in additional to halving, $\Delta$ is aggressively lowered to $\max_{v \in V} \fsupply_f(v)$ if there are no
active excess vertices and $f(\arc vw) = 0$ holds for every arc $\arc vw \in \hat{E}$.
Finally, flow values are not tracked within contracted components, but once an
optimal circulation is found on $\hat{G}$, optimal potentials $\pi^*$ can be
\EMPH{recovered} for $G$ by sequentially undoing the contractions.
The algorithm then performs a post-processing step which finds the optimal
circulation $f^*$ on $G$ by solving a max-flow problem on the set of admissible
arcs under $\pi^*$.

\end{toappendix}

\begin{theorem}[(Orlin~{\cite[Theorems~2 and 3]{O93}})]
\label{theorem:orlin_old}
Orlin's algorithm finds a set of optimal potentials after $O(n\log n)$ scaling phases
and $O(n\log n)$ total augmentations.
\end{theorem}

The remainder of the section focuses on showing that each augmentation can be
implemented in $O((r^2/\sqrt{n}+r\sqrt{n})\polylog n)$ time (after preprocessing).
%Additionally, we show that $f^*$ can be recovered from $\pi^*$ very quickl in our setting.
%
A subtle issue is that our geometric data structures must deal with real points
in the plane instead of the contracted components.
A solution is provided by Agarwal~\etal~\cite{AFPVX17};
for the sake of completeness, we describe the method for maintaining
contractions under dynamic data structures in Appendix~\ref{SSA:contraction}.

\begin{toappendix}
\subsection{Implementing contractions}
\label{SSA:contraction}

%\paragraph{Implementing contractions.}
\note{REWRITE}
Following Agarwal~\etal~\cite{AFPVX17}, our geometric data structures deals
with real points in the plane instead of the contracted components.
We will track the contracted components described in $\hat{G}$ (e.g.\ with a
disjoint-set data structure) and mark the arcs of $\supp(f)$ that are
contracted.
We maintain potentials on the points $A$ and $B$ directly, instead of the
contracted components.

When conducting the Hungarian search, we initialize $S$ to be the set of vertices from
\EMPH{active excess contracted components} who (in sum) meet the imbalance
criteria. \note{unclear}
Upon relaxing any $v \in \hat{v}$, we immediately relax all the contracted
support arcs which span $\hat{v}$.
Since the input network is uncapacitated, each contracted component is
strongly connected in the residual network by the admissible forward/backward
arcs of each contracted arc. \note{unparsable}
To relax arcs in $\hat{E}$, we relax the support arcs before attempting to
relax any non-support arcs; this will guarantee that the underlying graph of the support is acyclic (see Lemma~\ref{lemma:orlin_acyclic}).
Relaxations of support arcs can be performed without further potential changes,
since they are admissible by invariant.

During the augmentations, contracted residual arcs are considered to have infinite
capacity, and we do not update the value of flows on these arcs.
We allow augmenting paths to begin from any point $a \in \hat{v} \cap A$ in an active
excess component $\hat{v}$, and end at any point $b \in \hat{w} \cap B$ in an active
deficit component $\hat{w}$.

\end{toappendix}

\paragraph{Recovering optimal flow.}
%Rewinding contracted components to recover an optimal flow na\"ively takes $O(rn^2)$ time.
Using a strategy from Agarwal~\etal~\cite{AFPVX17}, we can recover the optimal
flow in time $O(n\polylog n)$.
%
If furthermore the cost function is just the $p$-norm (without the $q$th-power),
we show a structural result which is interesting on its own right:
the set of admissible arcs under an optimal potential forms a planar graph.
Thus, we can extract the admissible network in $O(n\polylog n)$ time thanks
to the sparsity of planar graphs, and compute the flow using planar
multiple-source multiple-sink maximum-flow algorithm by
Borradaile~\etal~\cite{BKMNW17} which runs in $O(n\log^3 n)$ time.
For details see Appendix~\ref{SSA:flow-recovery} and~\ref{SSA:flow-recovery-planar}.

\begin{toappendix}

\subsection{Recovering the optimal flow}
\label{SSA:flow-recovery}

We use the recovery strategy from Agarwal~\etal~\cite{AFPVX17}, which runs in
$O(n\polylog n)$ time.
The main idea is that, if $\EuScript{T}$ is an undirected \emph{spanning tree of admissible edges}
under optimal potentials $\pi^*$, then there exists an optimal flow $f^*$ with
support only on arcs corresponding to edges of $\EuScript{T}$.
Intuitively, $\EuScript{T}$ is a maximal set of linearly independent dual LP
constraints for the optimal dual ($\pi^*$), so there exists an optimal primal
solution ($f^*$) with support only on these arcs.
To see this, we can use a perturbation argument: raising the cost of each
non-tree edge by a positive amount does not change $\cost(\pi^*)$ or the feasibility
of $\pi^*$, but does raise the cost of any circulation $f$ using non-tree edges.
Strong duality suggests that $\cost(f^*) = \cost(\pi^*)$ is unchanged,
therefore $f^*$ must have support only on the tree edges.

Since the arcs corresponding to edges of $\EuScript{T}$ have no cycles, we can
solve the maximum flow in linear time using the following greedy algorithm.
Let $\parent(v)$ be the parent of vertex $v$ in $\EuScript{T}$.
We begin with $f^* = 0$ and process $\EuScript{T}$ from its leaves upwards.
For a supply leaf $v$, we satisfy its supply by choosing
$f^*(\arc{v}{\parent(v)}) \gets \fsupply(v)$.
Otherwise if leaf $v$ is a demand vertex, we choose
$f^*(\arc{\parent(v)}{v}) \gets -\fsupply(v)$.
Once we've solved the supplies/demands for each leaf, then we can \EMPH{trim}
the leaves, removing them from $\EuScript{T}$ and setting the supply/demand
of each parent-of-a-leaf to its current imbalance.
Then, we can recurse on this smaller tree and its new set of leaves.

\begin{lemma}
\label{lemma:orlin_tree_flow}
Let $G(\EuScript{T})$ be the subnetwork of $G$ corresponding to edges of the
undirected spanning tree $\EuScript{T}$.
If there exists a flow in $G(\EuScript{T})$ which satisfies every supply and demand,
then the greedy algorithm finds the maximum flow in $G(\EuScript{T})$ in $O(n)$ time.
\end{lemma}
\begin{proof}
Observe that, for any flow $f$ in $G$, $\supp(f)$ has no paths of length longer
than one.
Thus, if a flow $f^*$ satisfying supplies/demands exists within $G(\EuScript{T})$,
then each supply vertex has flow paths that terminate at its parent/children.
Similarly, each demand vertex receive all its flow from its parent/children.
Since there is only one option for a supply leaf (resp.\ demand leaf) to send
its flow (resp.\ receive its flow), the greedy algorithm correctly identifies
the values of $f^*$ for arcs adjoining $\EuScript{T}$ leaves.
Trimming these leaves, we can apply this argument recursively for their parents.
The running time of the greedy algorithm is $O(n)$, as leaves can be identified
in $O(n)$ time and no vertex becomes a leaf more than once.
\end{proof}

It remains to show how we construct $\EuScript{T}$.
We begin with a (spanning) \EMPH{shortest path tree} (SPT) $T$ in the residual
network of $f$, under reduced costs and rooted at an arbitrary vertex $r$.
For the SPT to span, we need the additional assumption that $G$ is strongly
connected.
We make can make $G$ strongly connected by adding a 0-supply vertex $s$ with
arcs $\arc sa$ for all $a \in A$ and $\arc ba$ for all $b \in B$, with some
high cost $M$.
Following Orlin~\cite{O93}, these arcs cannot appear in an optimal flow if $M$
is sufficiently high, and we can extend $\pi^*$ to include $s$ using
$\pi^*(s) = 0$ if $M > \max_{b \in B} \pi^*(b)$.
This extension to $\pi^*$ preserves feasibility.

The edges corresponding to arcs of $T$ do not suffice for $\EuScript{T}$, since
some SPT arcs may be inadmissible.
Let $d_r(v)$ be the shortest path distance of $v \in A \cup B \cup \{s\}$ from
$r$, and consider potentials $\pi^\# = \pi^* - d_r$.

\begin{lemma}[(Orlin~{\cite[Lemma 3]{O93}})]
\label{lemma:orlin_spt_dist}
Let $f$ be a flow satisfying the optimality conditions with respect to $\pi^*$.
Then, (i) $f$ satisfies the optimality conditions with respect to $\pi^\#$, and
(ii) all SPT arcs are admissible under $\pi^\#$.
\end{lemma}

We can use this lemma to argue that $\pi^\#$ is still optimal.
Recall that $f$ has values defined only on the non-contracted residual arcs;
we can apply the first part of Lemma~\ref{lemma:orlin_spt_dist} on these arcs.
For arcs within contracted components, we use a different argument.
Observe that each $\hat{v} \in \hat{V}$ is spanned by a set of $\supp(f)$ arcs,
which are admissible by invariant.
Thus, all $v \in \hat{v}$ are equidistant from $r$, and they will have the same
value $d_r(v)$.
It follows that the reduced costs of arcs with both endpoints in $\hat{v}$
do not change when replacing $\pi^*$ with $\pi^\#$, so arcs contained in
$\hat{v}$ that met the optimality conditions for $\pi^*$ still meet them for
$\pi^\#$.

From the second part of Lemma~\ref{lemma:orlin_spt_dist}, the SPT $T$ is a
spanning tree of admissible arcs under $\pi^\#$.
We set $\EuScript{T}$ to be the set of undirected edges corresponding to $T$.

\paragraph{Computing the SPT.}
We conclude by describing the procedure for building the SPT, i.e.\ by
running Dijkstra's algorithm in the residual network.
We use a geometric implementation that is very similar to Hungarian search.
We begin with $S = \{r\}$ and $d_r(r) = 0$, where $r$ is our arbitrary root.
For all other vertices, $d_r(v)$ is initially unknown.
In each iteration, we relax the minimum-reduced cost arc $\arc vw$ in the
frontier $S \times (A \cup B) \setminus S$, adding $w$ to $S$, and setting
$d_r(w) = d_r(v) + c_{\pi^*}(v, w)$.
Once $S = A \cup B$, the SPT $T$ is the set of relaxed arcs.

If an either direction of an arc of $\supp(f)$ enters the frontier, we relax it
immediately.
To detect support arcs, we build a list for each $v \in A \cup B$ of the
support arcs which use $v$ as an endpoint, and once $v \in S$ we check its list.
There are $O(n)$ support arcs in total (by acyclicity of $E(\supp(f))$; see Lemma~\ref{lemma:orlin_acyclic}), so
the total time spent searching these lists is $O(n)$.
Such relaxations are correct for the shortest path tree, since the support
edges are admissible and reduced costs are nonnegative.

Other edges appearing in the frontier can be split into three categories:
\begin{enumerate}
\item Forward $A$-to-$B$ arcs.
	We query these using a BCP with $P = A \cap S$ and $Q = B \setminus S$.
\item $B$-to-$s$ arcs.
	These will never have flow support.
	We can query the minimum with a max-heap on potentials of $B \cap S$.
	We query these while $s \in S$.
\item $s$-to-$A$ arcs.
	These will also never have flow support.
	We can query the minimum with a min-heap on potentials of $A \setminus S$.
	We query these while $s \in S$.
\end{enumerate}
We perform $O(n)$ relaxations and takes $O(\polylog n)$ time per relaxation,
for non-support relaxations.
An additional $O(n)$ time is spent relaxing support edges.
The total running time of Dijkstra's algorithm is $O(n\polylog n)$.
Combining with Lemma~\ref{lemma:orlin_tree_flow}, we obtain the following.

\begin{lemma}
Given optimal potentials $\pi^*$ and an optimal contracted flow $f$, the
optimal flow $f^*$ can be computed in $O(n\polylog n)$ time.
\end{lemma}

\subsection{Recovering the optimal flow for sum-of-distances.}
\label{SSA:flow-recovery-planar}

When the matching objective uses the just the $p$-norms (that is, when $q=1$),
we can prove that the subgraph formed by admissible arcs is in fact \emph{planar}.
Planarity gives us two things for recovery:
there are only a linear number of admissible arcs, and the max-flow on them can
be solved in near-linear time with a planar graph multiple-source multiple-sink
max-flow algorithm.
Note that this recovery algorithm is not asymptotically faster than the other,
and depending on the planar max-flow algorithm, not necessarily simpler either.

Up until now, we have not placed restrictions on coincidence between $A$ and $B$,
but for the next proof it is useful to do so.
We can assume that all points within $A \cup B$ are distinct, otherwise we can
replace all points coincident at $x \in \reals^2$ with a single point whose
supply/demand is $\sum_{v \in A \cup B: v=x}\tsupply(v)$.
This is roughly equivalent to transporting as much as we can between
coincident supply and demand, and is optimal by triangle inequality.

Without loss of generality, assume $\pi^*$ is nonnegative (raising $\pi^*$
uniformly on all points does not change the objective or feasibility).
Recall that $\pi^*$ is feasibility if for all $a \in A$ and $b \in B$
\[
	c_{\pi^*}(\arc ab) = \norm{a-b}_p - \pi^*(a) + \pi^*(b) \geq 0.
\]
An arc $\arc ab$ is admissible when
\[
	c_{\pi^*}(\arc ab) = \norm{a-b}_p - \pi^*(a) + \pi^*(b) = 0.
\]
We note that these definitions have a nice visual:
Place disks $D_q$ of radius $\pi(q)$ at each $q \in A \cup B$.
Feasibility states that for all $a \in A$ and $b \in B$, $D_a$ cannot contain
$D_b$ with a gap between their boundaries.
The arc $\arc ab$ is admissible when $D_a$ contains $D_b$ and their boundaries
are tangent.

\begin{lemmarep}
\label{lemma:admiss_planar}
Let $\pi^*$ be a set of optimal potentials for the point sets $A$ and $B$,
under costs $c(a, b) = \norm{a-b}_p$.
Then, the set of admissible arcs under $\pi^*$ form a planar graph.
\end{lemmarep}

\begin{proof}
We assume the points of $A \cup B$ are in general position (e.g.\ by symbolic
perturbation) such that no three points are collinear.
Let $\arc{a_1}{b_1}$ and $\arc{a_2}{b_2}$ be any pair of admissible arcs under
$\pi^*$.
We will isolate them from the rest of the points, considering $\pi^*$
restricted to the four points $\{a_1, a_2, b_1, b_2\}$.
Clearly, this does not change whether the two arcs cross.
Observe that we can raise $\pi^*(a_2)$ and $\pi^*(b_2)$ uniformly, until
$c_\pi(a_2, b_1) = 0$, without breaking feasibility or changing admissibility
of $\arc{a_1}{b_1}$ and $\arc{a_2}{b_2}$
Henceforth, we assume that we have modified $\pi^*$ in this way to make
$\arc{a_2}{b_1}$ admissible.
Given positions of $a_1$, $a_2$, and $b_1$, we now try to place $b_2$ such that
$\arc{a_1}{b_1}$ and $\arc{a_2}{b_2}$ cross.
Specifically, $b_2$ must be placed within a region $\EuScript{F}$ that lies
between the rays $\overrightarrow{a_2 a_1}$ and $\overrightarrow{a_2 b_1}$,
and within the halfplane bounded by $\overleftrightarrow{a_1 b_1}$ that does
not contain $a_2$.
%TODO figure of the feasible region

Let $g_a(q) \coloneqq \norm{a-q} - \pi^*(a)$ for $a \in A$ and
$q \in \reals^2$.
Let the \EMPH{bisector} between $a_1$ and $a_2$ be
$\beta \coloneqq \{q \in \reals^2 \mid g_{a_1}(q) = g_{a_2}(q)$.
$\beta$ is a curve subdividing the plane into two open faces, one where
$g_{a_1}$ is minimized and the other where $g_{a_2}$ is.
From these definitions, admissibility of $\arc{a_1}{b_1}$ and $\arc{a_2}{b_1}$
imply that $b_1$ is a point of the bisector.

We show that $\EuScript{F}$ lies entirely on the $g_{a_1}$ side of the
bisector.
First, we prove that the closed segment $\overline{a_1 b_1}$ lies entirely on
the $g_{a_1}$ side, except $b_1$ which lies on $\beta$.
Any $q \in \overline{a_1 b_1}$ can be written parametrically as
$q(t) = (1-t) b_1 + t a_1$ for $t \in [0,1]$.
Consider the single-variable functions $g_{a_1}(q(t))$ and $g_{a_2}(q(t))$.
\begin{equation*}
\begin{aligned}
	g_{a_1}(q(t)) &= (1-t)\norm{a_1 - b_1} - \pi(a_1) \\
	g_{a_2}(q(t)) &= \norm{(a_2 - b_1) - t(a_1 - b_1)} - \pi(a_2)
\end{aligned}
\end{equation*}
At $t=0$, these expressions are equal.
%TODO would like to work out this calculation; is this correct?
Observe that the derivative with respect to $t$ of $g_{a_1}(q(t))$ is less than
$g_{a_2}(q(t))$.
Indeed, the value of $\frac{d}{dt}\norm{(a_2 - b_1) - t(a_1 - b_1)}$ is at
least $-\norm{a_1 - b_1} = \frac{d}{dt}g_{a_1}(q(t))$, which is realized if and only if
$\frac{(a_2 - b_1)}{\norm{a_2 - b_1}} = \frac{(a_1 - b_1)}{\norm{a_1 - b_1}}$.
This corresponds to $\overrightarrow{a_2 b_1}$ and $\overrightarrow{a_1 b_1}$
being parallel, but this is disallowed since $a_1, a_2, b_1$ are in general
position.
Thus, $g_{a_1}(q(t)) \leq g_{a_2}(q(t))$ with equality only at $b_1$.

Now, we parameterize each point of $\EuScript{F}$ in terms of points on
$\overline{a_1 b_1}$.
Every $q \in \EuScript{F}$ can be written as $q(t') = q' + t'(q' - a_2)$ for
some $q' \in \overline{a_1 b_1}$ and $t \geq 0$, i.e.
$q' = \overline{a_1 b_1} \cap \overrightarrow{a_2 q}$.
We call $q'$ the \EMPH{projection} of $q$ onto $\overline{a_1 b_1}$.
We can write $g_{a_1}$ and $g_{a_2}$ in terms of $t'$ and observe that
$\frac{d}{dt'}g_{a_1}(q(t')) \leq \frac{d}{dt'}g_{a_2}(q(t'))$, as the
derivative of $g_a(q(t'))$ is maximized if $(q(t') - a)$ is parallel to
$(q(t') - a_2)$ and lower otherwise.
Notably, $q(t')$ with projection $b_1$ have
$\frac{d}{dt'}g_{a_1}(q(t')) < \frac{d}{dt'}g_{a_2}(q(t'))$, since
$a_1, a_2, b_1$ are in general position.
Any $q(t')$ with a different projection do not have strict inequality, but
the projection itself has $g_{a_1}(q') < g_{a_2}(q')$ for $q' \neq b_1$ since
it lies on $\overline{a_1 b_1}$.
Therefore, for all $q \in \EuScript{F} \setminus \{b_1\}$,
$g_{a_1}(q') < g_{a_2}(q')$, and $\EuScript{F}$ lies on the $g_{a_1}$ side of
the bisector except for $b_1$ which lies on $\beta$.
We can eliminate $b_1$ as a candidate position for $b_2$, since points of $B$
cannot coincide.

Observe that $g_{a_1}(b) < g_{a_2}(b)$ for $b \in B$ implies that
$c_\pi(a_1, b) < c_\pi(a_2, b)$, and $c_\pi(a_1, b) = c_\pi(a_2, b)$ if and
only if $b$ lies on $\beta$.
This holds for all $b \in \EuScript{F}$ including our prospective $b_2$,
but then $c_\pi(a_1, b_2) < c_\pi(a_2, b_2) = 0$ since $\arc{a_2}{b_2}$ is
admissible.
This violates feasibility of $\arc{a_1}{b_2}$, so there is no feasible
placement of $b_2$ which also crosses $\arc{a_1}{b_1}$ with $\arc{a_2}{b_2}$.
\end{proof}

We can construct the entire set of admissible arcs by repeatedly querying
the minimum-reduced-cost outgoing arc for each $a \in A$ until the result is
not admissible.
By Lemma~\ref{lemma:admiss_planar} the resulting arc set forms a planar graph,
so by Euler's formula the number of arcs to query is $O(n)$.
We can then find the maximum flow in time $O(n\log^3 n)$ time, using the
planar multiple-source multiple-sink maximum-flow algorithm by
Borradaile~\etal~\cite{BKMNW17}.

\begin{lemma}
If the transportation objective is sum-of-costs, then given the optimal
potentials $\pi^*$, we can compute an optimal flow $f^*$ in $O(n\polylog n)$
time.
\end{lemma}

\end{toappendix}


%\subsection{Dead vertices and support stars}
\subsection{Support stars}

%Given Theorem~\ref{theorem:orlin_old},
%Our goal is to implement each augmentation in $O(r\sqrt{n}\polylog n)$ time.
To find an augmenting path, we again use a Hungarian search with geometric data
structures to perform relaxations quickly.
% Like in Section~\ref{section:goldberg}, there are vertices which cannot be
% charged to the flow support.
% Even worse, the flow support for the transportation problem may have size $\Omega(n)$ (consider when $A$ has one point, and demands are uniformly distributed among the vertices of $B$).
Our strategy is summarized as follows:
\begin{itemize}
\item Discard vertices that lead to dead ends in the search (not on a path
	to a deficit vertex).
\item Cluster parts of the flow support, such that the number of support arcs
	outside clusters is $O(r)$.
	The number of relaxations we perform is proportional to the number of
	support arcs outside of clusters.
\end{itemize}
Querying/updating clusters degrades our amortized time per relaxation from $O(\polylog n)$ to $O(\sqrt{n}\polylog n)$.
Thus overall each augmentation takes $O(r\sqrt{n}\polylog n)$ time.

\paragraph{Support stars.}
The vertices of $B$ with support degree 1 are partitioned into subsets
$\Sigma_a \subset B$ by the $a \in A$ lying on the other end of their single
support arc.
We call \EMPH{$\Sigma_a$} the \EMPH{support star} centered at $a \in A$.

Roughly speaking, we would like to handle each support star as a single unit.
When the Hungarian search reaches $a$ or any $b \in \Sigma_a$, the
entirety of $\Sigma_a$ (as well as $a$) is also admissibly-reachable and can be
included into $S$ without further potential updates.
Additionally, the only outgoing residual arcs of every $b \in \Sigma_a$ lead to
$a$, thus the only way to leave $\Sigma_a \cup \{a\}$ is through an arc leaving $a$.
Once a relaxation step reaches some $b \in \Sigma_a$ or $a$ itself, we would
like to quickly update the state such that the rest of $b \in \Sigma_a$ is also
reached without performing relaxation steps to each individual
$b \in \Sigma_a$.


\begin{toappendix}
\subsection{Dead vertices}
\label{SSA:dead-vertices}

% Let $E(\supp(f)) \coloneqq \{(v, w) \mid \arc vw \in \supp(f)\}$ be the set
% of undirected edges corresponding to the arcs in $\supp(f)$.
% Clearly, $\abs{\supp(f)} = \abs{E(\supp(f))}$.
Let the \EMPH{support degree} of a vertex be its degree in the graph induced by the underlying edges of $\supp(f)$.
We call a vertex $b \in B$ \EMPH{dead} if $b$ has support degree $0$ and is not an
active excess or deficit vertex; call it \EMPH{living} otherwise.
Dead vertices are essentially equivalent to the \emph{null vertices} of
Section~\ref{section:goldberg}.
However, since the reduction in this section does not use a super-source/super-sink,
we can simply remove these from consideration during a Hungarian search ---
they will not terminate the search, and have no outgoing residual arcs.
Like the null vertices, we ignore dead vertex potentials and infer feasible
potentials when they become live again.
We use \EMPH{$A_\ell$} and \EMPH{$B_\ell$} to denote the living
vertices of points in $A$ and $B$, respectively.
Note that being dead/alive is a notion strictly defined only for vertices, and not for
contracted components.

%TODO how do we efficiently identify revived vertices?
We say a dead vertex is \EMPH{revived} when it stops meeting either condition
of the definition.
Dead vertices are only revived after $\Delta$ decreases (at the start of a
subsequent excess scale) as no augmenting path will cross a dead vertex and
they cannot meet the criteria for contractions.
When a dead vertex is revived, we must add it back into each of our data
structures and give it a feasible potential.
For revived $b \in B$, a feasible choice of potential is
$\pi(b) \gets \max_{a \in A} (\pi(a) - c(a, b))$ which we can query by
maintaining a weighted nearest neighbor data structure on the points of $A$.
The total number of revivals is bounded above by the number of augmentations:
since the final flow is a circulation on $\hat{G}$ and a newly revived vertex
$v$ has no incident arcs in $\supp(f)$ and cannot be contracted, there is at least
one subsequent augmentation which uses $v$ as its beginning or end.
Thus, the total number of revivals is $O(n\log n)$.

\end{toappendix}

\subsection{Implementation details}

Before describing our workaround for support stars, we analyze the number of
relaxation steps for arcs outside of support stars.
To this end we need to strip of some \EMPH{dead} vertices---having no incident flow support edges and not an active excess or deficit vertex---that does not affect the search.
We use \EMPH{$A_\ell$} and \EMPH{$B_\ell$} to denote vertices of points in $A$ and $B$ that are not dead.
The details for handling such vertices can be found in Appendix~\ref{SSA:dead-vertices}.
%
For a proof of the following lemma, see Appendix~\ref{SSA:relax-outside-stars}.

\begin{toappendix}
\subsection{Number of relaxations}
\label{SSA:relax-outside-stars}

By prioritizing the relaxation of support arcs, we also have the following
lemma.

\begin{lemmarep}[(Agarwal~\etal~\cite{AFPVX17})]
\label{lemma:orlin_acyclic}
If arcs of $\supp(f)$ are relaxed first as they arrive on the frontier, then
$E(\supp(f))$ is acyclic.
\end{lemmarep}

\begin{proof}
Let $f_i$ be the pseudoflow after the $i$-th augmentation, and let $T_i$ be the
forest of relaxed arcs generated by the Hungarian search for the $i$-th
augmentation.
Namely, the $i$-th augmenting path is an excess-deficit path in $T_i$, and all
arcs of $T_i$ are admissible by the time the augmentation is performed.
Let $E(T_i)$ be the undirected edges corresponding to arcs of $T_i$.
Notice that, $E(\supp(f_{i+1})) \subseteq E(\supp(f_i)) \cup E(T_i)$.
We prove that $E(\supp(f_i)) \cup E(T_i)$ is acyclic by induction on $i$;
as $E(\supp(f_{i+1}))$ is a subset of these edges, it must also be acyclic.
At the beginning with $f_0 = 0$, $E(\supp(f_0))$ is vacuously acyclic.

Let $E(\supp(f_i))$ be acyclic by induction hypothesis.
Since $T_i$ is a forest (thus, acyclic), any hypothetical cycle $\Gamma$ that
forms in $E(\supp(f_i)) \cup E(T_i)$ must contain edges from both
$E(\supp(f_i))$ and $E(T_i)$.
To give a visual analogy, we will color $e \in \Gamma$
\EMPH{purple} if $e \in E(\supp(f_i)) \cap E(T_i)$,
\EMPH{red} if $e \in E(\supp(f_i))$ but $e \not\in E(T_i)$,
and \EMPH{blue} if $e \in E(T_i)$ but $e \not\in E(\supp(f_i))$.
Then, $\Gamma$ is neither entirely red nor entirely blue.
We say that red and purple edges are \EMPH{red-tinted}, and similarly blue and
purple edges are \EMPH{blue-tinted}.
Roughly speaking, our implementation of the Hungarian search prioritizes
relaxing red-tinted admissible arcs over pure blue arcs. %TODO figure

We can sort the blue-tinted edges of $\Gamma$ by the order they were relaxed
into $S$ during the Hungarian search forming $T_i$.
Let $(v, w) \in \Gamma$ be the last pure blue edge relaxed, of all the
blue-tinted edges in $\Gamma$ --- after $(v, w)$ is relaxed, the remaining
unrelaxed, blue-tinted edges of $\Gamma$ are purple.

Let us pause the Hungarian search the moment before $(v, w)$ is relaxed.
At this point, $v \in S$ and $w \not\in S$, and the Hungarian search must have
finished relaxing all frontier support arcs.
By our choice of $(v, w)$, $\Gamma \setminus (v, w)$ is a path of relaxed blue
edges and red-tinted edges which connect $v$ and $w$.
Walking around $\Gamma \setminus (v, w)$ from $v$ to $w$, we see that every
vertex of the cycle must be in $S$ already: $v \in S$, relaxed blue edges have
both endpoints in $S$, and any unrelaxed red-tinted edge must have both
endpoints in $S$, since the Hungarian search would have prioritized relaxing
the red-tinted edges to grow $S$ before relaxing $(v, w)$ (a blue edge).
It follows that $w \in S$ already, a contradiction.

No such cycle $\Gamma$ can exist, thus $E(\supp(f_i)) \cup E(T_i)$ is acyclic
and $E(\supp(f_{i+1})) \subseteq E(\supp(f_i)) \cup E(T_i)$ is acyclic.
By induction, $E(\supp(f_i))$ is acyclic for all $i$.
\end{proof}

Let \EMPH{$E(\Sigma_a)$} \note{only used once} be the underlying edges of the support star centered
at $a$ and $\EMPH{$F$} \coloneqq E(\supp(f)) \setminus \bigcup_{a \in A} E(\Sigma_a)$.
Using Lemma~\ref{lemma:orlin_acyclic}, we can show that the number of support
arcs outside support stars ($\abs{F}$) is small.

\begin{lemmarep}
\label{lemma:no_star_support_size}
$\abs{B_\ell \setminus \bigcup_{a \in A} \Sigma_a} \leq r$.
\end{lemmarep}

\begin{proof}
$F$ is constructed from $E(\supp(f))$ by eliminating edges in support stars,
therefore all edges in $F$ must adjoin vertices in $B$ of support degree at
least 2.
By Lemma~\ref{lemma:orlin_acyclic}, $E(\supp(f))$ is acyclic and therefore forms
a spanning forest over $A \cup B_\ell$, so $F$ is also a bipartite forest.
All leaves of $F$ are therefore vertices of $A$.

Pick an arbitrary root for each connected component of $F$ to establish
parent-child relationships for each edge.
As no vertex in $B$ is a leaf, each vertex in $B$ has at least one child.
Charge each vertex in $B$ to one of its children in $F$, which must belong to $A$.
Each vertex in $A$ is charged at most once.
Thus, the number of $B_\ell$ vertices outside of support stars is no more than $r$.
\end{proof}

\end{toappendix}

\begin{lemmarep}
\label{lemma:orlin_relax_count}
Suppose we have stripped the graph of dead vertices.
The number of relaxation steps in a Hungarian search outside of support stars
is $O(r)$.
\end{lemmarep}

\begin{proof}
If there are no dead vertices, then each non-support star relaxation step adds
either
(i) an active deficit vertex,
(ii) a non-deficit vertex $a \in A_\ell$, or
(iii) a non-deficit vertex $b \in B_\ell$ of support degree at least 2.
There is a single relaxation of type (i), as it terminates the search.
The number of vertices of type (ii) is $r$, and the number of vertices of type
(iii) is at most $r$ by Lemma~\ref{lemma:no_star_support_size}.
The lemma follows.
\end{proof}

%The running time of a Hungarian search will be $O(r)$ times the time it takes us to implement each relaxation.

\paragraph{Relaxations outside support stars.}
For relaxations that don't involve support star vertices, we can once again
maintain a BCP data structure to query the minimum $A_\ell$-to-$B_\ell$ arc.
To elaborate, this is the BCP between $P = A_\ell \cap S$ and
$Q = (B_\ell \setminus (\bigcup_{a \in A_\ell} \Sigma_a)) \setminus S$,
weighted by potentials.
%This can be queried in $O(\log^2 n)$ time and updated in $O(\polylog n)$ time per point.
Since the query is outside the support stars, there is at most one update per relaxation.
%
Backward (support) arcs are kept admissible by the invariant, so we relax them immediately when they arrive at the frontier.

\paragraph{Relaxing support stars.}
We classify support stars into two categories: \EMPH{big stars} with
$\abs{\Sigma_a} > \sqrt{n}$, and \EMPH{small stars} with
$\abs{\Sigma_a} \leq \sqrt{n}$.
Let $\EMPH{$A_\text{big}$} \subseteq A$ denote the centers of big stars and
and $\EMPH{$A_\text{small}$} \subseteq A$ denote the centers of small stars.
%We keep the following data structures to manage support stars.
\begin{itemize}
\item For each big star $\Sigma_a$, we use a data structure
	\EMPH{$\EuScript{D}_\text{big}(a)$} to maintain BCP between
	$P = A_\ell \cap S$ and $Q = \Sigma_a$.
%weighted by potentials.
	We query this until $a \in S$ or any vertex of $\Sigma_a$ is added to~$S$.
\item All small stars are added to a single BCP data structure
	\EMPH{$\EuScript{D}_\text{small}$} between $P = A_\ell \cap S$ and
	$Q = (\bigcup_{a \in A_\text{small}} \Sigma_a) \setminus S$.
%weighted by potentials.
	When an $a \in A_\text{small}$ or any vertex of its support star is
	added to $S$, we remove the points of $\Sigma_a$ from
	$\EuScript{D}_\text{small}$ using $\abs{\Sigma_a}$ deletion operations.
\end{itemize}
We will update these data structures as each support star center is added into
$S$.
If a relaxation step adds some $b \in B_\ell$ and $b$ is in a support star
$\Sigma_a$, then we immediately relax $\arc ba$, as all support arcs are
admissible.
%Relaxations of vertex $b \in B_\ell$ not in any support star will not affect the support star data structures.

Suppose a relaxation step adds $a \in A_\ell$ to $S$.
%For the support star data structures, w
We must
(i) remove $a$ from every $\EuScript{D}_\text{big}$,
(ii) remove $a$ from $\EuScript{D}_\text{small}$.
If $a \in A_\text{big}$, we also (iii) deactivate $\EuScript{D}_\text{big}(a)$.
If $a \in  A_\text{small}$, we also (iv) remove the points of $\Sigma_a$ from
$\EuScript{D}_\text{small}$.
The operations (i--iii) can be performed in $O(\polylog n)$ time
each, but (iv) may take up to $O(\sqrt{n}\polylog n)$ time.
%
On the other hand, there are now $O(\sqrt{n})$ data structures to query during
each relaxation step, which takes $O(\sqrt{n}\log^2 n)$ time in total.
%as there are $O(n/\sqrt{n})$ data structures $\EuScript{D}_\text{big}(\cdot)$.
%Thus, the query time within each relaxation step is $O(\sqrt{n}\log^2 n)$.
%
Together with Lemma~\ref{lemma:orlin_relax_count} we bound the time for each Hungarian search.

\begin{lemmarep}
\label{lemma:orlin_hs_time}
Hungarian search takes $O(r\sqrt{n}\polylog n)$ time.
\end{lemmarep}

\begin{proof}
The number of relaxation steps outside of support stars is $O(r)$ by
Lemma~\ref{lemma:orlin_relax_count}.
The time per relaxation outside of support stars is $O(\sqrt{n}\polylog n)$.
The time spent processing relaxations within a support star is
$O(\sqrt{n}\polylog n)$, and at most $r$ are relaxed during the search.
The total time is therefore $O(r\sqrt{n}\polylog n)$.
\end{proof}

\paragraph{Updating support stars.}
As the flow support changes, the membership of support stars may shift causing
a big star to become small or vice versa.
To efficiently support this, we introduce a soft boundary in determining whether a support star is big or small.
%
Standard charging argument shows that the amortized update time per membership
change is $O(\polylog n)$ for big-to-small updates, and
$O((r/\sqrt{n})\polylog n)$ for small-to-big ones;
see Appendix~\ref{SSA:update-support-star}.
%
\begin{toappendix}
\subsection{Updating support stars}
\label{SSA:update-support-star}

Initially, we label stars big or small according to the $\sqrt{n}$ threshold.
Afterwards, a star that is currently big is turned into a small star once
$\abs{\Sigma_a} \leq \sqrt{n}/2$, and star that is currently small is turned
into a big star once $\abs{\Sigma_a} \geq 2\sqrt{n}$.
We say a star which crosses one of these size thresholds is \EMPH{changing state}
(from small-to-big or big-to-small), and must be represented in the opposite
type of data structure.
Our strategy is to charge the data structure update time associated with a
state change to the \EMPH{membership changes} in $\Sigma_a$ that preceded the
state change.

A star $\Sigma_a$ undergoing a big-to-small state change has size
$\abs{\Sigma_a} \leq \sqrt{n}/2$.
The state change deletes $\EuScript{D}_\text{big}(a)$ and inserts $\Sigma_a$
into $\EuScript{D}_\text{small}$.
Thus, the time spent for a big-to-small state change is $O(\sqrt{n}\polylog n)$,
and there were at least $\sqrt{n}/2$ points removed from $\Sigma_a$ since it
last changed state.
The amortized time for a big-to-small state change per star membership change
is $O(\polylog n)$.

A star $\Sigma_a$ undergoing a small-to-big state change has size
$\abs{\Sigma_a} \geq 2\sqrt{n}$.
We can write its size as $\abs{\Sigma_a} = \sqrt{n} + x$ for some integer
$x \geq \sqrt{n}$, so we also have $\abs{\Sigma_a} \leq 2x$.
When switching, we delete all $\abs{\Sigma_a}$ points from
$\EuScript{D}_\text{small}$ and construct a new $\EuScript{D}_\text{big}(a)$.
Constructing $\EuScript{D}_\text{big}(a)$ requires inserting up to $r$ points
of $A$ (into $P$) and the $\abs{\Sigma_a}$ points of the star (into $Q$).
Thus, the time spent for a small-to-big state change is $(r + 2x) \cdot O(\polylog n)$,
and there were at least $x$ points added to $\Sigma_a$ since it last changed state.
The amortized time for a small-to-big state change per star membership change
is $O((r/x)\polylog n)$.
Since $x \geq \sqrt{n}$, this is at most $O((r/\sqrt{n})\polylog n)$.

Star membership can only be changed by augmenting paths passing through the
vertex, therefore the total number of membership changes is $O(rn\log n)$
by Lemmas~\ref{theorem:orlin_old} and \ref{lemma:orlin_relax_count}.
Thus, the total time spent on state changes is $O((r^2\sqrt{n} + rn)\polylog n)$.

\end{toappendix}
%
%Membership of support stars can only be changed by augmentations,
The number of star membership changes by adding a single augmenting path is
bounded above by twice of its length, so $O(r)$.
By Theorem~\ref{theorem:orlin_old}, the total number of membership changes is $O(rn\log n)$.
The total time spent on big-to-small updates is $O(rn\polylog n)$, and the
total time spent on small-to-big updates is $O(r^2\sqrt{n}\polylog n)$.
Membership changes themselves can be performed in $O(\polylog n)$ time each.

\paragraph{Preprocessing time.}
It takes $O(rn\polylog n)$ time to build the very first set of data structures.
There are $r \cdot \abs{\Sigma_a}$ points to insert for each $\EuScript{D}_\text{big}(a)$,
%and the support stars are disjoint,
so the number of points to insert is $O(rn)$.
At most $O(rn)$ points have to be inserted for $\EuScript{D}_\text{small}$.
%Each BCP data structure can be constructed in its size times $O(\polylog n)$,
So the total preprocessing time is $O(rn\polylog n)$.

\paragraph{Between searches.}
After each augmentation, we reset the data structures to their initial
state plus the change from augmentation using rewinding mechanism (see Section~\ref{SS:rewinding}).
%By reversing the sequence of insertions/deletions to each data structure over
%the course of the Hungarian search, we can recover the versions data structures
%as they were when the Hungarian search began.
This takes time proportional to the time for Hungarian search,
which is $O(r\sqrt{n}\polylog n)$ by Lemma~\ref{lemma:orlin_hs_time}.
The most recent augmentation may have deactivated $O(1)$ active excess and deficit vertices, which takes
$O(\sqrt{n}\polylog n)$ time to update.
% Additionally, the augmentation may have changed the membership for some support
% stars, which we ahve analyzed earlier.
Finally, we note that an augmenting path cannot reduce the support degree of
a vertex to zero, and therefore no new dead vertices are created by augmentation.

\paragraph{Between excess scales.}
When the excess scale changes, vertices that were previously inactive may
become active, and vertices that were dead may be revived.
%(however, no active vertices deactivate, and no live vertices die as the result of $\Delta$ decreasing).
If we have the data structures built at the end of the
previous scale, then we can add in each new active vertex $a \in A$ and
charge the insertion to the (future) augmenting path or contraction which
eventually causes the vertex being inactive or absorbed.
By Theorem~\ref{theorem:orlin_old}, there are $O(n\log n)$ such newly active
vertices; each of them takes $O(\sqrt{n}\polylog n)$ to update.
So the total time spent is $O(n^{3/2}\polylog n)$.

\paragraph{Putting it together.}
After $O(rn\polylog n)$ preprocessing, we spend $O(r\sqrt{n}\polylog n)$ time
on relaxations each Hungarian search by Lemma~\ref{lemma:orlin_hs_time},
for a total of $O(rn^{3/2}\polylog n)$ time over the course of the algorithm.
Rewinding takes the same amount of time.
We spend up to $O((r^2\sqrt{n} + rn)\polylog n)$ time switching stars between big/small.
We spend $O(n^{3/2}\polylog n)$ time activating and reviving vertices.
Adding, the algorithm takes $O((r^2\sqrt{n} + rn^{3/2})\polylog n)$ time to
produce optimal potentials $\pi^*$, from which we can recover $f^*$ in
$O(n\polylog n)$ additional time.
This completes the proof of Theorem~\ref{theorem:orlin}.


% Acknowledgment and Bibliography
% \paragraph*{Acknowledgment.}
% We thank Haim Kaplan for useful discussion and suggesting to use Goldberg~\etal~\cite{GHKT17} for our approximation algorithm.

\bibliographystyle{newuser}
\bibliography{ref}

\newpage
\appendix

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
